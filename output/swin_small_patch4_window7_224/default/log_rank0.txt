[2022-02-14 13:22:35 swin_small_patch4_window7_224] (main.py 354): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-14 13:22:35 swin_small_patch4_window7_224] (main.py 357): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 48
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 22
  AUTO_RESUME: true
  BASE_LR: 0.00103125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0312500000000002e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.03125e-06
  WEIGHT_DECAY: 0.05

[2022-02-14 13:22:38 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-14 13:22:38 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-14 13:22:38 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-14 13:22:38 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-14 13:22:38 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-14 13:22:38 swin_small_patch4_window7_224] (main.py 139): INFO Start training
[2022-02-14 13:22:44 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][0/26690]	eta 1 day, 18:09:35 lr 0.000001	time 5.6866 (5.6866)	loss 0.3235 (0.3235)	grad_norm 21386.2656 (21386.2656)	mem 6353MB
[2022-02-14 13:22:46 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][10/26690]	eta 5:13:55 lr 0.000001	time 0.2069 (0.7060)	loss 0.3189 (0.3190)	grad_norm 22482.4648 (20680.7734)	mem 6533MB
[2022-02-14 13:22:48 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][20/26690]	eta 3:28:19 lr 0.000001	time 0.2067 (0.4687)	loss 0.3193 (0.3186)	grad_norm 20812.7520 (20674.5430)	mem 6533MB
[2022-02-14 13:22:50 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][30/26690]	eta 2:51:15 lr 0.000001	time 0.2080 (0.3854)	loss 0.3175 (0.3181)	grad_norm 22131.1992 (20421.7969)	mem 6907MB
[2022-02-14 13:22:52 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][40/26690]	eta 2:31:54 lr 0.000001	time 0.2067 (0.3420)	loss 0.3156 (0.3178)	grad_norm 19871.3477 (20353.2012)	mem 6907MB
[2022-02-14 13:22:54 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][50/26690]	eta 2:20:17 lr 0.000001	time 0.2080 (0.3160)	loss 0.3185 (0.3177)	grad_norm 20015.2617 (20291.9746)	mem 6907MB
[2022-02-14 13:22:56 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][60/26690]	eta 2:12:22 lr 0.000001	time 0.2083 (0.2983)	loss 0.3222 (0.3178)	grad_norm 20278.1934 (20422.4785)	mem 6907MB
[2022-02-14 13:22:58 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][70/26690]	eta 2:06:45 lr 0.000001	time 0.2071 (0.2857)	loss 0.3172 (0.3178)	grad_norm 19227.6230 (20446.6992)	mem 6907MB
[2022-02-14 13:23:00 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][80/26690]	eta 2:02:26 lr 0.000001	time 0.2073 (0.2761)	loss 0.3209 (0.3178)	grad_norm 20982.9004 (20385.9434)	mem 6907MB
[2022-02-14 13:23:03 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][90/26690]	eta 1:59:08 lr 0.000001	time 0.2067 (0.2687)	loss 0.3215 (0.3177)	grad_norm 21100.0352 (20404.5684)	mem 6907MB
[2022-02-14 13:23:05 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][100/26690]	eta 1:56:25 lr 0.000001	time 0.2082 (0.2627)	loss 0.3185 (0.3178)	grad_norm 21347.9941 (20451.5508)	mem 6907MB
[2022-02-14 13:23:07 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][110/26690]	eta 1:54:15 lr 0.000001	time 0.2062 (0.2579)	loss 0.3173 (0.3178)	grad_norm 20983.0078 (20444.7500)	mem 6907MB
[2022-02-14 13:23:09 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][120/26690]	eta 1:52:24 lr 0.000001	time 0.2089 (0.2538)	loss 0.3185 (0.3178)	grad_norm 23415.7227 (20435.2695)	mem 6907MB
[2022-02-14 13:23:11 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][130/26690]	eta 1:50:50 lr 0.000001	time 0.2088 (0.2504)	loss 0.3171 (0.3178)	grad_norm 18741.6797 (20446.9004)	mem 6907MB
[2022-02-14 13:23:13 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][140/26690]	eta 1:49:32 lr 0.000001	time 0.2080 (0.2475)	loss 0.3202 (0.3178)	grad_norm 21784.8672 (20456.7793)	mem 6907MB
[2022-02-14 13:23:15 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][150/26690]	eta 1:48:21 lr 0.000001	time 0.2091 (0.2450)	loss 0.3164 (0.3177)	grad_norm 21926.5332 (20480.4238)	mem 6907MB
[2022-02-14 13:23:17 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][160/26690]	eta 1:47:24 lr 0.000001	time 0.2110 (0.2429)	loss 0.3135 (0.3177)	grad_norm 17439.0449 (20468.4004)	mem 6907MB
[2022-02-14 13:23:19 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][170/26690]	eta 1:46:30 lr 0.000001	time 0.2089 (0.2410)	loss 0.3186 (0.3177)	grad_norm 20189.7695 (20449.1621)	mem 6907MB
[2022-02-14 13:23:21 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][180/26690]	eta 1:45:45 lr 0.000001	time 0.2095 (0.2393)	loss 0.3172 (0.3178)	grad_norm 18366.6270 (20473.2031)	mem 6907MB
[2022-02-14 13:23:24 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][190/26690]	eta 1:45:04 lr 0.000001	time 0.2145 (0.2379)	loss 0.3189 (0.3178)	grad_norm 21742.8164 (20472.9238)	mem 6907MB
[2022-02-14 13:23:26 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][200/26690]	eta 1:44:27 lr 0.000001	time 0.2078 (0.2366)	loss 0.3176 (0.3178)	grad_norm 21293.6895 (20515.1797)	mem 6907MB
[2022-02-14 13:23:28 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][210/26690]	eta 1:43:50 lr 0.000001	time 0.2076 (0.2353)	loss 0.3140 (0.3178)	grad_norm 17917.1133 (20543.7715)	mem 6907MB
[2022-02-14 13:23:30 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][220/26690]	eta 1:43:18 lr 0.000001	time 0.2073 (0.2342)	loss 0.3203 (0.3178)	grad_norm 20118.9863 (20562.6934)	mem 6907MB
[2022-02-14 13:23:32 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][230/26690]	eta 1:42:46 lr 0.000001	time 0.2087 (0.2331)	loss 0.3179 (0.3177)	grad_norm 23504.4609 (20569.0625)	mem 6907MB
[2022-02-14 13:23:34 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][240/26690]	eta 1:42:18 lr 0.000001	time 0.2078 (0.2321)	loss 0.3179 (0.3177)	grad_norm 21557.5859 (20611.2441)	mem 6907MB
[2022-02-14 13:23:36 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][250/26690]	eta 1:41:54 lr 0.000001	time 0.2102 (0.2313)	loss 0.3194 (0.3177)	grad_norm 20967.8750 (20630.9160)	mem 6907MB
[2022-02-14 13:23:38 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][260/26690]	eta 1:41:30 lr 0.000001	time 0.2094 (0.2304)	loss 0.3182 (0.3177)	grad_norm 21894.6660 (20657.6250)	mem 6907MB
[2022-02-14 13:23:40 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][270/26690]	eta 1:41:09 lr 0.000002	time 0.2101 (0.2297)	loss 0.3197 (0.3177)	grad_norm 20090.2461 (20694.6152)	mem 6907MB
[2022-02-14 13:23:42 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][280/26690]	eta 1:40:49 lr 0.000002	time 0.2104 (0.2290)	loss 0.3157 (0.3177)	grad_norm 18604.0078 (20645.2441)	mem 6907MB
[2022-02-14 13:23:45 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][290/26690]	eta 1:40:30 lr 0.000002	time 0.2106 (0.2284)	loss 0.3188 (0.3177)	grad_norm 24163.2227 (20663.6680)	mem 6907MB
[2022-02-14 13:23:47 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][300/26690]	eta 1:40:12 lr 0.000002	time 0.2097 (0.2278)	loss 0.3174 (0.3177)	grad_norm 20774.4004 (20680.9688)	mem 6907MB
[2022-02-14 13:23:49 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][310/26690]	eta 1:39:56 lr 0.000002	time 0.2096 (0.2273)	loss 0.3186 (0.3177)	grad_norm 19137.8633 (20678.0059)	mem 6907MB
[2022-02-14 13:23:51 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][320/26690]	eta 1:39:39 lr 0.000002	time 0.2098 (0.2268)	loss 0.3205 (0.3177)	grad_norm 20152.9707 (20649.6758)	mem 6907MB
[2022-02-14 13:23:53 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][330/26690]	eta 1:39:25 lr 0.000002	time 0.2086 (0.2263)	loss 0.3167 (0.3177)	grad_norm 23949.1055 (20658.5410)	mem 6907MB
[2022-02-14 13:23:55 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][340/26690]	eta 1:39:11 lr 0.000002	time 0.2100 (0.2259)	loss 0.3168 (0.3177)	grad_norm 17650.8184 (20620.4531)	mem 6907MB
[2022-02-14 13:23:57 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][350/26690]	eta 1:38:57 lr 0.000002	time 0.2092 (0.2254)	loss 0.3163 (0.3177)	grad_norm 20532.7500 (20618.4473)	mem 6907MB
[2022-02-14 13:23:59 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][360/26690]	eta 1:38:45 lr 0.000002	time 0.2098 (0.2250)	loss 0.3170 (0.3177)	grad_norm 21680.9414 (20622.7109)	mem 6907MB
[2022-02-14 13:24:01 swin_small_patch4_window7_224] (main.py 231): INFO Train: [0/300][370/26690]	eta 1:38:32 lr 0.000002	time 0.2111 (0.2247)	loss 0.3158 (0.3177)	grad_norm 22975.5430 (20634.0703)	mem 6907MB
