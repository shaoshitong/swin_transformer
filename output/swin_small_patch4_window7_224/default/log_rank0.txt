[2022-02-06 20:54:54 swin_small_patch4_window7_224] (main.py 353): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 20:54:54 swin_small_patch4_window7_224] (main.py 356): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 20:54:59 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 20:54:59 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 20:54:59 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 20:54:59 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:13:14 swin_small_patch4_window7_224] (main.py 351): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:13:14 swin_small_patch4_window7_224] (main.py 354): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:13:16 swin_small_patch4_window7_224] (main.py 81): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:13:17 swin_small_patch4_window7_224] (main.py 84): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:13:17 swin_small_patch4_window7_224] (main.py 93): INFO number of params: 49606258
[2022-02-06 21:13:17 swin_small_patch4_window7_224] (main.py 96): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:16:43 swin_small_patch4_window7_224] (main.py 351): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:16:43 swin_small_patch4_window7_224] (main.py 354): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:16:45 swin_small_patch4_window7_224] (main.py 81): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:16:46 swin_small_patch4_window7_224] (main.py 84): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:16:46 swin_small_patch4_window7_224] (main.py 93): INFO number of params: 49606258
[2022-02-06 21:16:46 swin_small_patch4_window7_224] (main.py 96): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:16:46 swin_small_patch4_window7_224] (main.py 120): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:16:46 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /data/swin_transformer/swin_small_patch4_window7_224.pth....................
[2022-02-06 21:16:46 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2022-02-06 21:16:48 swin_small_patch4_window7_224] (main.py 275): INFO Test: [0/391]	Time 2.247 (2.247)	Loss 5.4613 (5.4613)	Acc@1 38.281 (38.281)	Acc@5 53.125 (53.125)	Mem 2665MB
[2022-02-06 21:16:51 swin_small_patch4_window7_224] (main.py 275): INFO Test: [10/391]	Time 0.267 (0.452)	Loss 9.2980 (9.0560)	Acc@1 0.000 (3.480)	Acc@5 0.000 (4.972)	Mem 2666MB
[2022-02-06 21:16:54 swin_small_patch4_window7_224] (main.py 275): INFO Test: [20/391]	Time 0.272 (0.365)	Loss 9.4842 (9.2247)	Acc@1 0.000 (1.823)	Acc@5 0.000 (2.641)	Mem 2666MB
[2022-02-06 21:16:56 swin_small_patch4_window7_224] (main.py 275): INFO Test: [30/391]	Time 0.266 (0.333)	Loss 9.0501 (9.2540)	Acc@1 0.000 (1.235)	Acc@5 0.000 (1.815)	Mem 2666MB
[2022-02-06 21:16:59 swin_small_patch4_window7_224] (main.py 275): INFO Test: [40/391]	Time 0.267 (0.318)	Loss 9.2935 (9.2775)	Acc@1 0.000 (0.934)	Acc@5 0.000 (1.372)	Mem 2666MB
[2022-02-06 21:17:02 swin_small_patch4_window7_224] (main.py 275): INFO Test: [50/391]	Time 0.269 (0.308)	Loss 9.3276 (9.3020)	Acc@1 0.000 (0.751)	Acc@5 0.000 (1.103)	Mem 2666MB
[2022-02-06 21:17:04 swin_small_patch4_window7_224] (main.py 275): INFO Test: [60/391]	Time 0.267 (0.302)	Loss 9.4908 (9.3200)	Acc@1 0.000 (0.628)	Acc@5 0.000 (0.986)	Mem 2666MB
[2022-02-06 21:17:07 swin_small_patch4_window7_224] (main.py 275): INFO Test: [70/391]	Time 0.268 (0.297)	Loss 9.5421 (9.3458)	Acc@1 0.000 (0.539)	Acc@5 0.000 (0.847)	Mem 2666MB
[2022-02-06 21:17:10 swin_small_patch4_window7_224] (main.py 275): INFO Test: [80/391]	Time 0.276 (0.294)	Loss 9.4905 (9.3644)	Acc@1 0.000 (0.473)	Acc@5 0.000 (0.743)	Mem 2666MB
[2022-02-06 21:17:12 swin_small_patch4_window7_224] (main.py 275): INFO Test: [90/391]	Time 0.268 (0.291)	Loss 9.3866 (9.3715)	Acc@1 0.000 (0.421)	Acc@5 0.000 (0.661)	Mem 2666MB
[2022-02-06 21:17:15 swin_small_patch4_window7_224] (main.py 275): INFO Test: [100/391]	Time 0.268 (0.289)	Loss 9.3651 (9.3826)	Acc@1 0.000 (0.379)	Acc@5 0.000 (0.596)	Mem 2666MB
[2022-02-06 21:17:18 swin_small_patch4_window7_224] (main.py 275): INFO Test: [110/391]	Time 0.270 (0.287)	Loss 9.4972 (9.3816)	Acc@1 0.000 (0.345)	Acc@5 0.000 (0.542)	Mem 2666MB
[2022-02-06 21:17:21 swin_small_patch4_window7_224] (main.py 275): INFO Test: [120/391]	Time 0.283 (0.286)	Loss 9.5464 (9.3755)	Acc@1 0.000 (0.316)	Acc@5 0.000 (0.497)	Mem 2666MB
[2022-02-06 21:17:23 swin_small_patch4_window7_224] (main.py 275): INFO Test: [130/391]	Time 0.272 (0.285)	Loss 9.6369 (9.3729)	Acc@1 0.000 (0.292)	Acc@5 0.000 (0.459)	Mem 2666MB
[2022-02-06 21:17:26 swin_small_patch4_window7_224] (main.py 275): INFO Test: [140/391]	Time 0.277 (0.284)	Loss 9.2529 (9.3780)	Acc@1 0.000 (0.271)	Acc@5 0.000 (0.432)	Mem 2666MB
[2022-02-06 21:17:29 swin_small_patch4_window7_224] (main.py 275): INFO Test: [150/391]	Time 0.281 (0.284)	Loss 9.1792 (9.3707)	Acc@1 0.000 (0.254)	Acc@5 0.000 (0.404)	Mem 2666MB
[2022-02-06 21:17:32 swin_small_patch4_window7_224] (main.py 275): INFO Test: [160/391]	Time 0.276 (0.284)	Loss 9.2605 (9.3492)	Acc@1 0.000 (0.238)	Acc@5 0.000 (0.738)	Mem 2666MB
[2022-02-06 21:17:34 swin_small_patch4_window7_224] (main.py 275): INFO Test: [170/391]	Time 0.272 (0.283)	Loss 9.4036 (9.3523)	Acc@1 0.000 (0.224)	Acc@5 0.000 (0.694)	Mem 2666MB
[2022-02-06 21:17:37 swin_small_patch4_window7_224] (main.py 275): INFO Test: [180/391]	Time 0.275 (0.283)	Loss 9.3303 (9.3562)	Acc@1 0.000 (0.211)	Acc@5 0.000 (0.656)	Mem 2666MB
[2022-02-06 21:17:40 swin_small_patch4_window7_224] (main.py 275): INFO Test: [190/391]	Time 0.277 (0.282)	Loss 9.5104 (9.3599)	Acc@1 0.000 (0.200)	Acc@5 0.000 (0.626)	Mem 2666MB
[2022-02-06 21:17:43 swin_small_patch4_window7_224] (main.py 275): INFO Test: [200/391]	Time 0.284 (0.282)	Loss 9.4965 (9.3617)	Acc@1 0.000 (0.190)	Acc@5 0.000 (0.595)	Mem 2666MB
[2022-02-06 21:17:45 swin_small_patch4_window7_224] (main.py 275): INFO Test: [210/391]	Time 0.278 (0.281)	Loss 9.3390 (9.3640)	Acc@1 0.000 (0.181)	Acc@5 0.000 (0.566)	Mem 2666MB
[2022-02-06 21:17:48 swin_small_patch4_window7_224] (main.py 275): INFO Test: [220/391]	Time 0.282 (0.281)	Loss 9.4322 (9.3677)	Acc@1 0.000 (0.173)	Acc@5 0.000 (0.544)	Mem 2666MB
[2022-02-06 21:17:51 swin_small_patch4_window7_224] (main.py 275): INFO Test: [230/391]	Time 0.279 (0.281)	Loss 9.4056 (9.3704)	Acc@1 0.000 (0.166)	Acc@5 0.000 (0.521)	Mem 2666MB
[2022-02-06 21:17:54 swin_small_patch4_window7_224] (main.py 275): INFO Test: [240/391]	Time 0.273 (0.281)	Loss 9.5445 (9.3742)	Acc@1 0.000 (0.159)	Acc@5 0.000 (0.499)	Mem 2666MB
[2022-02-06 21:17:56 swin_small_patch4_window7_224] (main.py 275): INFO Test: [250/391]	Time 0.281 (0.281)	Loss 9.2141 (9.3774)	Acc@1 0.000 (0.153)	Acc@5 0.000 (0.479)	Mem 2666MB
[2022-02-06 21:17:59 swin_small_patch4_window7_224] (main.py 275): INFO Test: [260/391]	Time 0.281 (0.281)	Loss 9.6237 (9.3817)	Acc@1 0.000 (0.147)	Acc@5 0.000 (0.461)	Mem 2666MB
[2022-02-06 21:18:02 swin_small_patch4_window7_224] (main.py 275): INFO Test: [270/391]	Time 0.276 (0.280)	Loss 9.5301 (9.3841)	Acc@1 0.000 (0.141)	Acc@5 0.000 (0.444)	Mem 2666MB
[2022-02-06 21:18:05 swin_small_patch4_window7_224] (main.py 275): INFO Test: [280/391]	Time 0.279 (0.280)	Loss 9.5507 (9.3889)	Acc@1 0.000 (0.136)	Acc@5 0.000 (0.428)	Mem 2666MB
[2022-02-06 21:18:07 swin_small_patch4_window7_224] (main.py 275): INFO Test: [290/391]	Time 0.279 (0.280)	Loss 9.4961 (9.3916)	Acc@1 0.000 (0.132)	Acc@5 0.000 (0.413)	Mem 2666MB
[2022-02-06 21:18:10 swin_small_patch4_window7_224] (main.py 275): INFO Test: [300/391]	Time 0.273 (0.280)	Loss 9.4456 (9.3894)	Acc@1 0.000 (0.127)	Acc@5 0.000 (0.420)	Mem 2666MB
[2022-02-06 21:18:13 swin_small_patch4_window7_224] (main.py 275): INFO Test: [310/391]	Time 0.273 (0.280)	Loss 9.3871 (9.3945)	Acc@1 0.000 (0.123)	Acc@5 0.000 (0.407)	Mem 2666MB
[2022-02-06 21:18:16 swin_small_patch4_window7_224] (main.py 275): INFO Test: [320/391]	Time 0.275 (0.280)	Loss 9.4801 (9.3955)	Acc@1 0.000 (0.119)	Acc@5 0.000 (0.394)	Mem 2666MB
[2022-02-06 21:18:18 swin_small_patch4_window7_224] (main.py 275): INFO Test: [330/391]	Time 0.272 (0.280)	Loss 9.4506 (9.3972)	Acc@1 0.000 (0.116)	Acc@5 0.000 (0.385)	Mem 2666MB
[2022-02-06 21:18:21 swin_small_patch4_window7_224] (main.py 275): INFO Test: [340/391]	Time 0.282 (0.280)	Loss 9.4492 (9.3974)	Acc@1 0.000 (0.112)	Acc@5 0.000 (0.373)	Mem 2666MB
[2022-02-06 21:18:24 swin_small_patch4_window7_224] (main.py 275): INFO Test: [350/391]	Time 0.280 (0.279)	Loss 9.3268 (9.3980)	Acc@1 0.000 (0.109)	Acc@5 0.000 (0.363)	Mem 2666MB
[2022-02-06 21:18:27 swin_small_patch4_window7_224] (main.py 275): INFO Test: [360/391]	Time 0.279 (0.279)	Loss 9.3949 (9.3981)	Acc@1 0.000 (0.106)	Acc@5 0.000 (0.353)	Mem 2666MB
[2022-02-06 21:18:30 swin_small_patch4_window7_224] (main.py 275): INFO Test: [370/391]	Time 0.278 (0.279)	Loss 9.3634 (9.3974)	Acc@1 0.000 (0.103)	Acc@5 0.000 (0.343)	Mem 2666MB
[2022-02-06 21:18:32 swin_small_patch4_window7_224] (main.py 275): INFO Test: [380/391]	Time 0.280 (0.279)	Loss 9.5480 (9.3989)	Acc@1 0.000 (0.100)	Acc@5 0.000 (0.338)	Mem 2666MB
[2022-02-06 21:18:35 swin_small_patch4_window7_224] (main.py 275): INFO Test: [390/391]	Time 0.190 (0.279)	Loss 9.2334 (9.4007)	Acc@1 0.000 (0.098)	Acc@5 0.000 (0.330)	Mem 2666MB
[2022-02-06 21:18:35 swin_small_patch4_window7_224] (main.py 282): INFO  * Acc@1 0.098 Acc@5 0.330
[2022-02-06 21:18:35 swin_small_patch4_window7_224] (main.py 125): INFO Accuracy of the network on the 50000 test images: 0.1%
[2022-02-06 21:28:57 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:28:57 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: true
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:29:16 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:29:16 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: true
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:29:26 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:29:26 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:29:28 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:29:29 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:29:29 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 21:29:29 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:29:29 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:29:29 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /data/swin_transformer/swin_small_patch4_window7_224.pth....................
[2022-02-06 21:29:29 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2022-02-06 21:29:31 swin_small_patch4_window7_224] (main.py 276): INFO Test: [0/391]	Time 2.364 (2.364)	Loss 5.4613 (5.4613)	Acc@1 38.281 (38.281)	Acc@5 53.125 (53.125)	Mem 2665MB
[2022-02-06 21:31:49 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:31:49 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:33:12 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:33:12 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:33:14 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:33:15 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:33:15 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 21:33:15 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:33:15 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:33:15 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /data/swin_transformer/swin_small_patch4_window7_224.pth....................
[2022-02-06 21:33:15 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2022-02-06 21:33:17 swin_small_patch4_window7_224] (main.py 276): INFO Test: [0/391]	Time 2.177 (2.177)	Loss 5.4613 (5.4613)	Acc@1 38.281 (38.281)	Acc@5 53.125 (53.125)	Mem 2665MB
[2022-02-06 21:33:20 swin_small_patch4_window7_224] (main.py 276): INFO Test: [10/391]	Time 0.268 (0.444)	Loss 9.2980 (9.0560)	Acc@1 0.000 (3.480)	Acc@5 0.000 (4.972)	Mem 2666MB
[2022-02-06 21:40:13 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:40:13 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:40:15 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:40:16 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:40:16 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 21:40:16 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:40:16 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:40:16 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /data/swin_transformer/swin_small_patch4_window7_224.pth....................
[2022-02-06 21:40:16 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2022-02-06 21:40:18 swin_small_patch4_window7_224] (main.py 276): INFO Test: [0/391]	Time 2.233 (2.233)	Loss 0.2877 (0.2877)	Acc@1 96.094 (96.094)	Acc@5 98.438 (98.438)	Mem 2665MB
[2022-02-06 21:40:21 swin_small_patch4_window7_224] (main.py 276): INFO Test: [10/391]	Time 0.268 (0.450)	Loss 0.9373 (0.3851)	Acc@1 77.344 (92.330)	Acc@5 97.656 (98.651)	Mem 2666MB
[2022-02-06 21:40:23 swin_small_patch4_window7_224] (main.py 276): INFO Test: [20/391]	Time 0.268 (0.364)	Loss 0.4377 (0.5177)	Acc@1 90.625 (88.653)	Acc@5 97.656 (98.065)	Mem 2666MB
[2022-02-06 21:40:26 swin_small_patch4_window7_224] (main.py 276): INFO Test: [30/391]	Time 0.269 (0.334)	Loss 1.0918 (0.6317)	Acc@1 75.781 (85.610)	Acc@5 92.969 (97.404)	Mem 2666MB
[2022-02-06 21:40:29 swin_small_patch4_window7_224] (main.py 276): INFO Test: [40/391]	Time 0.273 (0.318)	Loss 0.4753 (0.5604)	Acc@1 92.969 (87.729)	Acc@5 96.094 (97.771)	Mem 2666MB
[2022-02-06 21:40:32 swin_small_patch4_window7_224] (main.py 276): INFO Test: [50/391]	Time 0.267 (0.308)	Loss 0.2409 (0.5616)	Acc@1 95.312 (87.960)	Acc@5 100.000 (97.718)	Mem 2666MB
[2022-02-06 21:40:34 swin_small_patch4_window7_224] (main.py 276): INFO Test: [60/391]	Time 0.269 (0.302)	Loss 0.7142 (0.5262)	Acc@1 84.375 (88.768)	Acc@5 96.875 (97.925)	Mem 2666MB
[2022-02-06 21:40:37 swin_small_patch4_window7_224] (main.py 276): INFO Test: [70/391]	Time 0.267 (0.297)	Loss 0.7248 (0.5530)	Acc@1 81.250 (87.995)	Acc@5 97.656 (97.843)	Mem 2666MB
[2022-02-06 21:40:40 swin_small_patch4_window7_224] (main.py 276): INFO Test: [80/391]	Time 0.268 (0.294)	Loss 0.6008 (0.5651)	Acc@1 88.281 (87.616)	Acc@5 96.875 (97.801)	Mem 2666MB
[2022-02-06 21:40:42 swin_small_patch4_window7_224] (main.py 276): INFO Test: [90/391]	Time 0.267 (0.291)	Loss 0.7313 (0.5668)	Acc@1 75.781 (87.646)	Acc@5 97.656 (97.716)	Mem 2666MB
[2022-02-06 21:40:45 swin_small_patch4_window7_224] (main.py 276): INFO Test: [100/391]	Time 0.269 (0.289)	Loss 0.4607 (0.5668)	Acc@1 91.406 (87.523)	Acc@5 96.875 (97.741)	Mem 2666MB
[2022-02-06 21:40:48 swin_small_patch4_window7_224] (main.py 276): INFO Test: [110/391]	Time 0.276 (0.288)	Loss 1.0530 (0.5771)	Acc@1 64.062 (87.176)	Acc@5 98.438 (97.727)	Mem 2666MB
[2022-02-06 21:40:50 swin_small_patch4_window7_224] (main.py 276): INFO Test: [120/391]	Time 0.268 (0.286)	Loss 0.4622 (0.5767)	Acc@1 90.625 (87.339)	Acc@5 97.656 (97.727)	Mem 2666MB
[2022-02-06 21:40:53 swin_small_patch4_window7_224] (main.py 276): INFO Test: [130/391]	Time 0.268 (0.285)	Loss 0.2737 (0.5690)	Acc@1 95.312 (87.524)	Acc@5 100.000 (97.758)	Mem 2666MB
[2022-02-06 21:40:56 swin_small_patch4_window7_224] (main.py 276): INFO Test: [140/391]	Time 0.269 (0.284)	Loss 0.9362 (0.5682)	Acc@1 78.125 (87.411)	Acc@5 99.219 (97.839)	Mem 2666MB
[2022-02-06 21:40:59 swin_small_patch4_window7_224] (main.py 276): INFO Test: [150/391]	Time 0.268 (0.283)	Loss 0.7660 (0.5771)	Acc@1 75.781 (87.246)	Acc@5 98.438 (97.786)	Mem 2666MB
[2022-02-06 21:41:01 swin_small_patch4_window7_224] (main.py 276): INFO Test: [160/391]	Time 0.270 (0.282)	Loss 0.6084 (0.5795)	Acc@1 84.375 (87.228)	Acc@5 97.656 (97.714)	Mem 2666MB
[2022-02-06 21:41:04 swin_small_patch4_window7_224] (main.py 276): INFO Test: [170/391]	Time 0.269 (0.282)	Loss 1.3787 (0.5963)	Acc@1 64.844 (86.719)	Acc@5 90.625 (97.579)	Mem 2666MB
[2022-02-06 21:41:07 swin_small_patch4_window7_224] (main.py 276): INFO Test: [180/391]	Time 0.274 (0.281)	Loss 1.5492 (0.6117)	Acc@1 62.500 (86.356)	Acc@5 92.969 (97.445)	Mem 2666MB
[2022-02-06 21:41:09 swin_small_patch4_window7_224] (main.py 276): INFO Test: [190/391]	Time 0.270 (0.281)	Loss 1.3922 (0.6333)	Acc@1 66.406 (85.762)	Acc@5 93.750 (97.255)	Mem 2666MB
[2022-02-06 21:41:12 swin_small_patch4_window7_224] (main.py 276): INFO Test: [200/391]	Time 0.276 (0.280)	Loss 0.4982 (0.6480)	Acc@1 85.938 (85.440)	Acc@5 99.219 (97.108)	Mem 2666MB
[2022-02-06 21:41:15 swin_small_patch4_window7_224] (main.py 276): INFO Test: [210/391]	Time 0.275 (0.280)	Loss 0.7462 (0.6605)	Acc@1 81.250 (85.064)	Acc@5 98.438 (97.027)	Mem 2666MB
[2022-02-06 21:41:18 swin_small_patch4_window7_224] (main.py 276): INFO Test: [220/391]	Time 0.274 (0.280)	Loss 0.3094 (0.6618)	Acc@1 93.750 (85.022)	Acc@5 98.438 (96.988)	Mem 2666MB
[2022-02-06 21:41:20 swin_small_patch4_window7_224] (main.py 276): INFO Test: [230/391]	Time 0.272 (0.279)	Loss 1.1749 (0.6680)	Acc@1 71.875 (84.909)	Acc@5 90.625 (96.905)	Mem 2666MB
[2022-02-06 21:41:23 swin_small_patch4_window7_224] (main.py 276): INFO Test: [240/391]	Time 0.270 (0.279)	Loss 0.7361 (0.6680)	Acc@1 88.281 (84.962)	Acc@5 92.969 (96.852)	Mem 2666MB
[2022-02-06 21:41:26 swin_small_patch4_window7_224] (main.py 276): INFO Test: [250/391]	Time 0.272 (0.279)	Loss 0.3566 (0.6822)	Acc@1 94.531 (84.471)	Acc@5 100.000 (96.741)	Mem 2666MB
[2022-02-06 21:41:28 swin_small_patch4_window7_224] (main.py 276): INFO Test: [260/391]	Time 0.268 (0.279)	Loss 0.7171 (0.6926)	Acc@1 81.250 (84.192)	Acc@5 96.094 (96.624)	Mem 2666MB
[2022-02-06 21:41:31 swin_small_patch4_window7_224] (main.py 276): INFO Test: [270/391]	Time 0.270 (0.278)	Loss 1.3057 (0.6991)	Acc@1 67.188 (84.049)	Acc@5 92.188 (96.546)	Mem 2666MB
[2022-02-06 21:41:34 swin_small_patch4_window7_224] (main.py 276): INFO Test: [280/391]	Time 0.270 (0.278)	Loss 0.8394 (0.7004)	Acc@1 79.688 (84.047)	Acc@5 98.438 (96.533)	Mem 2666MB
[2022-02-06 21:41:37 swin_small_patch4_window7_224] (main.py 276): INFO Test: [290/391]	Time 0.270 (0.278)	Loss 1.2197 (0.7050)	Acc@1 54.688 (83.897)	Acc@5 94.531 (96.464)	Mem 2666MB
[2022-02-06 21:41:39 swin_small_patch4_window7_224] (main.py 276): INFO Test: [300/391]	Time 0.271 (0.278)	Loss 0.5424 (0.7097)	Acc@1 88.281 (83.809)	Acc@5 98.438 (96.408)	Mem 2666MB
[2022-02-06 21:41:42 swin_small_patch4_window7_224] (main.py 276): INFO Test: [310/391]	Time 0.271 (0.278)	Loss 0.7093 (0.7114)	Acc@1 83.594 (83.787)	Acc@5 96.875 (96.388)	Mem 2666MB
[2022-02-06 21:41:45 swin_small_patch4_window7_224] (main.py 276): INFO Test: [320/391]	Time 0.272 (0.277)	Loss 0.4802 (0.7175)	Acc@1 88.281 (83.703)	Acc@5 96.094 (96.313)	Mem 2666MB
[2022-02-06 21:41:48 swin_small_patch4_window7_224] (main.py 276): INFO Test: [330/391]	Time 0.270 (0.277)	Loss 1.1700 (0.7270)	Acc@1 65.625 (83.429)	Acc@5 92.188 (96.238)	Mem 2666MB
[2022-02-06 21:41:50 swin_small_patch4_window7_224] (main.py 276): INFO Test: [340/391]	Time 0.276 (0.277)	Loss 0.5581 (0.7295)	Acc@1 85.938 (83.369)	Acc@5 98.438 (96.217)	Mem 2666MB
[2022-02-06 21:41:53 swin_small_patch4_window7_224] (main.py 276): INFO Test: [350/391]	Time 0.274 (0.277)	Loss 0.6340 (0.7306)	Acc@1 84.375 (83.307)	Acc@5 96.875 (96.214)	Mem 2666MB
[2022-02-06 21:41:56 swin_small_patch4_window7_224] (main.py 276): INFO Test: [360/391]	Time 0.269 (0.277)	Loss 1.2378 (0.7369)	Acc@1 72.656 (83.178)	Acc@5 93.750 (96.174)	Mem 2666MB
[2022-02-06 21:41:59 swin_small_patch4_window7_224] (main.py 276): INFO Test: [370/391]	Time 0.279 (0.277)	Loss 0.8990 (0.7336)	Acc@1 72.656 (83.232)	Acc@5 97.656 (96.212)	Mem 2666MB
[2022-02-06 21:42:01 swin_small_patch4_window7_224] (main.py 276): INFO Test: [380/391]	Time 0.274 (0.277)	Loss 0.6969 (0.7358)	Acc@1 82.031 (83.163)	Acc@5 99.219 (96.215)	Mem 2666MB
[2022-02-06 21:42:04 swin_small_patch4_window7_224] (main.py 276): INFO Test: [390/391]	Time 0.191 (0.277)	Loss 1.3587 (0.7343)	Acc@1 61.250 (83.178)	Acc@5 95.000 (96.240)	Mem 2666MB
[2022-02-06 21:42:04 swin_small_patch4_window7_224] (main.py 283): INFO  * Acc@1 83.178 Acc@5 96.240
[2022-02-06 21:42:04 swin_small_patch4_window7_224] (main.py 126): INFO Accuracy of the network on the 50000 test images: 83.2%
[2022-02-06 21:48:33 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:48:33 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: /data/swin_transformer/swin_small_patch4_window7_224.pth
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:48:35 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:48:36 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:48:36 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 21:48:36 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:48:36 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:48:36 swin_small_patch4_window7_224] (utils.py 20): INFO ==============> Resuming form /data/swin_transformer/swin_small_patch4_window7_224.pth....................
[2022-02-06 21:48:36 swin_small_patch4_window7_224] (utils.py 27): INFO <All keys matched successfully>
[2022-02-06 21:48:38 swin_small_patch4_window7_224] (main.py 276): INFO Test: [0/391]	Time 2.288 (2.288)	Loss 0.2877 (0.2877)	Acc@1 96.094 (96.094)	Acc@5 98.438 (98.438)	Mem 2665MB
[2022-02-06 21:48:41 swin_small_patch4_window7_224] (main.py 276): INFO Test: [10/391]	Time 0.268 (0.453)	Loss 0.9373 (0.3851)	Acc@1 77.344 (92.330)	Acc@5 97.656 (98.651)	Mem 2666MB
[2022-02-06 21:48:43 swin_small_patch4_window7_224] (main.py 276): INFO Test: [20/391]	Time 0.268 (0.366)	Loss 0.4377 (0.5177)	Acc@1 90.625 (88.653)	Acc@5 97.656 (98.065)	Mem 2666MB
[2022-02-06 21:48:46 swin_small_patch4_window7_224] (main.py 276): INFO Test: [30/391]	Time 0.270 (0.335)	Loss 1.0918 (0.6317)	Acc@1 75.781 (85.610)	Acc@5 92.969 (97.404)	Mem 2666MB
[2022-02-06 21:48:49 swin_small_patch4_window7_224] (main.py 276): INFO Test: [40/391]	Time 0.269 (0.320)	Loss 0.4753 (0.5604)	Acc@1 92.969 (87.729)	Acc@5 96.094 (97.771)	Mem 2666MB
[2022-02-06 21:48:52 swin_small_patch4_window7_224] (main.py 276): INFO Test: [50/391]	Time 0.270 (0.310)	Loss 0.2409 (0.5616)	Acc@1 95.312 (87.960)	Acc@5 100.000 (97.718)	Mem 2666MB
[2022-02-06 21:48:54 swin_small_patch4_window7_224] (main.py 276): INFO Test: [60/391]	Time 0.268 (0.304)	Loss 0.7142 (0.5262)	Acc@1 84.375 (88.768)	Acc@5 96.875 (97.925)	Mem 2666MB
[2022-02-06 21:48:57 swin_small_patch4_window7_224] (main.py 276): INFO Test: [70/391]	Time 0.269 (0.299)	Loss 0.7248 (0.5530)	Acc@1 81.250 (87.995)	Acc@5 97.656 (97.843)	Mem 2666MB
[2022-02-06 21:49:00 swin_small_patch4_window7_224] (main.py 276): INFO Test: [80/391]	Time 0.269 (0.295)	Loss 0.6008 (0.5651)	Acc@1 88.281 (87.616)	Acc@5 96.875 (97.801)	Mem 2666MB
[2022-02-06 21:49:02 swin_small_patch4_window7_224] (main.py 276): INFO Test: [90/391]	Time 0.267 (0.292)	Loss 0.7313 (0.5668)	Acc@1 75.781 (87.646)	Acc@5 97.656 (97.716)	Mem 2666MB
[2022-02-06 21:49:05 swin_small_patch4_window7_224] (main.py 276): INFO Test: [100/391]	Time 0.271 (0.290)	Loss 0.4607 (0.5668)	Acc@1 91.406 (87.523)	Acc@5 96.875 (97.741)	Mem 2666MB
[2022-02-06 21:49:08 swin_small_patch4_window7_224] (main.py 276): INFO Test: [110/391]	Time 0.274 (0.289)	Loss 1.0530 (0.5771)	Acc@1 64.062 (87.176)	Acc@5 98.438 (97.727)	Mem 2666MB
[2022-02-06 21:49:11 swin_small_patch4_window7_224] (main.py 276): INFO Test: [120/391]	Time 0.271 (0.287)	Loss 0.4622 (0.5767)	Acc@1 90.625 (87.339)	Acc@5 97.656 (97.727)	Mem 2666MB
[2022-02-06 21:49:13 swin_small_patch4_window7_224] (main.py 276): INFO Test: [130/391]	Time 0.269 (0.286)	Loss 0.2737 (0.5690)	Acc@1 95.312 (87.524)	Acc@5 100.000 (97.758)	Mem 2666MB
[2022-02-06 21:49:16 swin_small_patch4_window7_224] (main.py 276): INFO Test: [140/391]	Time 0.271 (0.285)	Loss 0.9362 (0.5682)	Acc@1 78.125 (87.411)	Acc@5 99.219 (97.839)	Mem 2666MB
[2022-02-06 21:49:43 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:49:43 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-02-06 21:49:45 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:49:46 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:49:46 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 21:49:46 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:49:46 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:49:46 swin_small_patch4_window7_224] (main.py 139): INFO Start training
[2022-02-06 21:52:43 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:52:43 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 96
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 9.375e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 9.375000000000001e-07
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 9.375e-08
  WEIGHT_DECAY: 0.05

[2022-02-06 21:52:46 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:52:46 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:52:46 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 21:52:46 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:52:46 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:52:46 swin_small_patch4_window7_224] (main.py 139): INFO Start training
[2022-02-06 21:53:38 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:53:38 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 80
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 7.8125e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 7.8125e-07
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 7.812499999999999e-08
  WEIGHT_DECAY: 0.05

[2022-02-06 21:53:40 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:53:41 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:53:41 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 21:53:41 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:53:41 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:53:41 swin_small_patch4_window7_224] (main.py 139): INFO Start training
[2022-02-06 21:53:55 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 21:53:55 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 40
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 3.90625e-05
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 3.90625e-07
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 3.9062499999999997e-08
  WEIGHT_DECAY: 0.05

[2022-02-06 21:53:57 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 21:53:58 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 21:53:58 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 21:53:58 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 21:53:58 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 21:53:58 swin_small_patch4_window7_224] (main.py 139): INFO Start training
[2022-02-06 21:53:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][0/32029]	eta 13:34:46 lr 0.000000	time 1.5263 (1.5263)	loss 7.0904 (7.0904)	grad_norm 528051.0000 (528051.0000)	mem 5358MB
[2022-02-06 21:54:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10/32029]	eta 2:47:18 lr 0.000000	time 0.1920 (0.3135)	loss 7.0462 (7.0158)	grad_norm 509413.1250 (506493.5000)	mem 5945MB
[2022-02-06 21:54:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][20/32029]	eta 2:16:24 lr 0.000000	time 0.1893 (0.2557)	loss 7.0007 (7.0084)	grad_norm 471118.9375 (493193.2812)	mem 5945MB
[2022-02-06 21:54:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][30/32029]	eta 2:05:21 lr 0.000000	time 0.1915 (0.2350)	loss 7.0068 (6.9986)	grad_norm 546251.0625 (487951.5938)	mem 5945MB
[2022-02-06 21:54:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][40/32029]	eta 1:59:36 lr 0.000000	time 0.1901 (0.2243)	loss 7.0056 (6.9908)	grad_norm 450468.3438 (485412.5312)	mem 5945MB
[2022-02-06 21:54:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][50/32029]	eta 1:56:06 lr 0.000000	time 0.1882 (0.2179)	loss 6.9793 (6.9882)	grad_norm 502006.3125 (484974.5625)	mem 5945MB
[2022-02-06 21:54:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][60/32029]	eta 1:53:42 lr 0.000000	time 0.1892 (0.2134)	loss 7.0015 (6.9844)	grad_norm 472098.1875 (487985.1562)	mem 5945MB
[2022-02-06 21:54:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][70/32029]	eta 1:51:59 lr 0.000000	time 0.1886 (0.2103)	loss 7.0409 (6.9856)	grad_norm 479510.0625 (489544.2812)	mem 5945MB
[2022-02-06 21:54:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][80/32029]	eta 1:50:40 lr 0.000000	time 0.1906 (0.2078)	loss 6.9547 (6.9840)	grad_norm 493755.2188 (487714.3125)	mem 5945MB
[2022-02-06 22:16:06 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 22:16:06 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 16
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2022-02-06 22:16:09 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 22:16:09 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 22:16:09 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 22:16:09 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 22:16:09 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 22:16:09 swin_small_patch4_window7_224] (main.py 139): INFO Start training
[2022-02-06 22:16:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][0/20018]	eta 10:07:05 lr 0.000001	time 1.8196 (1.8196)	loss 0.4409 (0.4409)	grad_norm 26850.3574 (26850.3574)	mem 8194MB
[2022-02-06 22:16:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10/20018]	eta 2:16:34 lr 0.000001	time 0.2705 (0.4095)	loss 0.4353 (0.4385)	grad_norm 26178.8105 (25171.5176)	mem 8376MB
[2022-02-06 22:17:35 swin_small_patch4_window7_224] (main.py 352): INFO Full config saved to output/swin_small_patch4_window7_224/default/config.json
[2022-02-06 22:17:35 swin_small_patch4_window7_224] (main.py 355): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /home/qiuziming/data/Imagenet/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_small_patch4_window7_224
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_small_patch4_window7_224/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 32
  AUTO_RESUME: true
  BASE_LR: 0.001
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.05

[2022-02-06 22:17:37 swin_small_patch4_window7_224] (main.py 82): INFO Creating model:swin/swin_small_patch4_window7_224
[2022-02-06 22:17:38 swin_small_patch4_window7_224] (main.py 85): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-02-06 22:17:38 swin_small_patch4_window7_224] (main.py 94): INFO number of params: 49606258
[2022-02-06 22:17:38 swin_small_patch4_window7_224] (main.py 97): INFO number of GFLOPs: 8.746520064
[2022-02-06 22:17:38 swin_small_patch4_window7_224] (main.py 121): INFO no checkpoint found in output/swin_small_patch4_window7_224/default, ignoring auto resume
[2022-02-06 22:17:38 swin_small_patch4_window7_224] (main.py 139): INFO Start training
[2022-02-06 22:17:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][0/40036]	eta 14:52:26 lr 0.000001	time 1.3375 (1.3375)	loss 0.2197 (0.2197)	grad_norm 16944.8145 (16944.8145)	mem 4355MB
[2022-02-06 22:17:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10/40036]	eta 2:46:50 lr 0.000001	time 0.1411 (0.2501)	loss 0.2202 (0.2194)	grad_norm 18209.5703 (17509.5293)	mem 4537MB
[2022-02-06 22:17:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][20/40036]	eta 2:12:23 lr 0.000001	time 0.1411 (0.1985)	loss 0.2201 (0.2193)	grad_norm 15792.8770 (17360.7520)	mem 4537MB
[2022-02-06 22:17:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][30/40036]	eta 2:00:10 lr 0.000001	time 0.1408 (0.1802)	loss 0.2204 (0.2191)	grad_norm 19365.4004 (17168.8496)	mem 4537MB
[2022-02-06 22:17:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][40/40036]	eta 1:54:13 lr 0.000001	time 0.1420 (0.1714)	loss 0.2156 (0.2188)	grad_norm 17593.3359 (17154.9102)	mem 4918MB
[2022-02-06 22:17:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][50/40036]	eta 1:50:25 lr 0.000001	time 0.1411 (0.1657)	loss 0.2176 (0.2185)	grad_norm 17543.3379 (17091.1836)	mem 4918MB
[2022-02-06 22:17:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][60/40036]	eta 1:47:53 lr 0.000001	time 0.1413 (0.1619)	loss 0.2159 (0.2185)	grad_norm 16955.4629 (17126.2129)	mem 4918MB
[2022-02-06 22:17:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][70/40036]	eta 1:46:10 lr 0.000001	time 0.1413 (0.1594)	loss 0.2168 (0.2183)	grad_norm 16087.7627 (17174.7305)	mem 4918MB
[2022-02-06 22:17:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][80/40036]	eta 1:44:46 lr 0.000001	time 0.1420 (0.1573)	loss 0.2197 (0.2183)	grad_norm 16792.6543 (17170.5059)	mem 4918MB
[2022-02-06 22:17:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][90/40036]	eta 1:43:40 lr 0.000001	time 0.1424 (0.1557)	loss 0.2215 (0.2183)	grad_norm 18654.1953 (17205.9941)	mem 4918MB
[2022-02-06 22:17:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][100/40036]	eta 1:42:51 lr 0.000001	time 0.1428 (0.1545)	loss 0.2161 (0.2182)	grad_norm 17053.7559 (17172.1152)	mem 4918MB
[2022-02-06 22:17:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][110/40036]	eta 1:42:07 lr 0.000001	time 0.1423 (0.1535)	loss 0.2211 (0.2183)	grad_norm 18645.9492 (17196.5938)	mem 4918MB
[2022-02-06 22:17:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][120/40036]	eta 1:41:27 lr 0.000001	time 0.1410 (0.1525)	loss 0.2202 (0.2182)	grad_norm 19485.2285 (17193.0234)	mem 4918MB
[2022-02-06 22:17:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][130/40036]	eta 1:40:57 lr 0.000001	time 0.1422 (0.1518)	loss 0.2174 (0.2183)	grad_norm 15232.2363 (17234.3809)	mem 4918MB
[2022-02-06 22:17:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][140/40036]	eta 1:40:29 lr 0.000001	time 0.1412 (0.1511)	loss 0.2182 (0.2183)	grad_norm 17221.1094 (17237.5508)	mem 4918MB
[2022-02-06 22:18:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][150/40036]	eta 1:40:03 lr 0.000001	time 0.1405 (0.1505)	loss 0.2202 (0.2184)	grad_norm 18903.7207 (17290.1172)	mem 4918MB
[2022-02-06 22:18:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][160/40036]	eta 1:39:44 lr 0.000001	time 0.1398 (0.1501)	loss 0.2171 (0.2183)	grad_norm 14054.5645 (17248.2363)	mem 4918MB
[2022-02-06 22:18:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][170/40036]	eta 1:39:24 lr 0.000001	time 0.1414 (0.1496)	loss 0.2187 (0.2184)	grad_norm 16310.9043 (17223.1406)	mem 4918MB
[2022-02-06 22:18:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][180/40036]	eta 1:39:06 lr 0.000001	time 0.1411 (0.1492)	loss 0.2163 (0.2184)	grad_norm 16229.2549 (17261.1660)	mem 4918MB
[2022-02-06 22:18:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][190/40036]	eta 1:38:50 lr 0.000001	time 0.1409 (0.1488)	loss 0.2183 (0.2185)	grad_norm 17279.6758 (17249.6133)	mem 4918MB
[2022-02-06 22:18:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][200/40036]	eta 1:38:38 lr 0.000001	time 0.1409 (0.1486)	loss 0.2159 (0.2184)	grad_norm 17244.0469 (17298.8418)	mem 4918MB
[2022-02-06 22:18:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][210/40036]	eta 1:38:25 lr 0.000001	time 0.1419 (0.1483)	loss 0.2196 (0.2185)	grad_norm 15051.9463 (17311.5586)	mem 4918MB
[2022-02-06 22:18:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][220/40036]	eta 1:38:13 lr 0.000001	time 0.1420 (0.1480)	loss 0.2155 (0.2185)	grad_norm 18008.8848 (17319.9004)	mem 4918MB
[2022-02-06 22:18:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][230/40036]	eta 1:38:04 lr 0.000001	time 0.1421 (0.1478)	loss 0.2194 (0.2185)	grad_norm 19917.0762 (17342.0215)	mem 4918MB
[2022-02-06 22:18:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][240/40036]	eta 1:37:53 lr 0.000001	time 0.1414 (0.1476)	loss 0.2147 (0.2184)	grad_norm 18249.7363 (17368.4727)	mem 4918MB
[2022-02-06 22:18:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][250/40036]	eta 1:37:43 lr 0.000001	time 0.1415 (0.1474)	loss 0.2188 (0.2185)	grad_norm 17095.1992 (17375.0957)	mem 4918MB
[2022-02-06 22:18:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][260/40036]	eta 1:37:37 lr 0.000001	time 0.1425 (0.1473)	loss 0.2149 (0.2185)	grad_norm 16952.4785 (17380.4941)	mem 4918MB
[2022-02-06 22:18:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][270/40036]	eta 1:37:29 lr 0.000001	time 0.1417 (0.1471)	loss 0.2192 (0.2184)	grad_norm 17074.3203 (17415.0645)	mem 4918MB
[2022-02-06 22:18:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][280/40036]	eta 1:37:22 lr 0.000001	time 0.1417 (0.1469)	loss 0.2209 (0.2185)	grad_norm 15760.6201 (17387.4512)	mem 4918MB
[2022-02-06 22:18:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][290/40036]	eta 1:37:16 lr 0.000001	time 0.1417 (0.1469)	loss 0.2159 (0.2185)	grad_norm 19417.6562 (17408.0234)	mem 4918MB
[2022-02-06 22:18:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][300/40036]	eta 1:37:15 lr 0.000001	time 0.1464 (0.1469)	loss 0.2206 (0.2185)	grad_norm 16298.0381 (17418.7480)	mem 4918MB
[2022-02-06 22:18:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][310/40036]	eta 1:37:11 lr 0.000001	time 0.1447 (0.1468)	loss 0.2178 (0.2184)	grad_norm 16944.0547 (17425.6035)	mem 4918MB
[2022-02-06 22:18:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][320/40036]	eta 1:37:08 lr 0.000001	time 0.1416 (0.1467)	loss 0.2166 (0.2184)	grad_norm 18806.4727 (17403.9434)	mem 4918MB
[2022-02-06 22:18:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][330/40036]	eta 1:37:04 lr 0.000001	time 0.1424 (0.1467)	loss 0.2214 (0.2184)	grad_norm 19656.9004 (17417.4902)	mem 4918MB
[2022-02-06 22:18:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][340/40036]	eta 1:37:00 lr 0.000001	time 0.1413 (0.1466)	loss 0.2197 (0.2185)	grad_norm 14248.6543 (17393.1875)	mem 4918MB
[2022-02-06 22:18:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][350/40036]	eta 1:36:55 lr 0.000001	time 0.1468 (0.1465)	loss 0.2179 (0.2184)	grad_norm 17567.8105 (17399.1016)	mem 4918MB
[2022-02-06 22:18:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][360/40036]	eta 1:36:57 lr 0.000001	time 0.1495 (0.1466)	loss 0.2193 (0.2184)	grad_norm 19383.4258 (17409.3477)	mem 4918MB
[2022-02-06 22:18:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][370/40036]	eta 1:36:57 lr 0.000001	time 0.1495 (0.1467)	loss 0.2157 (0.2184)	grad_norm 19209.8848 (17404.9570)	mem 4918MB
[2022-02-06 22:18:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][380/40036]	eta 1:36:55 lr 0.000001	time 0.1484 (0.1466)	loss 0.2220 (0.2184)	grad_norm 20094.6484 (17410.0781)	mem 4918MB
[2022-02-06 22:18:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][390/40036]	eta 1:36:52 lr 0.000001	time 0.1425 (0.1466)	loss 0.2190 (0.2184)	grad_norm 17400.4824 (17436.1309)	mem 4918MB
[2022-02-06 22:18:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][400/40036]	eta 1:36:47 lr 0.000001	time 0.1423 (0.1465)	loss 0.2189 (0.2184)	grad_norm 15947.6367 (17419.5586)	mem 4918MB
[2022-02-06 22:18:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][410/40036]	eta 1:36:42 lr 0.000001	time 0.1425 (0.1464)	loss 0.2174 (0.2184)	grad_norm 15352.5830 (17403.9746)	mem 4918MB
[2022-02-06 22:18:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][420/40036]	eta 1:36:39 lr 0.000002	time 0.1433 (0.1464)	loss 0.2186 (0.2184)	grad_norm 15954.9746 (17409.4102)	mem 4918MB
[2022-02-06 22:18:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][430/40036]	eta 1:36:35 lr 0.000002	time 0.1425 (0.1463)	loss 0.2192 (0.2184)	grad_norm 16831.1211 (17408.4336)	mem 4918MB
[2022-02-06 22:18:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][440/40036]	eta 1:36:30 lr 0.000002	time 0.1427 (0.1462)	loss 0.2197 (0.2184)	grad_norm 19033.2266 (17400.3359)	mem 4918MB
[2022-02-06 22:18:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][450/40036]	eta 1:36:27 lr 0.000002	time 0.1428 (0.1462)	loss 0.2157 (0.2184)	grad_norm 17847.9414 (17399.1934)	mem 4918MB
[2022-02-06 22:18:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][460/40036]	eta 1:36:23 lr 0.000002	time 0.1427 (0.1461)	loss 0.2149 (0.2184)	grad_norm 15715.3906 (17428.1172)	mem 4918MB
[2022-02-06 22:18:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][470/40036]	eta 1:36:19 lr 0.000002	time 0.1422 (0.1461)	loss 0.2203 (0.2184)	grad_norm 15462.1123 (17439.5234)	mem 4918MB
[2022-02-06 22:18:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][480/40036]	eta 1:36:17 lr 0.000002	time 0.1420 (0.1461)	loss 0.2190 (0.2184)	grad_norm 17864.4863 (17441.4590)	mem 4918MB
[2022-02-06 22:18:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][490/40036]	eta 1:36:13 lr 0.000002	time 0.1424 (0.1460)	loss 0.2172 (0.2184)	grad_norm 14570.7959 (17423.1035)	mem 4918MB
[2022-02-06 22:18:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][500/40036]	eta 1:36:10 lr 0.000002	time 0.1429 (0.1459)	loss 0.2186 (0.2184)	grad_norm 14745.3428 (17415.9141)	mem 4918MB
[2022-02-06 22:18:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][510/40036]	eta 1:36:06 lr 0.000002	time 0.1425 (0.1459)	loss 0.2179 (0.2184)	grad_norm 17682.1016 (17406.5586)	mem 4918MB
[2022-02-06 22:18:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][520/40036]	eta 1:36:04 lr 0.000002	time 0.1427 (0.1459)	loss 0.2189 (0.2184)	grad_norm 17927.9785 (17403.0957)	mem 4918MB
[2022-02-06 22:18:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][530/40036]	eta 1:36:01 lr 0.000002	time 0.1431 (0.1458)	loss 0.2151 (0.2184)	grad_norm 15647.1162 (17406.5625)	mem 4918MB
[2022-02-06 22:18:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][540/40036]	eta 1:35:58 lr 0.000002	time 0.1429 (0.1458)	loss 0.2182 (0.2184)	grad_norm 14912.3018 (17405.6719)	mem 4918MB
[2022-02-06 22:18:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][550/40036]	eta 1:35:56 lr 0.000002	time 0.1427 (0.1458)	loss 0.2168 (0.2184)	grad_norm 17342.0566 (17421.7441)	mem 4918MB
[2022-02-06 22:19:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][560/40036]	eta 1:35:53 lr 0.000002	time 0.1423 (0.1457)	loss 0.2187 (0.2184)	grad_norm 15858.8662 (17420.5566)	mem 4918MB
[2022-02-06 22:19:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][570/40036]	eta 1:35:49 lr 0.000002	time 0.1432 (0.1457)	loss 0.2187 (0.2184)	grad_norm 14989.2861 (17413.6367)	mem 4918MB
[2022-02-06 22:19:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][580/40036]	eta 1:35:48 lr 0.000002	time 0.1431 (0.1457)	loss 0.2184 (0.2184)	grad_norm 17113.5977 (17419.1816)	mem 4918MB
[2022-02-06 22:19:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][590/40036]	eta 1:35:45 lr 0.000002	time 0.1421 (0.1456)	loss 0.2176 (0.2184)	grad_norm 15560.2354 (17413.1445)	mem 4918MB
[2022-02-06 22:19:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][600/40036]	eta 1:35:42 lr 0.000002	time 0.1420 (0.1456)	loss 0.2181 (0.2184)	grad_norm 19005.5508 (17410.6758)	mem 4918MB
[2022-02-06 22:19:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][610/40036]	eta 1:35:40 lr 0.000002	time 0.1427 (0.1456)	loss 0.2186 (0.2184)	grad_norm 16323.9775 (17410.6035)	mem 4918MB
[2022-02-06 22:19:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][620/40036]	eta 1:35:37 lr 0.000002	time 0.1431 (0.1456)	loss 0.2183 (0.2184)	grad_norm 17566.1621 (17404.0605)	mem 4918MB
[2022-02-06 22:19:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][630/40036]	eta 1:35:35 lr 0.000002	time 0.1432 (0.1455)	loss 0.2209 (0.2184)	grad_norm 15640.1885 (17400.5449)	mem 4918MB
[2022-02-06 22:19:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][640/40036]	eta 1:35:33 lr 0.000002	time 0.1427 (0.1455)	loss 0.2211 (0.2184)	grad_norm 19662.5859 (17406.4414)	mem 4918MB
[2022-02-06 22:19:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][650/40036]	eta 1:35:30 lr 0.000002	time 0.1430 (0.1455)	loss 0.2171 (0.2184)	grad_norm 18091.8555 (17411.0898)	mem 4918MB
[2022-02-06 22:19:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][660/40036]	eta 1:35:28 lr 0.000002	time 0.1435 (0.1455)	loss 0.2183 (0.2184)	grad_norm 19842.8828 (17425.9961)	mem 4918MB
[2022-02-06 22:19:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][670/40036]	eta 1:35:26 lr 0.000002	time 0.1436 (0.1455)	loss 0.2146 (0.2184)	grad_norm 18579.8945 (17412.5645)	mem 4918MB
[2022-02-06 22:19:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][680/40036]	eta 1:35:24 lr 0.000002	time 0.1426 (0.1455)	loss 0.2189 (0.2184)	grad_norm 17139.2910 (17415.6211)	mem 4918MB
[2022-02-06 22:19:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][690/40036]	eta 1:35:22 lr 0.000002	time 0.1435 (0.1454)	loss 0.2167 (0.2184)	grad_norm 18293.8066 (17426.5703)	mem 4918MB
[2022-02-06 22:19:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][700/40036]	eta 1:35:19 lr 0.000002	time 0.1447 (0.1454)	loss 0.2198 (0.2184)	grad_norm 15957.9580 (17425.6445)	mem 4918MB
[2022-02-06 22:19:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][710/40036]	eta 1:35:18 lr 0.000002	time 0.1436 (0.1454)	loss 0.2174 (0.2184)	grad_norm 19307.9336 (17424.6445)	mem 4918MB
[2022-02-06 22:19:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][720/40036]	eta 1:35:16 lr 0.000002	time 0.1431 (0.1454)	loss 0.2207 (0.2184)	grad_norm 19606.6855 (17435.7051)	mem 4918MB
[2022-02-06 22:19:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][730/40036]	eta 1:35:13 lr 0.000002	time 0.1431 (0.1454)	loss 0.2167 (0.2184)	grad_norm 14540.9121 (17426.7344)	mem 4918MB
[2022-02-06 22:19:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][740/40036]	eta 1:35:12 lr 0.000002	time 0.1433 (0.1454)	loss 0.2175 (0.2184)	grad_norm 16931.1621 (17416.9785)	mem 4918MB
[2022-02-06 22:19:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][750/40036]	eta 1:35:09 lr 0.000002	time 0.1432 (0.1453)	loss 0.2156 (0.2184)	grad_norm 16183.7793 (17415.8750)	mem 4918MB
[2022-02-06 22:19:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][760/40036]	eta 1:35:07 lr 0.000002	time 0.1432 (0.1453)	loss 0.2177 (0.2184)	grad_norm 17093.6133 (17421.1406)	mem 4918MB
[2022-02-06 22:19:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][770/40036]	eta 1:35:06 lr 0.000002	time 0.1439 (0.1453)	loss 0.2193 (0.2184)	grad_norm 18143.3770 (17418.4453)	mem 4918MB
[2022-02-06 22:19:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][780/40036]	eta 1:35:04 lr 0.000002	time 0.1439 (0.1453)	loss 0.2149 (0.2184)	grad_norm 16885.6172 (17414.6602)	mem 4918MB
[2022-02-06 22:19:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][790/40036]	eta 1:35:02 lr 0.000002	time 0.1441 (0.1453)	loss 0.2184 (0.2184)	grad_norm 17148.6074 (17408.3145)	mem 4918MB
[2022-02-06 22:19:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][800/40036]	eta 1:35:01 lr 0.000002	time 0.1433 (0.1453)	loss 0.2184 (0.2184)	grad_norm 18835.9102 (17408.1660)	mem 4918MB
[2022-02-06 22:19:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][810/40036]	eta 1:34:59 lr 0.000002	time 0.1437 (0.1453)	loss 0.2162 (0.2184)	grad_norm 19445.9805 (17418.6465)	mem 4918MB
[2022-02-06 22:19:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][820/40036]	eta 1:34:57 lr 0.000002	time 0.1439 (0.1453)	loss 0.2183 (0.2184)	grad_norm 17017.4102 (17413.7598)	mem 4918MB
[2022-02-06 22:19:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][830/40036]	eta 1:34:55 lr 0.000002	time 0.1441 (0.1453)	loss 0.2194 (0.2184)	grad_norm 18072.4863 (17416.8242)	mem 4918MB
[2022-02-06 22:19:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][840/40036]	eta 1:34:54 lr 0.000002	time 0.1440 (0.1453)	loss 0.2154 (0.2184)	grad_norm 16676.3203 (17429.1074)	mem 4918MB
[2022-02-06 22:19:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][850/40036]	eta 1:34:52 lr 0.000002	time 0.1435 (0.1453)	loss 0.2186 (0.2184)	grad_norm 16869.6133 (17434.6055)	mem 4918MB
[2022-02-06 22:19:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][860/40036]	eta 1:34:50 lr 0.000002	time 0.1430 (0.1453)	loss 0.2168 (0.2184)	grad_norm 14985.8438 (17431.9668)	mem 4918MB
[2022-02-06 22:19:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][870/40036]	eta 1:34:49 lr 0.000002	time 0.1432 (0.1453)	loss 0.2171 (0.2184)	grad_norm 18087.8594 (17429.1777)	mem 4918MB
[2022-02-06 22:19:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][880/40036]	eta 1:34:47 lr 0.000002	time 0.1440 (0.1453)	loss 0.2196 (0.2184)	grad_norm 19133.7168 (17437.6758)	mem 4918MB
[2022-02-06 22:19:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][890/40036]	eta 1:34:45 lr 0.000002	time 0.1439 (0.1453)	loss 0.2210 (0.2184)	grad_norm 19238.1289 (17439.1934)	mem 4918MB
[2022-02-06 22:19:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][900/40036]	eta 1:34:44 lr 0.000002	time 0.1444 (0.1453)	loss 0.2198 (0.2184)	grad_norm 14904.9697 (17437.9238)	mem 4918MB
[2022-02-06 22:19:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][910/40036]	eta 1:34:43 lr 0.000002	time 0.1441 (0.1453)	loss 0.2177 (0.2184)	grad_norm 18744.1562 (17440.4395)	mem 4918MB
[2022-02-06 22:19:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][920/40036]	eta 1:34:41 lr 0.000002	time 0.1437 (0.1452)	loss 0.2179 (0.2184)	grad_norm 17336.2188 (17440.5430)	mem 4918MB
[2022-02-06 22:19:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][930/40036]	eta 1:34:40 lr 0.000002	time 0.1444 (0.1452)	loss 0.2180 (0.2184)	grad_norm 16053.7861 (17434.9473)	mem 4918MB
[2022-02-06 22:19:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][940/40036]	eta 1:34:38 lr 0.000002	time 0.1440 (0.1452)	loss 0.2167 (0.2184)	grad_norm 18271.1270 (17443.0137)	mem 4918MB
[2022-02-06 22:19:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][950/40036]	eta 1:34:36 lr 0.000002	time 0.1440 (0.1452)	loss 0.2198 (0.2184)	grad_norm 17650.8496 (17453.0723)	mem 4918MB
[2022-02-06 22:19:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][960/40036]	eta 1:34:35 lr 0.000002	time 0.1431 (0.1452)	loss 0.2177 (0.2184)	grad_norm 14504.3125 (17450.5996)	mem 4918MB
[2022-02-06 22:19:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][970/40036]	eta 1:34:33 lr 0.000002	time 0.1441 (0.1452)	loss 0.2186 (0.2184)	grad_norm 16558.5234 (17437.8945)	mem 4918MB
[2022-02-06 22:20:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][980/40036]	eta 1:34:32 lr 0.000002	time 0.1444 (0.1452)	loss 0.2172 (0.2184)	grad_norm 15333.1455 (17438.5176)	mem 4918MB
[2022-02-06 22:20:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][990/40036]	eta 1:34:30 lr 0.000002	time 0.1441 (0.1452)	loss 0.2154 (0.2184)	grad_norm 16736.1250 (17440.6621)	mem 4918MB
[2022-02-06 22:20:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1000/40036]	eta 1:34:29 lr 0.000002	time 0.1439 (0.1452)	loss 0.2176 (0.2184)	grad_norm 17317.7539 (17438.6719)	mem 4918MB
[2022-02-06 22:20:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1010/40036]	eta 1:34:27 lr 0.000002	time 0.1439 (0.1452)	loss 0.2198 (0.2184)	grad_norm 15761.2949 (17433.5312)	mem 4918MB
[2022-02-06 22:20:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1020/40036]	eta 1:34:25 lr 0.000002	time 0.1448 (0.1452)	loss 0.2182 (0.2184)	grad_norm 16407.5547 (17441.0488)	mem 4918MB
[2022-02-06 22:20:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1030/40036]	eta 1:34:24 lr 0.000002	time 0.1439 (0.1452)	loss 0.2175 (0.2184)	grad_norm 14781.7383 (17444.6875)	mem 4918MB
[2022-02-06 22:20:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1040/40036]	eta 1:34:22 lr 0.000002	time 0.1441 (0.1452)	loss 0.2178 (0.2184)	grad_norm 17590.3906 (17444.0879)	mem 4918MB
[2022-02-06 22:20:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1050/40036]	eta 1:34:21 lr 0.000002	time 0.1439 (0.1452)	loss 0.2190 (0.2184)	grad_norm 18071.7773 (17445.0039)	mem 4918MB
[2022-02-06 22:20:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1060/40036]	eta 1:34:20 lr 0.000002	time 0.1450 (0.1452)	loss 0.2163 (0.2184)	grad_norm 19219.2812 (17438.3320)	mem 4918MB
[2022-02-06 22:20:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1070/40036]	eta 1:34:18 lr 0.000002	time 0.1441 (0.1452)	loss 0.2216 (0.2184)	grad_norm 15804.7539 (17437.3418)	mem 4918MB
[2022-02-06 22:20:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1080/40036]	eta 1:34:16 lr 0.000002	time 0.1440 (0.1452)	loss 0.2160 (0.2184)	grad_norm 19611.1641 (17435.0645)	mem 4918MB
[2022-02-06 22:20:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1090/40036]	eta 1:34:15 lr 0.000002	time 0.1441 (0.1452)	loss 0.2173 (0.2184)	grad_norm 16920.6289 (17441.8340)	mem 4918MB
[2022-02-06 22:20:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1100/40036]	eta 1:34:13 lr 0.000002	time 0.1446 (0.1452)	loss 0.2216 (0.2184)	grad_norm 16018.5244 (17444.8379)	mem 4918MB
[2022-02-06 22:20:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1110/40036]	eta 1:34:12 lr 0.000002	time 0.1441 (0.1452)	loss 0.2205 (0.2184)	grad_norm 16908.7090 (17441.2773)	mem 4918MB
[2022-02-06 22:20:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1120/40036]	eta 1:34:11 lr 0.000002	time 0.1428 (0.1452)	loss 0.2195 (0.2184)	grad_norm 18818.2910 (17435.6680)	mem 4918MB
[2022-02-06 22:20:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1130/40036]	eta 1:34:09 lr 0.000002	time 0.1435 (0.1452)	loss 0.2200 (0.2184)	grad_norm 18969.8945 (17431.5371)	mem 4918MB
[2022-02-06 22:20:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1140/40036]	eta 1:34:07 lr 0.000002	time 0.1437 (0.1452)	loss 0.2167 (0.2184)	grad_norm 18795.1934 (17430.3789)	mem 4918MB
[2022-02-06 22:20:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1150/40036]	eta 1:34:06 lr 0.000002	time 0.1443 (0.1452)	loss 0.2157 (0.2184)	grad_norm 16582.4473 (17427.4941)	mem 4918MB
[2022-02-06 22:20:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1160/40036]	eta 1:34:04 lr 0.000002	time 0.1443 (0.1452)	loss 0.2181 (0.2184)	grad_norm 17016.0762 (17426.0742)	mem 4918MB
[2022-02-06 22:20:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1170/40036]	eta 1:34:03 lr 0.000002	time 0.1443 (0.1452)	loss 0.2170 (0.2184)	grad_norm 15962.7197 (17425.2539)	mem 4918MB
[2022-02-06 22:20:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1180/40036]	eta 1:34:01 lr 0.000002	time 0.1432 (0.1452)	loss 0.2190 (0.2184)	grad_norm 16804.7695 (17414.8750)	mem 4918MB
[2022-02-06 22:20:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1190/40036]	eta 1:34:00 lr 0.000002	time 0.1444 (0.1452)	loss 0.2173 (0.2184)	grad_norm 18578.6055 (17407.8438)	mem 4918MB
[2022-02-06 22:20:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1200/40036]	eta 1:33:58 lr 0.000002	time 0.1444 (0.1452)	loss 0.2193 (0.2184)	grad_norm 16184.4033 (17403.9512)	mem 4918MB
[2022-02-06 22:20:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1210/40036]	eta 1:33:57 lr 0.000002	time 0.1442 (0.1452)	loss 0.2205 (0.2184)	grad_norm 19292.3535 (17407.9336)	mem 4918MB
[2022-02-06 22:20:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1220/40036]	eta 1:33:56 lr 0.000003	time 0.1447 (0.1452)	loss 0.2160 (0.2184)	grad_norm 20448.0586 (17414.0078)	mem 4918MB
[2022-02-06 22:20:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1230/40036]	eta 1:33:54 lr 0.000003	time 0.1445 (0.1452)	loss 0.2195 (0.2184)	grad_norm 16582.1777 (17409.1953)	mem 4918MB
[2022-02-06 22:20:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1240/40036]	eta 1:33:53 lr 0.000003	time 0.1440 (0.1452)	loss 0.2186 (0.2184)	grad_norm 18055.2910 (17408.5762)	mem 4918MB
[2022-02-06 22:20:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1250/40036]	eta 1:33:51 lr 0.000003	time 0.1438 (0.1452)	loss 0.2156 (0.2184)	grad_norm 17193.5215 (17407.7227)	mem 4918MB
[2022-02-06 22:20:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1260/40036]	eta 1:33:50 lr 0.000003	time 0.1445 (0.1452)	loss 0.2210 (0.2184)	grad_norm 17315.5469 (17403.8828)	mem 4918MB
[2022-02-06 22:20:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1270/40036]	eta 1:33:48 lr 0.000003	time 0.1441 (0.1452)	loss 0.2191 (0.2184)	grad_norm 19741.0859 (17405.2500)	mem 4918MB
[2022-02-06 22:20:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1280/40036]	eta 1:33:47 lr 0.000003	time 0.1433 (0.1452)	loss 0.2186 (0.2184)	grad_norm 17759.7246 (17401.3945)	mem 4918MB
[2022-02-06 22:20:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1290/40036]	eta 1:33:46 lr 0.000003	time 0.1461 (0.1452)	loss 0.2188 (0.2184)	grad_norm 17148.8477 (17394.1113)	mem 4918MB
[2022-02-06 22:20:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1300/40036]	eta 1:33:44 lr 0.000003	time 0.1440 (0.1452)	loss 0.2192 (0.2184)	grad_norm 17085.7812 (17392.5664)	mem 4918MB
[2022-02-06 22:20:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1310/40036]	eta 1:33:43 lr 0.000003	time 0.1447 (0.1452)	loss 0.2197 (0.2184)	grad_norm 18260.7656 (17396.9883)	mem 4918MB
[2022-02-06 22:20:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1320/40036]	eta 1:33:41 lr 0.000003	time 0.1440 (0.1452)	loss 0.2170 (0.2184)	grad_norm 18958.0430 (17399.5527)	mem 4918MB
[2022-02-06 22:20:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1330/40036]	eta 1:33:40 lr 0.000003	time 0.1442 (0.1452)	loss 0.2187 (0.2184)	grad_norm 17307.5410 (17400.6270)	mem 4918MB
[2022-02-06 22:20:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1340/40036]	eta 1:33:38 lr 0.000003	time 0.1449 (0.1452)	loss 0.2168 (0.2184)	grad_norm 17190.3418 (17394.8027)	mem 4918MB
[2022-02-06 22:20:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1350/40036]	eta 1:33:37 lr 0.000003	time 0.1440 (0.1452)	loss 0.2175 (0.2184)	grad_norm 16464.0293 (17393.5020)	mem 4918MB
[2022-02-06 22:20:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1360/40036]	eta 1:33:35 lr 0.000003	time 0.1445 (0.1452)	loss 0.2207 (0.2184)	grad_norm 14770.6416 (17389.2344)	mem 4918MB
[2022-02-06 22:20:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1370/40036]	eta 1:33:34 lr 0.000003	time 0.1444 (0.1452)	loss 0.2139 (0.2184)	grad_norm 15057.1875 (17378.1484)	mem 4918MB
[2022-02-06 22:20:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1380/40036]	eta 1:33:33 lr 0.000003	time 0.1442 (0.1452)	loss 0.2207 (0.2184)	grad_norm 15669.0137 (17373.0977)	mem 4918MB
[2022-02-06 22:21:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1390/40036]	eta 1:33:31 lr 0.000003	time 0.1444 (0.1452)	loss 0.2166 (0.2184)	grad_norm 18160.5273 (17374.4512)	mem 4918MB
[2022-02-06 22:21:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1400/40036]	eta 1:33:30 lr 0.000003	time 0.1444 (0.1452)	loss 0.2150 (0.2184)	grad_norm 16079.8926 (17372.4824)	mem 4918MB
[2022-02-06 22:21:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1410/40036]	eta 1:33:29 lr 0.000003	time 0.1434 (0.1452)	loss 0.2212 (0.2184)	grad_norm 17684.3984 (17367.9883)	mem 4918MB
[2022-02-06 22:21:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1420/40036]	eta 1:33:27 lr 0.000003	time 0.1444 (0.1452)	loss 0.2164 (0.2184)	grad_norm 16139.6152 (17366.0879)	mem 4918MB
[2022-02-06 22:21:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1430/40036]	eta 1:33:25 lr 0.000003	time 0.1442 (0.1452)	loss 0.2190 (0.2184)	grad_norm 17561.1289 (17367.9258)	mem 4918MB
[2022-02-06 22:21:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1440/40036]	eta 1:33:24 lr 0.000003	time 0.1437 (0.1452)	loss 0.2169 (0.2184)	grad_norm 15691.6611 (17364.9297)	mem 4918MB
[2022-02-06 22:21:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1450/40036]	eta 1:33:23 lr 0.000003	time 0.1449 (0.1452)	loss 0.2184 (0.2184)	grad_norm 16716.7285 (17365.2871)	mem 4918MB
[2022-02-06 22:21:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1460/40036]	eta 1:33:21 lr 0.000003	time 0.1439 (0.1452)	loss 0.2187 (0.2184)	grad_norm 15076.7480 (17364.2578)	mem 4918MB
[2022-02-06 22:21:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1470/40036]	eta 1:33:20 lr 0.000003	time 0.1453 (0.1452)	loss 0.2194 (0.2184)	grad_norm 20608.3945 (17361.6172)	mem 4918MB
[2022-02-06 22:21:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1480/40036]	eta 1:33:18 lr 0.000003	time 0.1444 (0.1452)	loss 0.2179 (0.2184)	grad_norm 16454.5664 (17362.1699)	mem 4918MB
[2022-02-06 22:21:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1490/40036]	eta 1:33:17 lr 0.000003	time 0.1437 (0.1452)	loss 0.2166 (0.2184)	grad_norm 17686.9023 (17366.8516)	mem 4918MB
[2022-02-06 22:21:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1500/40036]	eta 1:33:15 lr 0.000003	time 0.1450 (0.1452)	loss 0.2213 (0.2184)	grad_norm 16993.0176 (17366.6074)	mem 4918MB
[2022-02-06 22:21:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1510/40036]	eta 1:33:14 lr 0.000003	time 0.1444 (0.1452)	loss 0.2192 (0.2184)	grad_norm 15575.6484 (17360.3379)	mem 4918MB
[2022-02-06 22:21:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1520/40036]	eta 1:33:13 lr 0.000003	time 0.1441 (0.1452)	loss 0.2167 (0.2184)	grad_norm 18086.5996 (17357.7637)	mem 4918MB
[2022-02-06 22:21:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1530/40036]	eta 1:33:11 lr 0.000003	time 0.1450 (0.1452)	loss 0.2202 (0.2184)	grad_norm 20220.1250 (17358.4336)	mem 4918MB
[2022-02-06 22:21:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1540/40036]	eta 1:33:10 lr 0.000003	time 0.1437 (0.1452)	loss 0.2190 (0.2184)	grad_norm 18943.3555 (17353.3496)	mem 4918MB
[2022-02-06 22:21:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1550/40036]	eta 1:33:08 lr 0.000003	time 0.1441 (0.1452)	loss 0.2141 (0.2184)	grad_norm 18301.0469 (17354.9219)	mem 4918MB
[2022-02-06 22:21:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1560/40036]	eta 1:33:07 lr 0.000003	time 0.1448 (0.1452)	loss 0.2169 (0.2184)	grad_norm 19490.7109 (17353.7715)	mem 4918MB
[2022-02-06 22:21:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1570/40036]	eta 1:33:06 lr 0.000003	time 0.1447 (0.1452)	loss 0.2178 (0.2184)	grad_norm 16943.2695 (17358.6191)	mem 4918MB
[2022-02-06 22:21:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1580/40036]	eta 1:33:04 lr 0.000003	time 0.1448 (0.1452)	loss 0.2183 (0.2184)	grad_norm 17507.2930 (17357.5469)	mem 4918MB
[2022-02-06 22:21:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1590/40036]	eta 1:33:03 lr 0.000003	time 0.1443 (0.1452)	loss 0.2183 (0.2184)	grad_norm 17517.7344 (17356.4746)	mem 4918MB
[2022-02-06 22:21:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1600/40036]	eta 1:33:02 lr 0.000003	time 0.1435 (0.1452)	loss 0.2194 (0.2184)	grad_norm 16569.2070 (17349.0742)	mem 4918MB
[2022-02-06 22:21:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1610/40036]	eta 1:33:00 lr 0.000003	time 0.1440 (0.1452)	loss 0.2184 (0.2184)	grad_norm 16701.1504 (17347.5215)	mem 4918MB
[2022-02-06 22:21:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1620/40036]	eta 1:32:59 lr 0.000003	time 0.1447 (0.1452)	loss 0.2202 (0.2184)	grad_norm 17097.7285 (17349.3672)	mem 4918MB
[2022-02-06 22:21:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1630/40036]	eta 1:32:58 lr 0.000003	time 0.1453 (0.1452)	loss 0.2182 (0.2184)	grad_norm 15752.7041 (17349.8867)	mem 4918MB
[2022-02-06 22:21:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1640/40036]	eta 1:32:56 lr 0.000003	time 0.1449 (0.1452)	loss 0.2213 (0.2184)	grad_norm 18165.4805 (17349.0781)	mem 4918MB
[2022-02-06 22:21:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1650/40036]	eta 1:32:55 lr 0.000003	time 0.1447 (0.1452)	loss 0.2187 (0.2184)	grad_norm 19907.3965 (17355.7363)	mem 4918MB
[2022-02-06 22:21:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1660/40036]	eta 1:32:54 lr 0.000003	time 0.1444 (0.1452)	loss 0.2216 (0.2184)	grad_norm 18475.2578 (17358.0176)	mem 4918MB
[2022-02-06 22:21:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1670/40036]	eta 1:32:52 lr 0.000003	time 0.1450 (0.1453)	loss 0.2176 (0.2184)	grad_norm 14951.8301 (17361.0977)	mem 4918MB
[2022-02-06 22:21:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1680/40036]	eta 1:32:51 lr 0.000003	time 0.1447 (0.1453)	loss 0.2201 (0.2184)	grad_norm 15998.4014 (17366.4277)	mem 4918MB
[2022-02-06 22:21:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1690/40036]	eta 1:32:50 lr 0.000003	time 0.1440 (0.1453)	loss 0.2201 (0.2184)	grad_norm 16780.7402 (17366.2812)	mem 4918MB
[2022-02-06 22:21:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1700/40036]	eta 1:32:48 lr 0.000003	time 0.1439 (0.1453)	loss 0.2184 (0.2184)	grad_norm 17239.3145 (17363.5254)	mem 4918MB
[2022-02-06 22:21:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1710/40036]	eta 1:32:47 lr 0.000003	time 0.1444 (0.1453)	loss 0.2154 (0.2184)	grad_norm 17843.3184 (17365.7148)	mem 4918MB
[2022-02-06 22:21:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1720/40036]	eta 1:32:45 lr 0.000003	time 0.1439 (0.1453)	loss 0.2181 (0.2184)	grad_norm 19169.5918 (17363.5742)	mem 4918MB
[2022-02-06 22:21:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1730/40036]	eta 1:32:44 lr 0.000003	time 0.1441 (0.1453)	loss 0.2201 (0.2184)	grad_norm 14808.6943 (17357.9355)	mem 4918MB
[2022-02-06 22:21:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1740/40036]	eta 1:32:43 lr 0.000003	time 0.1441 (0.1453)	loss 0.2159 (0.2184)	grad_norm 17489.4199 (17358.7207)	mem 4918MB
[2022-02-06 22:21:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1750/40036]	eta 1:32:41 lr 0.000003	time 0.1443 (0.1453)	loss 0.2171 (0.2184)	grad_norm 17502.1797 (17359.3789)	mem 4918MB
[2022-02-06 22:21:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1760/40036]	eta 1:32:40 lr 0.000003	time 0.1435 (0.1453)	loss 0.2169 (0.2184)	grad_norm 16981.4492 (17360.0312)	mem 4918MB
[2022-02-06 22:21:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1770/40036]	eta 1:32:39 lr 0.000003	time 0.1435 (0.1453)	loss 0.2207 (0.2184)	grad_norm 16095.9600 (17356.5762)	mem 4918MB
[2022-02-06 22:21:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1780/40036]	eta 1:32:37 lr 0.000003	time 0.1444 (0.1453)	loss 0.2179 (0.2184)	grad_norm 16608.5566 (17353.4199)	mem 4918MB
[2022-02-06 22:21:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1790/40036]	eta 1:32:36 lr 0.000003	time 0.1456 (0.1453)	loss 0.2205 (0.2184)	grad_norm 16769.0781 (17354.1777)	mem 4918MB
[2022-02-06 22:22:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1800/40036]	eta 1:32:34 lr 0.000003	time 0.1453 (0.1453)	loss 0.2211 (0.2184)	grad_norm 18188.8691 (17355.1465)	mem 4918MB
[2022-02-06 22:22:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1810/40036]	eta 1:32:33 lr 0.000003	time 0.1448 (0.1453)	loss 0.2205 (0.2184)	grad_norm 19407.5293 (17354.3652)	mem 4918MB
[2022-02-06 22:22:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1820/40036]	eta 1:32:32 lr 0.000003	time 0.1448 (0.1453)	loss 0.2167 (0.2184)	grad_norm 15397.7695 (17351.3262)	mem 4918MB
[2022-02-06 22:22:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1830/40036]	eta 1:32:30 lr 0.000003	time 0.1444 (0.1453)	loss 0.2170 (0.2184)	grad_norm 17158.1094 (17349.7812)	mem 4918MB
[2022-02-06 22:22:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1840/40036]	eta 1:32:29 lr 0.000003	time 0.1443 (0.1453)	loss 0.2203 (0.2184)	grad_norm 20054.5352 (17349.7129)	mem 4918MB
[2022-02-06 22:22:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1850/40036]	eta 1:32:28 lr 0.000003	time 0.1440 (0.1453)	loss 0.2206 (0.2184)	grad_norm 19484.0098 (17349.6699)	mem 4918MB
[2022-02-06 22:22:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1860/40036]	eta 1:32:26 lr 0.000003	time 0.1448 (0.1453)	loss 0.2190 (0.2184)	grad_norm 17593.0605 (17350.3867)	mem 4918MB
[2022-02-06 22:22:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1870/40036]	eta 1:32:25 lr 0.000003	time 0.1450 (0.1453)	loss 0.2176 (0.2184)	grad_norm 16893.2773 (17352.8320)	mem 4918MB
[2022-02-06 22:22:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1880/40036]	eta 1:32:24 lr 0.000003	time 0.1449 (0.1453)	loss 0.2212 (0.2184)	grad_norm 16799.5410 (17350.4473)	mem 4918MB
[2022-02-06 22:22:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1890/40036]	eta 1:32:22 lr 0.000003	time 0.1446 (0.1453)	loss 0.2180 (0.2184)	grad_norm 18508.0039 (17352.8438)	mem 4918MB
[2022-02-06 22:22:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1900/40036]	eta 1:32:21 lr 0.000003	time 0.1444 (0.1453)	loss 0.2191 (0.2184)	grad_norm 17931.8809 (17355.6758)	mem 4918MB
[2022-02-06 22:22:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1910/40036]	eta 1:32:20 lr 0.000003	time 0.1443 (0.1453)	loss 0.2169 (0.2184)	grad_norm 14254.3965 (17349.7578)	mem 4918MB
[2022-02-06 22:22:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1920/40036]	eta 1:32:18 lr 0.000003	time 0.1436 (0.1453)	loss 0.2180 (0.2184)	grad_norm 18869.9883 (17354.4023)	mem 4918MB
[2022-02-06 22:22:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1930/40036]	eta 1:32:17 lr 0.000003	time 0.1446 (0.1453)	loss 0.2188 (0.2184)	grad_norm 17526.8457 (17357.6348)	mem 4918MB
[2022-02-06 22:22:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1940/40036]	eta 1:32:16 lr 0.000003	time 0.1449 (0.1453)	loss 0.2158 (0.2184)	grad_norm 16901.1953 (17358.4414)	mem 4918MB
[2022-02-06 22:22:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1950/40036]	eta 1:32:14 lr 0.000003	time 0.1444 (0.1453)	loss 0.2201 (0.2184)	grad_norm 17679.8867 (17355.7812)	mem 4918MB
[2022-02-06 22:22:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1960/40036]	eta 1:32:13 lr 0.000003	time 0.1450 (0.1453)	loss 0.2219 (0.2184)	grad_norm 16772.4727 (17355.0820)	mem 4918MB
[2022-02-06 22:22:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1970/40036]	eta 1:32:12 lr 0.000003	time 0.1442 (0.1453)	loss 0.2190 (0.2184)	grad_norm 17352.5996 (17356.8770)	mem 4918MB
[2022-02-06 22:22:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1980/40036]	eta 1:32:10 lr 0.000003	time 0.1439 (0.1453)	loss 0.2172 (0.2184)	grad_norm 20201.5605 (17358.5801)	mem 4918MB
[2022-02-06 22:22:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][1990/40036]	eta 1:32:09 lr 0.000003	time 0.1451 (0.1453)	loss 0.2195 (0.2184)	grad_norm 17835.8027 (17362.5488)	mem 4918MB
[2022-02-06 22:22:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2000/40036]	eta 1:32:08 lr 0.000003	time 0.1444 (0.1453)	loss 0.2196 (0.2184)	grad_norm 16383.4346 (17363.6191)	mem 4918MB
[2022-02-06 22:22:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2010/40036]	eta 1:32:06 lr 0.000003	time 0.1443 (0.1453)	loss 0.2181 (0.2184)	grad_norm 16310.6230 (17358.9453)	mem 4918MB
[2022-02-06 22:22:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2020/40036]	eta 1:32:05 lr 0.000004	time 0.1446 (0.1453)	loss 0.2162 (0.2184)	grad_norm 18059.2109 (17357.3809)	mem 4918MB
[2022-02-06 22:22:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2030/40036]	eta 1:32:04 lr 0.000004	time 0.1466 (0.1453)	loss 0.2168 (0.2184)	grad_norm 15594.5361 (17351.7441)	mem 4918MB
[2022-02-06 22:22:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2040/40036]	eta 1:32:02 lr 0.000004	time 0.1455 (0.1453)	loss 0.2177 (0.2184)	grad_norm 14764.3750 (17350.5195)	mem 4918MB
[2022-02-06 22:22:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2050/40036]	eta 1:32:01 lr 0.000004	time 0.1452 (0.1454)	loss 0.2211 (0.2184)	grad_norm 18807.6211 (17351.5020)	mem 4918MB
[2022-02-06 22:22:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2060/40036]	eta 1:32:00 lr 0.000004	time 0.1445 (0.1454)	loss 0.2193 (0.2184)	grad_norm 16080.0557 (17354.2910)	mem 4918MB
[2022-02-06 22:22:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2070/40036]	eta 1:31:58 lr 0.000004	time 0.1443 (0.1454)	loss 0.2187 (0.2184)	grad_norm 15354.1797 (17354.1445)	mem 4918MB
[2022-02-06 22:22:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2080/40036]	eta 1:31:57 lr 0.000004	time 0.1449 (0.1454)	loss 0.2178 (0.2184)	grad_norm 18522.8555 (17351.6387)	mem 4918MB
[2022-02-06 22:22:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2090/40036]	eta 1:31:55 lr 0.000004	time 0.1451 (0.1454)	loss 0.2188 (0.2184)	grad_norm 16323.5723 (17346.9531)	mem 4918MB
[2022-02-06 22:22:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2100/40036]	eta 1:31:54 lr 0.000004	time 0.1450 (0.1454)	loss 0.2180 (0.2184)	grad_norm 18722.0391 (17352.0742)	mem 4918MB
[2022-02-06 22:22:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2110/40036]	eta 1:31:52 lr 0.000004	time 0.1452 (0.1454)	loss 0.2189 (0.2184)	grad_norm 15465.5244 (17353.0410)	mem 4918MB
[2022-02-06 22:22:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2120/40036]	eta 1:31:51 lr 0.000004	time 0.1448 (0.1454)	loss 0.2198 (0.2184)	grad_norm 15705.6553 (17351.1348)	mem 4918MB
[2022-02-06 22:22:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2130/40036]	eta 1:31:50 lr 0.000004	time 0.1446 (0.1454)	loss 0.2203 (0.2184)	grad_norm 18048.4922 (17347.9551)	mem 4918MB
[2022-02-06 22:22:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2140/40036]	eta 1:31:48 lr 0.000004	time 0.1451 (0.1454)	loss 0.2221 (0.2184)	grad_norm 19467.7988 (17345.5742)	mem 4918MB
[2022-02-06 22:22:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2150/40036]	eta 1:31:47 lr 0.000004	time 0.1454 (0.1454)	loss 0.2191 (0.2184)	grad_norm 17248.9141 (17343.7598)	mem 4918MB
[2022-02-06 22:22:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2160/40036]	eta 1:31:46 lr 0.000004	time 0.1446 (0.1454)	loss 0.2235 (0.2184)	grad_norm 16822.7422 (17340.8184)	mem 4918MB
[2022-02-06 22:22:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2170/40036]	eta 1:31:44 lr 0.000004	time 0.1448 (0.1454)	loss 0.2181 (0.2184)	grad_norm 17130.5117 (17341.6543)	mem 4918MB
[2022-02-06 22:22:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2180/40036]	eta 1:31:43 lr 0.000004	time 0.1457 (0.1454)	loss 0.2181 (0.2184)	grad_norm 19725.0586 (17342.8203)	mem 4918MB
[2022-02-06 22:22:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2190/40036]	eta 1:31:42 lr 0.000004	time 0.1453 (0.1454)	loss 0.2168 (0.2184)	grad_norm 16016.1230 (17339.9473)	mem 4918MB
[2022-02-06 22:22:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2200/40036]	eta 1:31:40 lr 0.000004	time 0.1453 (0.1454)	loss 0.2170 (0.2184)	grad_norm 15696.2715 (17340.5273)	mem 4918MB
[2022-02-06 22:22:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2210/40036]	eta 1:31:39 lr 0.000004	time 0.1452 (0.1454)	loss 0.2204 (0.2184)	grad_norm 18153.7676 (17339.9238)	mem 4918MB
[2022-02-06 22:23:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2220/40036]	eta 1:31:38 lr 0.000004	time 0.1449 (0.1454)	loss 0.2197 (0.2184)	grad_norm 20347.6113 (17340.0078)	mem 4918MB
[2022-02-06 22:23:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2230/40036]	eta 1:31:36 lr 0.000004	time 0.1447 (0.1454)	loss 0.2168 (0.2184)	grad_norm 18122.9023 (17341.5918)	mem 4918MB
[2022-02-06 22:23:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2240/40036]	eta 1:31:35 lr 0.000004	time 0.1440 (0.1454)	loss 0.2160 (0.2184)	grad_norm 16345.3105 (17343.1875)	mem 4918MB
[2022-02-06 22:23:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2250/40036]	eta 1:31:34 lr 0.000004	time 0.1444 (0.1454)	loss 0.2175 (0.2184)	grad_norm 15444.8525 (17343.0723)	mem 4918MB
[2022-02-06 22:23:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2260/40036]	eta 1:31:32 lr 0.000004	time 0.1448 (0.1454)	loss 0.2184 (0.2184)	grad_norm 14808.9062 (17343.4434)	mem 4918MB
[2022-02-06 22:23:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2270/40036]	eta 1:31:31 lr 0.000004	time 0.1452 (0.1454)	loss 0.2163 (0.2184)	grad_norm 16907.1699 (17342.3027)	mem 4918MB
[2022-02-06 22:23:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2280/40036]	eta 1:31:29 lr 0.000004	time 0.1451 (0.1454)	loss 0.2164 (0.2184)	grad_norm 17542.7090 (17341.5508)	mem 4918MB
[2022-02-06 22:23:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2290/40036]	eta 1:31:28 lr 0.000004	time 0.1451 (0.1454)	loss 0.2201 (0.2184)	grad_norm 18817.7832 (17345.2617)	mem 4918MB
[2022-02-06 22:23:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2300/40036]	eta 1:31:26 lr 0.000004	time 0.1448 (0.1454)	loss 0.2196 (0.2184)	grad_norm 17269.8398 (17345.4668)	mem 4918MB
[2022-02-06 22:23:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2310/40036]	eta 1:31:25 lr 0.000004	time 0.1446 (0.1454)	loss 0.2214 (0.2184)	grad_norm 19336.1074 (17345.8750)	mem 4918MB
[2022-02-06 22:23:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2320/40036]	eta 1:31:24 lr 0.000004	time 0.1455 (0.1454)	loss 0.2210 (0.2184)	grad_norm 18051.5430 (17343.5742)	mem 4918MB
[2022-02-06 22:23:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2330/40036]	eta 1:31:22 lr 0.000004	time 0.1445 (0.1454)	loss 0.2195 (0.2184)	grad_norm 15290.5049 (17338.3770)	mem 4918MB
[2022-02-06 22:23:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2340/40036]	eta 1:31:21 lr 0.000004	time 0.1453 (0.1454)	loss 0.2185 (0.2184)	grad_norm 17496.0957 (17339.4668)	mem 4918MB
[2022-02-06 22:23:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2350/40036]	eta 1:31:20 lr 0.000004	time 0.1456 (0.1454)	loss 0.2182 (0.2184)	grad_norm 20263.0293 (17337.1660)	mem 4918MB
[2022-02-06 22:23:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2360/40036]	eta 1:31:18 lr 0.000004	time 0.1449 (0.1454)	loss 0.2181 (0.2184)	grad_norm 17885.9824 (17338.9805)	mem 4918MB
[2022-02-06 22:23:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2370/40036]	eta 1:31:17 lr 0.000004	time 0.1442 (0.1454)	loss 0.2178 (0.2184)	grad_norm 15878.3242 (17339.2480)	mem 4918MB
[2022-02-06 22:23:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2380/40036]	eta 1:31:16 lr 0.000004	time 0.1450 (0.1454)	loss 0.2198 (0.2184)	grad_norm 17183.3574 (17337.1328)	mem 4918MB
[2022-02-06 22:23:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2390/40036]	eta 1:31:14 lr 0.000004	time 0.1456 (0.1454)	loss 0.2175 (0.2184)	grad_norm 18334.0586 (17338.3184)	mem 4918MB
[2022-02-06 22:23:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2400/40036]	eta 1:31:13 lr 0.000004	time 0.1445 (0.1454)	loss 0.2215 (0.2184)	grad_norm 15543.8467 (17336.2500)	mem 4918MB
[2022-02-06 22:23:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2410/40036]	eta 1:31:11 lr 0.000004	time 0.1449 (0.1454)	loss 0.2197 (0.2184)	grad_norm 20854.1289 (17334.0723)	mem 4918MB
[2022-02-06 22:23:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2420/40036]	eta 1:31:10 lr 0.000004	time 0.1445 (0.1454)	loss 0.2134 (0.2184)	grad_norm 15997.4199 (17339.0918)	mem 4918MB
[2022-02-06 22:23:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2430/40036]	eta 1:31:09 lr 0.000004	time 0.1440 (0.1454)	loss 0.2168 (0.2184)	grad_norm 14217.0254 (17339.8164)	mem 4918MB
[2022-02-06 22:23:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2440/40036]	eta 1:31:07 lr 0.000004	time 0.1449 (0.1454)	loss 0.2167 (0.2184)	grad_norm 17739.0039 (17337.8145)	mem 4918MB
[2022-02-06 22:23:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2450/40036]	eta 1:31:06 lr 0.000004	time 0.1449 (0.1454)	loss 0.2176 (0.2184)	grad_norm 15528.2295 (17334.7539)	mem 4918MB
[2022-02-06 22:23:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2460/40036]	eta 1:31:04 lr 0.000004	time 0.1453 (0.1454)	loss 0.2185 (0.2184)	grad_norm 15692.5244 (17336.1035)	mem 4918MB
[2022-02-06 22:23:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2470/40036]	eta 1:31:03 lr 0.000004	time 0.1454 (0.1454)	loss 0.2177 (0.2184)	grad_norm 15175.0996 (17337.2148)	mem 4918MB
[2022-02-06 22:23:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2480/40036]	eta 1:31:02 lr 0.000004	time 0.1452 (0.1454)	loss 0.2172 (0.2184)	grad_norm 18200.4199 (17338.0898)	mem 4918MB
[2022-02-06 22:23:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2490/40036]	eta 1:31:00 lr 0.000004	time 0.1448 (0.1454)	loss 0.2189 (0.2184)	grad_norm 17002.5859 (17335.4551)	mem 4918MB
[2022-02-06 22:23:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2500/40036]	eta 1:30:59 lr 0.000004	time 0.1455 (0.1454)	loss 0.2146 (0.2184)	grad_norm 16958.4492 (17333.4785)	mem 4918MB
[2022-02-06 22:23:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2510/40036]	eta 1:30:58 lr 0.000004	time 0.1458 (0.1454)	loss 0.2183 (0.2184)	grad_norm 17904.0234 (17334.8770)	mem 4918MB
[2022-02-06 22:23:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2520/40036]	eta 1:30:56 lr 0.000004	time 0.1446 (0.1454)	loss 0.2168 (0.2184)	grad_norm 16296.0293 (17333.2793)	mem 4918MB
[2022-02-06 22:23:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2530/40036]	eta 1:30:55 lr 0.000004	time 0.1449 (0.1455)	loss 0.2175 (0.2184)	grad_norm 16037.9531 (17330.0098)	mem 4918MB
[2022-02-06 22:23:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2540/40036]	eta 1:30:53 lr 0.000004	time 0.1452 (0.1455)	loss 0.2179 (0.2184)	grad_norm 18815.9141 (17330.9102)	mem 4918MB
[2022-02-06 22:23:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2550/40036]	eta 1:30:52 lr 0.000004	time 0.1448 (0.1455)	loss 0.2188 (0.2184)	grad_norm 15888.2354 (17329.9824)	mem 4918MB
[2022-02-06 22:23:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2560/40036]	eta 1:30:51 lr 0.000004	time 0.1453 (0.1455)	loss 0.2189 (0.2184)	grad_norm 19011.3535 (17327.0664)	mem 4918MB
[2022-02-06 22:23:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2570/40036]	eta 1:30:49 lr 0.000004	time 0.1457 (0.1455)	loss 0.2173 (0.2184)	grad_norm 16836.6562 (17323.8281)	mem 4918MB
[2022-02-06 22:23:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2580/40036]	eta 1:30:48 lr 0.000004	time 0.1441 (0.1455)	loss 0.2195 (0.2184)	grad_norm 17745.8633 (17322.4336)	mem 4918MB
[2022-02-06 22:23:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2590/40036]	eta 1:30:46 lr 0.000004	time 0.1441 (0.1455)	loss 0.2159 (0.2184)	grad_norm 18348.3027 (17321.4180)	mem 4918MB
[2022-02-06 22:23:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2600/40036]	eta 1:30:45 lr 0.000004	time 0.1452 (0.1455)	loss 0.2184 (0.2184)	grad_norm 17528.3496 (17323.0410)	mem 4918MB
[2022-02-06 22:23:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2610/40036]	eta 1:30:44 lr 0.000004	time 0.1448 (0.1455)	loss 0.2193 (0.2184)	grad_norm 18578.6289 (17325.5039)	mem 4918MB
[2022-02-06 22:23:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2620/40036]	eta 1:30:42 lr 0.000004	time 0.1448 (0.1455)	loss 0.2183 (0.2184)	grad_norm 18509.3184 (17324.2344)	mem 4918MB
[2022-02-06 22:24:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2630/40036]	eta 1:30:41 lr 0.000004	time 0.1446 (0.1455)	loss 0.2178 (0.2184)	grad_norm 15417.5283 (17320.8145)	mem 4918MB
[2022-02-06 22:24:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2640/40036]	eta 1:30:39 lr 0.000004	time 0.1455 (0.1455)	loss 0.2196 (0.2184)	grad_norm 17082.0156 (17320.4336)	mem 4918MB
[2022-02-06 22:24:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2650/40036]	eta 1:30:38 lr 0.000004	time 0.1449 (0.1455)	loss 0.2187 (0.2184)	grad_norm 20384.3750 (17322.2148)	mem 4918MB
[2022-02-06 22:24:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2660/40036]	eta 1:30:37 lr 0.000004	time 0.1454 (0.1455)	loss 0.2186 (0.2184)	grad_norm 16248.2324 (17319.6445)	mem 4918MB
[2022-02-06 22:24:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2670/40036]	eta 1:30:35 lr 0.000004	time 0.1444 (0.1455)	loss 0.2175 (0.2184)	grad_norm 15711.2363 (17317.4355)	mem 4918MB
[2022-02-06 22:24:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2680/40036]	eta 1:30:34 lr 0.000004	time 0.1450 (0.1455)	loss 0.2167 (0.2184)	grad_norm 17921.5352 (17314.4961)	mem 4918MB
[2022-02-06 22:24:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2690/40036]	eta 1:30:33 lr 0.000004	time 0.1447 (0.1455)	loss 0.2178 (0.2184)	grad_norm 14741.8838 (17313.2207)	mem 4918MB
[2022-02-06 22:24:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2700/40036]	eta 1:30:31 lr 0.000004	time 0.1451 (0.1455)	loss 0.2187 (0.2184)	grad_norm 15738.3623 (17310.5293)	mem 4918MB
[2022-02-06 22:24:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2710/40036]	eta 1:30:30 lr 0.000004	time 0.1444 (0.1455)	loss 0.2179 (0.2184)	grad_norm 17045.1426 (17307.6250)	mem 4918MB
[2022-02-06 22:24:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2720/40036]	eta 1:30:28 lr 0.000004	time 0.1441 (0.1455)	loss 0.2198 (0.2184)	grad_norm 16960.1855 (17308.5234)	mem 4918MB
[2022-02-06 22:24:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2730/40036]	eta 1:30:27 lr 0.000004	time 0.1450 (0.1455)	loss 0.2170 (0.2184)	grad_norm 18322.5020 (17308.2539)	mem 4918MB
[2022-02-06 22:24:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2740/40036]	eta 1:30:25 lr 0.000004	time 0.1447 (0.1455)	loss 0.2183 (0.2184)	grad_norm 19269.7559 (17311.5781)	mem 4918MB
[2022-02-06 22:24:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2750/40036]	eta 1:30:24 lr 0.000004	time 0.1453 (0.1455)	loss 0.2163 (0.2184)	grad_norm 17265.9473 (17310.3613)	mem 4918MB
[2022-02-06 22:24:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2760/40036]	eta 1:30:23 lr 0.000004	time 0.1450 (0.1455)	loss 0.2186 (0.2184)	grad_norm 18717.9375 (17313.5215)	mem 4918MB
[2022-02-06 22:24:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2770/40036]	eta 1:30:21 lr 0.000004	time 0.1446 (0.1455)	loss 0.2165 (0.2184)	grad_norm 15253.9141 (17309.7422)	mem 4918MB
[2022-02-06 22:24:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2780/40036]	eta 1:30:20 lr 0.000004	time 0.1452 (0.1455)	loss 0.2175 (0.2184)	grad_norm 14945.7080 (17309.5020)	mem 4918MB
[2022-02-06 22:24:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2790/40036]	eta 1:30:19 lr 0.000004	time 0.1452 (0.1455)	loss 0.2189 (0.2184)	grad_norm 16785.6152 (17312.2324)	mem 4918MB
[2022-02-06 22:24:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2800/40036]	eta 1:30:17 lr 0.000004	time 0.1454 (0.1455)	loss 0.2138 (0.2184)	grad_norm 15351.6826 (17309.4902)	mem 4918MB
[2022-02-06 22:24:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2810/40036]	eta 1:30:16 lr 0.000004	time 0.1453 (0.1455)	loss 0.2171 (0.2184)	grad_norm 18270.8066 (17308.8164)	mem 4918MB
[2022-02-06 22:24:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2820/40036]	eta 1:30:14 lr 0.000005	time 0.1455 (0.1455)	loss 0.2193 (0.2184)	grad_norm 16314.9443 (17308.1895)	mem 4918MB
[2022-02-06 22:24:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2830/40036]	eta 1:30:13 lr 0.000005	time 0.1449 (0.1455)	loss 0.2182 (0.2184)	grad_norm 18341.7012 (17309.5293)	mem 4918MB
[2022-02-06 22:24:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2840/40036]	eta 1:30:11 lr 0.000005	time 0.1446 (0.1455)	loss 0.2149 (0.2184)	grad_norm 17565.0527 (17309.5312)	mem 4918MB
[2022-02-06 22:24:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2850/40036]	eta 1:30:10 lr 0.000005	time 0.1453 (0.1455)	loss 0.2198 (0.2184)	grad_norm 17537.3047 (17309.9707)	mem 4918MB
[2022-02-06 22:24:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2860/40036]	eta 1:30:09 lr 0.000005	time 0.1459 (0.1455)	loss 0.2181 (0.2184)	grad_norm 21059.1484 (17312.2285)	mem 4918MB
[2022-02-06 22:24:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2870/40036]	eta 1:30:07 lr 0.000005	time 0.1449 (0.1455)	loss 0.2197 (0.2184)	grad_norm 18503.0176 (17311.4375)	mem 4918MB
[2022-02-06 22:24:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2880/40036]	eta 1:30:06 lr 0.000005	time 0.1446 (0.1455)	loss 0.2169 (0.2184)	grad_norm 17737.4902 (17309.8418)	mem 4918MB
[2022-02-06 22:24:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2890/40036]	eta 1:30:05 lr 0.000005	time 0.1452 (0.1455)	loss 0.2159 (0.2183)	grad_norm 14645.0000 (17311.2656)	mem 4918MB
[2022-02-06 22:24:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2900/40036]	eta 1:30:03 lr 0.000005	time 0.1447 (0.1455)	loss 0.2168 (0.2183)	grad_norm 17375.6289 (17310.1621)	mem 4918MB
[2022-02-06 22:24:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2910/40036]	eta 1:30:02 lr 0.000005	time 0.1442 (0.1455)	loss 0.2178 (0.2183)	grad_norm 15584.6191 (17311.9180)	mem 4918MB
[2022-02-06 22:24:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2920/40036]	eta 1:30:00 lr 0.000005	time 0.1452 (0.1455)	loss 0.2197 (0.2183)	grad_norm 19104.0176 (17312.6973)	mem 4918MB
[2022-02-06 22:24:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2930/40036]	eta 1:29:59 lr 0.000005	time 0.1454 (0.1455)	loss 0.2143 (0.2183)	grad_norm 16619.6582 (17310.6152)	mem 4918MB
[2022-02-06 22:24:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2940/40036]	eta 1:29:57 lr 0.000005	time 0.1453 (0.1455)	loss 0.2186 (0.2183)	grad_norm 16317.0283 (17307.5137)	mem 4918MB
[2022-02-06 22:24:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2950/40036]	eta 1:29:56 lr 0.000005	time 0.1455 (0.1455)	loss 0.2174 (0.2183)	grad_norm 17050.8926 (17307.5430)	mem 4918MB
[2022-02-06 22:24:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2960/40036]	eta 1:29:55 lr 0.000005	time 0.1442 (0.1455)	loss 0.2143 (0.2183)	grad_norm 15010.5146 (17308.9570)	mem 4918MB
[2022-02-06 22:24:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2970/40036]	eta 1:29:53 lr 0.000005	time 0.1446 (0.1455)	loss 0.2162 (0.2183)	grad_norm 16985.3477 (17307.3262)	mem 4918MB
[2022-02-06 22:24:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2980/40036]	eta 1:29:52 lr 0.000005	time 0.1459 (0.1455)	loss 0.2166 (0.2183)	grad_norm 19123.4941 (17305.1523)	mem 4918MB
[2022-02-06 22:24:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][2990/40036]	eta 1:29:50 lr 0.000005	time 0.1453 (0.1455)	loss 0.2141 (0.2183)	grad_norm 14647.5029 (17303.9004)	mem 4918MB
[2022-02-06 22:24:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3000/40036]	eta 1:29:49 lr 0.000005	time 0.1451 (0.1455)	loss 0.2167 (0.2183)	grad_norm 18172.9180 (17303.4707)	mem 4918MB
[2022-02-06 22:24:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3010/40036]	eta 1:29:48 lr 0.000005	time 0.1456 (0.1455)	loss 0.2172 (0.2183)	grad_norm 17099.2832 (17302.0059)	mem 4918MB
[2022-02-06 22:24:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3020/40036]	eta 1:29:46 lr 0.000005	time 0.1459 (0.1455)	loss 0.2182 (0.2183)	grad_norm 17419.0488 (17301.6348)	mem 4918MB
[2022-02-06 22:24:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3030/40036]	eta 1:29:45 lr 0.000005	time 0.1444 (0.1455)	loss 0.2199 (0.2183)	grad_norm 17414.2734 (17302.3027)	mem 4918MB
[2022-02-06 22:25:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3040/40036]	eta 1:29:43 lr 0.000005	time 0.1446 (0.1455)	loss 0.2184 (0.2183)	grad_norm 21192.4512 (17302.1426)	mem 4918MB
[2022-02-06 22:25:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3050/40036]	eta 1:29:42 lr 0.000005	time 0.1455 (0.1455)	loss 0.2162 (0.2183)	grad_norm 16734.5000 (17299.6934)	mem 4918MB
[2022-02-06 22:25:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3060/40036]	eta 1:29:41 lr 0.000005	time 0.1448 (0.1455)	loss 0.2194 (0.2183)	grad_norm 17210.1348 (17297.7227)	mem 4918MB
[2022-02-06 22:25:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3070/40036]	eta 1:29:39 lr 0.000005	time 0.1438 (0.1455)	loss 0.2186 (0.2183)	grad_norm 16392.5469 (17296.0664)	mem 4918MB
[2022-02-06 22:25:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3080/40036]	eta 1:29:38 lr 0.000005	time 0.1453 (0.1455)	loss 0.2148 (0.2183)	grad_norm 18751.1562 (17295.0938)	mem 4918MB
[2022-02-06 22:25:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3090/40036]	eta 1:29:36 lr 0.000005	time 0.1443 (0.1455)	loss 0.2179 (0.2183)	grad_norm 14559.1963 (17295.1387)	mem 4918MB
[2022-02-06 22:25:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3100/40036]	eta 1:29:35 lr 0.000005	time 0.1448 (0.1455)	loss 0.2179 (0.2183)	grad_norm 18388.3535 (17292.2695)	mem 4918MB
[2022-02-06 22:25:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3110/40036]	eta 1:29:34 lr 0.000005	time 0.1447 (0.1455)	loss 0.2178 (0.2183)	grad_norm 16612.6895 (17291.6699)	mem 4918MB
[2022-02-06 22:25:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3120/40036]	eta 1:29:32 lr 0.000005	time 0.1450 (0.1455)	loss 0.2170 (0.2183)	grad_norm 18347.8184 (17291.1973)	mem 4918MB
[2022-02-06 22:25:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3130/40036]	eta 1:29:31 lr 0.000005	time 0.1444 (0.1455)	loss 0.2191 (0.2183)	grad_norm 15897.0166 (17292.0273)	mem 4918MB
[2022-02-06 22:25:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3140/40036]	eta 1:29:29 lr 0.000005	time 0.1448 (0.1455)	loss 0.2201 (0.2183)	grad_norm 17004.0664 (17294.7480)	mem 4918MB
[2022-02-06 22:25:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3150/40036]	eta 1:29:28 lr 0.000005	time 0.1455 (0.1455)	loss 0.2196 (0.2183)	grad_norm 19338.6035 (17293.9453)	mem 4918MB
[2022-02-06 22:25:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3160/40036]	eta 1:29:26 lr 0.000005	time 0.1451 (0.1455)	loss 0.2182 (0.2183)	grad_norm 17744.9648 (17295.3516)	mem 4918MB
[2022-02-06 22:25:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3170/40036]	eta 1:29:25 lr 0.000005	time 0.1452 (0.1455)	loss 0.2158 (0.2183)	grad_norm 15672.1572 (17291.7715)	mem 4918MB
[2022-02-06 22:25:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3180/40036]	eta 1:29:24 lr 0.000005	time 0.1451 (0.1455)	loss 0.2167 (0.2183)	grad_norm 13405.0371 (17290.7539)	mem 4918MB
[2022-02-06 22:25:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3190/40036]	eta 1:29:22 lr 0.000005	time 0.1448 (0.1455)	loss 0.2170 (0.2183)	grad_norm 16621.2773 (17291.2246)	mem 4918MB
[2022-02-06 22:25:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3200/40036]	eta 1:29:21 lr 0.000005	time 0.1443 (0.1455)	loss 0.2151 (0.2183)	grad_norm 18266.9219 (17289.4355)	mem 4918MB
[2022-02-06 22:25:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3210/40036]	eta 1:29:19 lr 0.000005	time 0.1457 (0.1455)	loss 0.2180 (0.2183)	grad_norm 16687.3027 (17289.1094)	mem 4918MB
[2022-02-06 22:25:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3220/40036]	eta 1:29:18 lr 0.000005	time 0.1451 (0.1455)	loss 0.2202 (0.2183)	grad_norm 18735.3027 (17289.5391)	mem 4918MB
[2022-02-06 22:25:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3230/40036]	eta 1:29:17 lr 0.000005	time 0.1446 (0.1455)	loss 0.2196 (0.2183)	grad_norm 17151.4180 (17288.6191)	mem 4918MB
[2022-02-06 22:25:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3240/40036]	eta 1:29:15 lr 0.000005	time 0.1456 (0.1456)	loss 0.2176 (0.2183)	grad_norm 17279.0195 (17284.8477)	mem 4918MB
[2022-02-06 22:25:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3250/40036]	eta 1:29:14 lr 0.000005	time 0.1446 (0.1456)	loss 0.2182 (0.2183)	grad_norm 18377.2324 (17286.8828)	mem 4918MB
[2022-02-06 22:25:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3260/40036]	eta 1:29:12 lr 0.000005	time 0.1452 (0.1456)	loss 0.2163 (0.2183)	grad_norm 18237.5449 (17286.4980)	mem 4918MB
[2022-02-06 22:25:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3270/40036]	eta 1:29:11 lr 0.000005	time 0.1444 (0.1456)	loss 0.2175 (0.2183)	grad_norm 17144.4609 (17286.4258)	mem 4918MB
[2022-02-06 22:25:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3280/40036]	eta 1:29:10 lr 0.000005	time 0.1446 (0.1456)	loss 0.2143 (0.2183)	grad_norm 15007.3838 (17283.5566)	mem 4918MB
[2022-02-06 22:25:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3290/40036]	eta 1:29:08 lr 0.000005	time 0.1448 (0.1456)	loss 0.2166 (0.2183)	grad_norm 14004.8955 (17284.9961)	mem 4918MB
[2022-02-06 22:25:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3300/40036]	eta 1:29:07 lr 0.000005	time 0.1455 (0.1456)	loss 0.2185 (0.2183)	grad_norm 18660.3984 (17284.2070)	mem 4918MB
[2022-02-06 22:25:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3310/40036]	eta 1:29:05 lr 0.000005	time 0.1449 (0.1456)	loss 0.2197 (0.2183)	grad_norm 18881.9102 (17283.1914)	mem 4918MB
[2022-02-06 22:25:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3320/40036]	eta 1:29:04 lr 0.000005	time 0.1445 (0.1456)	loss 0.2187 (0.2183)	grad_norm 14953.2969 (17281.2246)	mem 4918MB
[2022-02-06 22:25:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3330/40036]	eta 1:29:03 lr 0.000005	time 0.1443 (0.1456)	loss 0.2188 (0.2183)	grad_norm 19234.8535 (17283.8984)	mem 4918MB
[2022-02-06 22:25:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3340/40036]	eta 1:29:01 lr 0.000005	time 0.1453 (0.1456)	loss 0.2213 (0.2183)	grad_norm 19966.9707 (17285.2500)	mem 4918MB
[2022-02-06 22:25:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3350/40036]	eta 1:29:00 lr 0.000005	time 0.1452 (0.1456)	loss 0.2175 (0.2183)	grad_norm 13558.9980 (17283.8438)	mem 4918MB
[2022-02-06 22:25:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3360/40036]	eta 1:28:58 lr 0.000005	time 0.1449 (0.1456)	loss 0.2192 (0.2183)	grad_norm 15586.4834 (17283.3105)	mem 4918MB
[2022-02-06 22:25:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3370/40036]	eta 1:28:57 lr 0.000005	time 0.1451 (0.1456)	loss 0.2171 (0.2183)	grad_norm 15163.4541 (17282.0645)	mem 4918MB
[2022-02-06 22:25:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3380/40036]	eta 1:28:55 lr 0.000005	time 0.1452 (0.1456)	loss 0.2168 (0.2183)	grad_norm 13950.7510 (17280.8184)	mem 4918MB
[2022-02-06 22:25:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3390/40036]	eta 1:28:54 lr 0.000005	time 0.1446 (0.1456)	loss 0.2180 (0.2183)	grad_norm 16328.0713 (17278.6543)	mem 4918MB
[2022-02-06 22:25:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3400/40036]	eta 1:28:53 lr 0.000005	time 0.1449 (0.1456)	loss 0.2182 (0.2183)	grad_norm 19361.1465 (17280.7598)	mem 4918MB
[2022-02-06 22:25:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3410/40036]	eta 1:28:51 lr 0.000005	time 0.1446 (0.1456)	loss 0.2172 (0.2183)	grad_norm 16735.4785 (17278.3418)	mem 4918MB
[2022-02-06 22:25:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3420/40036]	eta 1:28:50 lr 0.000005	time 0.1438 (0.1456)	loss 0.2159 (0.2183)	grad_norm 18445.5703 (17277.9668)	mem 4918MB
[2022-02-06 22:25:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3430/40036]	eta 1:28:48 lr 0.000005	time 0.1454 (0.1456)	loss 0.2201 (0.2183)	grad_norm 18072.7363 (17277.3438)	mem 4918MB
[2022-02-06 22:25:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3440/40036]	eta 1:28:47 lr 0.000005	time 0.1450 (0.1456)	loss 0.2185 (0.2183)	grad_norm 16588.7734 (17276.6777)	mem 4918MB
[2022-02-06 22:26:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3450/40036]	eta 1:28:45 lr 0.000005	time 0.1450 (0.1456)	loss 0.2198 (0.2183)	grad_norm 17584.5898 (17276.4062)	mem 4918MB
[2022-02-06 22:26:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3460/40036]	eta 1:28:44 lr 0.000005	time 0.1447 (0.1456)	loss 0.2161 (0.2183)	grad_norm 16144.9912 (17275.1758)	mem 4918MB
[2022-02-06 22:26:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3470/40036]	eta 1:28:43 lr 0.000005	time 0.1453 (0.1456)	loss 0.2175 (0.2183)	grad_norm 16734.0996 (17275.6836)	mem 4918MB
[2022-02-06 22:26:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3480/40036]	eta 1:28:41 lr 0.000005	time 0.1444 (0.1456)	loss 0.2189 (0.2183)	grad_norm 17347.3496 (17275.9824)	mem 4918MB
[2022-02-06 22:26:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3490/40036]	eta 1:28:40 lr 0.000005	time 0.1457 (0.1456)	loss 0.2184 (0.2183)	grad_norm 17637.0918 (17277.5293)	mem 4918MB
[2022-02-06 22:26:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3500/40036]	eta 1:28:38 lr 0.000005	time 0.1455 (0.1456)	loss 0.2186 (0.2183)	grad_norm 16772.8027 (17277.8926)	mem 4918MB
[2022-02-06 22:26:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3510/40036]	eta 1:28:37 lr 0.000005	time 0.1450 (0.1456)	loss 0.2200 (0.2183)	grad_norm 18040.6680 (17277.3477)	mem 4918MB
[2022-02-06 22:26:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3520/40036]	eta 1:28:36 lr 0.000005	time 0.1442 (0.1456)	loss 0.2172 (0.2183)	grad_norm 18808.9824 (17280.0352)	mem 4918MB
[2022-02-06 22:26:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3530/40036]	eta 1:28:34 lr 0.000005	time 0.1447 (0.1456)	loss 0.2172 (0.2183)	grad_norm 17842.2852 (17278.2031)	mem 4918MB
[2022-02-06 22:26:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3540/40036]	eta 1:28:33 lr 0.000005	time 0.1436 (0.1456)	loss 0.2177 (0.2183)	grad_norm 16936.8535 (17278.1445)	mem 4918MB
[2022-02-06 22:26:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3550/40036]	eta 1:28:31 lr 0.000005	time 0.1446 (0.1456)	loss 0.2219 (0.2183)	grad_norm 18464.1406 (17278.3848)	mem 4918MB
[2022-02-06 22:26:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3560/40036]	eta 1:28:30 lr 0.000005	time 0.1447 (0.1456)	loss 0.2157 (0.2183)	grad_norm 17359.8223 (17277.0547)	mem 4918MB
[2022-02-06 22:26:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3570/40036]	eta 1:28:28 lr 0.000005	time 0.1449 (0.1456)	loss 0.2191 (0.2183)	grad_norm 17916.0039 (17277.4922)	mem 4918MB
[2022-02-06 22:26:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3580/40036]	eta 1:28:27 lr 0.000005	time 0.1444 (0.1456)	loss 0.2169 (0.2183)	grad_norm 16712.7480 (17275.1660)	mem 4918MB
[2022-02-06 22:26:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3590/40036]	eta 1:28:26 lr 0.000005	time 0.1447 (0.1456)	loss 0.2174 (0.2183)	grad_norm 15049.3154 (17272.7559)	mem 4918MB
[2022-02-06 22:26:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3600/40036]	eta 1:28:24 lr 0.000005	time 0.1444 (0.1456)	loss 0.2178 (0.2183)	grad_norm 18915.2734 (17272.1777)	mem 4918MB
[2022-02-06 22:26:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3610/40036]	eta 1:28:23 lr 0.000005	time 0.1446 (0.1456)	loss 0.2166 (0.2183)	grad_norm 16342.9688 (17271.3457)	mem 4918MB
[2022-02-06 22:26:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3620/40036]	eta 1:28:21 lr 0.000006	time 0.1453 (0.1456)	loss 0.2185 (0.2183)	grad_norm 13865.0166 (17266.8965)	mem 4918MB
[2022-02-06 22:26:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3630/40036]	eta 1:28:20 lr 0.000006	time 0.1445 (0.1456)	loss 0.2181 (0.2183)	grad_norm 18320.1582 (17266.0078)	mem 4918MB
[2022-02-06 22:26:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3640/40036]	eta 1:28:18 lr 0.000006	time 0.1445 (0.1456)	loss 0.2209 (0.2183)	grad_norm 15922.9277 (17264.2520)	mem 4918MB
[2022-02-06 22:26:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3650/40036]	eta 1:28:17 lr 0.000006	time 0.1453 (0.1456)	loss 0.2182 (0.2183)	grad_norm 17097.9062 (17262.7773)	mem 4918MB
[2022-02-06 22:26:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3660/40036]	eta 1:28:16 lr 0.000006	time 0.1453 (0.1456)	loss 0.2182 (0.2183)	grad_norm 13468.0840 (17261.6504)	mem 4918MB
[2022-02-06 22:26:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3670/40036]	eta 1:28:14 lr 0.000006	time 0.1455 (0.1456)	loss 0.2174 (0.2183)	grad_norm 16759.0098 (17259.8750)	mem 4918MB
[2022-02-06 22:26:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3680/40036]	eta 1:28:13 lr 0.000006	time 0.1441 (0.1456)	loss 0.2177 (0.2183)	grad_norm 16160.8613 (17259.1621)	mem 4918MB
[2022-02-06 22:26:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3690/40036]	eta 1:28:12 lr 0.000006	time 0.1453 (0.1456)	loss 0.2193 (0.2183)	grad_norm 18516.5312 (17256.6289)	mem 4918MB
[2022-02-06 22:26:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3700/40036]	eta 1:28:10 lr 0.000006	time 0.1448 (0.1456)	loss 0.2165 (0.2183)	grad_norm 16635.1504 (17255.4941)	mem 4918MB
[2022-02-06 22:26:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3710/40036]	eta 1:28:09 lr 0.000006	time 0.1439 (0.1456)	loss 0.2154 (0.2183)	grad_norm 16377.4131 (17255.1133)	mem 4918MB
[2022-02-06 22:26:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3720/40036]	eta 1:28:08 lr 0.000006	time 0.1442 (0.1456)	loss 0.2173 (0.2183)	grad_norm 16665.1211 (17255.4180)	mem 4918MB
[2022-02-06 22:26:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3730/40036]	eta 1:28:06 lr 0.000006	time 0.1450 (0.1456)	loss 0.2205 (0.2183)	grad_norm 17771.2754 (17253.5801)	mem 4918MB
[2022-02-06 22:26:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3740/40036]	eta 1:28:05 lr 0.000006	time 0.1453 (0.1456)	loss 0.2195 (0.2183)	grad_norm 15893.5186 (17252.1934)	mem 4918MB
[2022-02-06 22:26:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3750/40036]	eta 1:28:03 lr 0.000006	time 0.1451 (0.1456)	loss 0.2193 (0.2183)	grad_norm 17373.3008 (17250.8848)	mem 4918MB
[2022-02-06 22:26:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3760/40036]	eta 1:28:02 lr 0.000006	time 0.1443 (0.1456)	loss 0.2170 (0.2183)	grad_norm 17847.2207 (17249.0625)	mem 4918MB
[2022-02-06 22:26:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3770/40036]	eta 1:28:00 lr 0.000006	time 0.1451 (0.1456)	loss 0.2196 (0.2183)	grad_norm 17652.5254 (17248.3164)	mem 4918MB
[2022-02-06 22:26:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3780/40036]	eta 1:27:59 lr 0.000006	time 0.1449 (0.1456)	loss 0.2201 (0.2183)	grad_norm 16106.7568 (17248.5312)	mem 4918MB
[2022-02-06 22:26:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3790/40036]	eta 1:27:58 lr 0.000006	time 0.1452 (0.1456)	loss 0.2209 (0.2183)	grad_norm 19579.5156 (17246.1973)	mem 4918MB
[2022-02-06 22:26:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3800/40036]	eta 1:27:56 lr 0.000006	time 0.1446 (0.1456)	loss 0.2171 (0.2183)	grad_norm 15493.6904 (17244.6230)	mem 4918MB
[2022-02-06 22:26:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3810/40036]	eta 1:27:55 lr 0.000006	time 0.1456 (0.1456)	loss 0.2171 (0.2183)	grad_norm 15778.2422 (17243.6387)	mem 4918MB
[2022-02-06 22:26:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3820/40036]	eta 1:27:53 lr 0.000006	time 0.1443 (0.1456)	loss 0.2155 (0.2183)	grad_norm 14858.5020 (17244.1895)	mem 4918MB
[2022-02-06 22:26:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3830/40036]	eta 1:27:52 lr 0.000006	time 0.1451 (0.1456)	loss 0.2162 (0.2183)	grad_norm 19055.8301 (17241.7520)	mem 4918MB
[2022-02-06 22:26:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3840/40036]	eta 1:27:50 lr 0.000006	time 0.1448 (0.1456)	loss 0.2182 (0.2183)	grad_norm 18899.2012 (17242.4102)	mem 4918MB
[2022-02-06 22:26:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3850/40036]	eta 1:27:49 lr 0.000006	time 0.1447 (0.1456)	loss 0.2228 (0.2183)	grad_norm 17968.5918 (17241.9961)	mem 4918MB
[2022-02-06 22:27:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3860/40036]	eta 1:27:48 lr 0.000006	time 0.1450 (0.1456)	loss 0.2170 (0.2183)	grad_norm 17944.1035 (17241.8652)	mem 4918MB
[2022-02-06 22:27:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3870/40036]	eta 1:27:46 lr 0.000006	time 0.1454 (0.1456)	loss 0.2189 (0.2183)	grad_norm 15531.3525 (17240.8750)	mem 4918MB
[2022-02-06 22:27:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3880/40036]	eta 1:27:45 lr 0.000006	time 0.1454 (0.1456)	loss 0.2178 (0.2183)	grad_norm 16973.8926 (17241.6348)	mem 4918MB
[2022-02-06 22:27:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3890/40036]	eta 1:27:43 lr 0.000006	time 0.1453 (0.1456)	loss 0.2188 (0.2183)	grad_norm 16295.4375 (17240.4766)	mem 4918MB
[2022-02-06 22:27:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3900/40036]	eta 1:27:42 lr 0.000006	time 0.1448 (0.1456)	loss 0.2183 (0.2183)	grad_norm 16661.2520 (17240.1445)	mem 4918MB
[2022-02-06 22:27:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3910/40036]	eta 1:27:41 lr 0.000006	time 0.1448 (0.1456)	loss 0.2178 (0.2183)	grad_norm 15094.6816 (17238.7441)	mem 4918MB
[2022-02-06 22:27:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3920/40036]	eta 1:27:39 lr 0.000006	time 0.1442 (0.1456)	loss 0.2177 (0.2183)	grad_norm 16867.8594 (17239.3164)	mem 4918MB
[2022-02-06 22:27:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3930/40036]	eta 1:27:38 lr 0.000006	time 0.1445 (0.1456)	loss 0.2194 (0.2183)	grad_norm 17531.2891 (17239.2520)	mem 4918MB
[2022-02-06 22:27:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3940/40036]	eta 1:27:36 lr 0.000006	time 0.1448 (0.1456)	loss 0.2173 (0.2183)	grad_norm 16733.2207 (17239.7812)	mem 4918MB
[2022-02-06 22:27:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3950/40036]	eta 1:27:35 lr 0.000006	time 0.1446 (0.1456)	loss 0.2166 (0.2183)	grad_norm 18400.1230 (17241.3711)	mem 4918MB
[2022-02-06 22:27:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3960/40036]	eta 1:27:33 lr 0.000006	time 0.1451 (0.1456)	loss 0.2200 (0.2183)	grad_norm 16028.1865 (17240.9648)	mem 4918MB
[2022-02-06 22:27:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3970/40036]	eta 1:27:32 lr 0.000006	time 0.1450 (0.1456)	loss 0.2192 (0.2183)	grad_norm 18149.2188 (17238.5898)	mem 4918MB
[2022-02-06 22:27:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3980/40036]	eta 1:27:31 lr 0.000006	time 0.1456 (0.1456)	loss 0.2174 (0.2183)	grad_norm 18785.7695 (17237.7246)	mem 4918MB
[2022-02-06 22:27:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][3990/40036]	eta 1:27:29 lr 0.000006	time 0.1451 (0.1456)	loss 0.2167 (0.2183)	grad_norm 15028.3857 (17236.6660)	mem 4918MB
[2022-02-06 22:27:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4000/40036]	eta 1:27:28 lr 0.000006	time 0.1449 (0.1456)	loss 0.2196 (0.2183)	grad_norm 17593.4668 (17236.5137)	mem 4918MB
[2022-02-06 22:27:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4010/40036]	eta 1:27:26 lr 0.000006	time 0.1448 (0.1456)	loss 0.2195 (0.2183)	grad_norm 16560.2188 (17238.1191)	mem 4918MB
[2022-02-06 22:27:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4020/40036]	eta 1:27:25 lr 0.000006	time 0.1446 (0.1456)	loss 0.2193 (0.2183)	grad_norm 15185.4170 (17236.1973)	mem 4918MB
[2022-02-06 22:27:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4030/40036]	eta 1:27:23 lr 0.000006	time 0.1454 (0.1456)	loss 0.2156 (0.2183)	grad_norm 18384.9824 (17235.8555)	mem 4918MB
[2022-02-06 22:27:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4040/40036]	eta 1:27:22 lr 0.000006	time 0.1452 (0.1456)	loss 0.2173 (0.2183)	grad_norm 16436.4082 (17233.7266)	mem 4918MB
[2022-02-06 22:27:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4050/40036]	eta 1:27:21 lr 0.000006	time 0.1451 (0.1456)	loss 0.2198 (0.2183)	grad_norm 14761.6953 (17229.8125)	mem 4918MB
[2022-02-06 22:27:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4060/40036]	eta 1:27:19 lr 0.000006	time 0.1453 (0.1456)	loss 0.2165 (0.2183)	grad_norm 15131.7676 (17227.4336)	mem 4918MB
[2022-02-06 22:27:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4070/40036]	eta 1:27:18 lr 0.000006	time 0.1447 (0.1456)	loss 0.2215 (0.2183)	grad_norm 17617.1426 (17229.3555)	mem 4918MB
[2022-02-06 22:27:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4080/40036]	eta 1:27:16 lr 0.000006	time 0.1453 (0.1456)	loss 0.2175 (0.2183)	grad_norm 15354.5996 (17227.8223)	mem 4918MB
[2022-02-06 22:27:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4090/40036]	eta 1:27:15 lr 0.000006	time 0.1446 (0.1456)	loss 0.2205 (0.2183)	grad_norm 19101.3086 (17229.8633)	mem 4918MB
[2022-02-06 22:27:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4100/40036]	eta 1:27:14 lr 0.000006	time 0.1449 (0.1457)	loss 0.2201 (0.2183)	grad_norm 18131.2676 (17228.9375)	mem 4918MB
[2022-02-06 22:27:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4110/40036]	eta 1:27:12 lr 0.000006	time 0.1450 (0.1457)	loss 0.2183 (0.2183)	grad_norm 16868.0586 (17227.9961)	mem 4918MB
[2022-02-06 22:27:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4120/40036]	eta 1:27:11 lr 0.000006	time 0.1445 (0.1457)	loss 0.2177 (0.2183)	grad_norm 17576.8613 (17228.0684)	mem 4918MB
[2022-02-06 22:27:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4130/40036]	eta 1:27:09 lr 0.000006	time 0.1456 (0.1457)	loss 0.2163 (0.2183)	grad_norm 16934.0078 (17227.5703)	mem 4918MB
[2022-02-06 22:27:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4140/40036]	eta 1:27:08 lr 0.000006	time 0.1443 (0.1457)	loss 0.2192 (0.2183)	grad_norm 18332.6055 (17227.2031)	mem 4918MB
[2022-02-06 22:27:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4150/40036]	eta 1:27:06 lr 0.000006	time 0.1448 (0.1457)	loss 0.2175 (0.2183)	grad_norm 15880.1182 (17227.0938)	mem 4918MB
[2022-02-06 22:27:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4160/40036]	eta 1:27:05 lr 0.000006	time 0.1440 (0.1457)	loss 0.2178 (0.2183)	grad_norm 19792.2930 (17226.5801)	mem 4918MB
[2022-02-06 22:27:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4170/40036]	eta 1:27:04 lr 0.000006	time 0.1450 (0.1457)	loss 0.2199 (0.2183)	grad_norm 17094.8027 (17224.1953)	mem 4918MB
[2022-02-06 22:27:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4180/40036]	eta 1:27:02 lr 0.000006	time 0.1455 (0.1457)	loss 0.2183 (0.2183)	grad_norm 15146.0107 (17221.7344)	mem 4918MB
[2022-02-06 22:27:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4190/40036]	eta 1:27:01 lr 0.000006	time 0.1454 (0.1457)	loss 0.2158 (0.2183)	grad_norm 16520.4180 (17219.5996)	mem 4918MB
[2022-02-06 22:27:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4200/40036]	eta 1:26:59 lr 0.000006	time 0.1451 (0.1457)	loss 0.2167 (0.2183)	grad_norm 16830.0020 (17220.0723)	mem 4918MB
[2022-02-06 22:27:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4210/40036]	eta 1:26:58 lr 0.000006	time 0.1451 (0.1457)	loss 0.2164 (0.2183)	grad_norm 15460.3613 (17216.9023)	mem 4918MB
[2022-02-06 22:27:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4220/40036]	eta 1:26:57 lr 0.000006	time 0.1457 (0.1457)	loss 0.2200 (0.2183)	grad_norm 14964.2666 (17213.4922)	mem 4918MB
[2022-02-06 22:27:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4230/40036]	eta 1:26:55 lr 0.000006	time 0.1438 (0.1457)	loss 0.2167 (0.2183)	grad_norm 18072.7344 (17214.8535)	mem 4918MB
[2022-02-06 22:27:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4240/40036]	eta 1:26:54 lr 0.000006	time 0.1450 (0.1457)	loss 0.2183 (0.2183)	grad_norm 14842.2930 (17212.9434)	mem 4918MB
[2022-02-06 22:27:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4250/40036]	eta 1:26:52 lr 0.000006	time 0.1448 (0.1457)	loss 0.2191 (0.2183)	grad_norm 16740.7266 (17213.4102)	mem 4918MB
[2022-02-06 22:27:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4260/40036]	eta 1:26:51 lr 0.000006	time 0.1439 (0.1457)	loss 0.2189 (0.2183)	grad_norm 18627.0625 (17210.8711)	mem 4918MB
[2022-02-06 22:28:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4270/40036]	eta 1:26:49 lr 0.000006	time 0.1446 (0.1457)	loss 0.2170 (0.2183)	grad_norm 18726.4434 (17211.9824)	mem 4918MB
[2022-02-06 22:28:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4280/40036]	eta 1:26:48 lr 0.000006	time 0.1455 (0.1457)	loss 0.2187 (0.2183)	grad_norm 15880.0586 (17210.1504)	mem 4918MB
[2022-02-06 22:28:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4290/40036]	eta 1:26:47 lr 0.000006	time 0.1450 (0.1457)	loss 0.2154 (0.2183)	grad_norm 13130.1924 (17209.1699)	mem 4918MB
[2022-02-06 22:28:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4300/40036]	eta 1:26:45 lr 0.000006	time 0.1449 (0.1457)	loss 0.2167 (0.2183)	grad_norm 20184.0625 (17209.0137)	mem 4918MB
[2022-02-06 22:28:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4310/40036]	eta 1:26:44 lr 0.000006	time 0.1438 (0.1457)	loss 0.2200 (0.2183)	grad_norm 14958.3311 (17206.7949)	mem 4918MB
[2022-02-06 22:28:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4320/40036]	eta 1:26:42 lr 0.000006	time 0.1445 (0.1457)	loss 0.2212 (0.2183)	grad_norm 17355.0059 (17204.4551)	mem 4918MB
[2022-02-06 22:28:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4330/40036]	eta 1:26:41 lr 0.000006	time 0.1445 (0.1457)	loss 0.2167 (0.2183)	grad_norm 17175.0098 (17202.8750)	mem 4918MB
[2022-02-06 22:28:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4340/40036]	eta 1:26:40 lr 0.000006	time 0.1446 (0.1457)	loss 0.2124 (0.2183)	grad_norm 15571.5107 (17203.6445)	mem 4918MB
[2022-02-06 22:28:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4350/40036]	eta 1:26:38 lr 0.000006	time 0.1453 (0.1457)	loss 0.2162 (0.2183)	grad_norm 15535.6807 (17202.0762)	mem 4918MB
[2022-02-06 22:28:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4360/40036]	eta 1:26:37 lr 0.000006	time 0.1454 (0.1457)	loss 0.2187 (0.2183)	grad_norm 17473.0039 (17200.4766)	mem 4918MB
[2022-02-06 22:28:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4370/40036]	eta 1:26:35 lr 0.000006	time 0.1455 (0.1457)	loss 0.2179 (0.2183)	grad_norm 13920.7021 (17197.7402)	mem 4918MB
[2022-02-06 22:28:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4380/40036]	eta 1:26:34 lr 0.000006	time 0.1461 (0.1457)	loss 0.2196 (0.2183)	grad_norm 18721.2637 (17198.2910)	mem 4918MB
[2022-02-06 22:28:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4390/40036]	eta 1:26:33 lr 0.000006	time 0.1447 (0.1457)	loss 0.2171 (0.2183)	grad_norm 15378.3447 (17197.4160)	mem 4918MB
[2022-02-06 22:28:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4400/40036]	eta 1:26:31 lr 0.000006	time 0.1449 (0.1457)	loss 0.2155 (0.2183)	grad_norm 14780.8555 (17193.8730)	mem 4918MB
[2022-02-06 22:28:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4410/40036]	eta 1:26:30 lr 0.000006	time 0.1443 (0.1457)	loss 0.2177 (0.2183)	grad_norm 14269.1865 (17191.3887)	mem 4918MB
[2022-02-06 22:28:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4420/40036]	eta 1:26:28 lr 0.000007	time 0.1455 (0.1457)	loss 0.2196 (0.2183)	grad_norm 17116.1895 (17191.1914)	mem 4918MB
[2022-02-06 22:28:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4430/40036]	eta 1:26:27 lr 0.000007	time 0.1446 (0.1457)	loss 0.2182 (0.2183)	grad_norm 16322.7334 (17191.0391)	mem 4918MB
[2022-02-06 22:28:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4440/40036]	eta 1:26:25 lr 0.000007	time 0.1450 (0.1457)	loss 0.2185 (0.2183)	grad_norm 14314.6797 (17189.7090)	mem 4918MB
[2022-02-06 22:28:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4450/40036]	eta 1:26:24 lr 0.000007	time 0.1447 (0.1457)	loss 0.2177 (0.2183)	grad_norm 19564.5840 (17188.7266)	mem 4918MB
[2022-02-06 22:28:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4460/40036]	eta 1:26:23 lr 0.000007	time 0.1454 (0.1457)	loss 0.2174 (0.2183)	grad_norm 18054.9746 (17189.5527)	mem 4918MB
[2022-02-06 22:28:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4470/40036]	eta 1:26:21 lr 0.000007	time 0.1454 (0.1457)	loss 0.2167 (0.2183)	grad_norm 13749.9971 (17187.4316)	mem 4918MB
[2022-02-06 22:28:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4480/40036]	eta 1:26:20 lr 0.000007	time 0.1448 (0.1457)	loss 0.2200 (0.2183)	grad_norm 17010.0547 (17187.7754)	mem 4918MB
[2022-02-06 22:28:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4490/40036]	eta 1:26:18 lr 0.000007	time 0.1454 (0.1457)	loss 0.2212 (0.2183)	grad_norm 18726.5410 (17188.0820)	mem 4918MB
[2022-02-06 22:28:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4500/40036]	eta 1:26:17 lr 0.000007	time 0.1452 (0.1457)	loss 0.2205 (0.2183)	grad_norm 17695.8477 (17187.3555)	mem 4918MB
[2022-02-06 22:28:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4510/40036]	eta 1:26:15 lr 0.000007	time 0.1445 (0.1457)	loss 0.2180 (0.2183)	grad_norm 15849.4033 (17184.8770)	mem 4918MB
[2022-02-06 22:28:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4520/40036]	eta 1:26:14 lr 0.000007	time 0.1444 (0.1457)	loss 0.2204 (0.2183)	grad_norm 17615.0996 (17185.0527)	mem 4918MB
[2022-02-06 22:28:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4530/40036]	eta 1:26:13 lr 0.000007	time 0.1452 (0.1457)	loss 0.2175 (0.2183)	grad_norm 17292.0645 (17182.8301)	mem 4918MB
[2022-02-06 22:28:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4540/40036]	eta 1:26:11 lr 0.000007	time 0.1445 (0.1457)	loss 0.2165 (0.2183)	grad_norm 14440.4287 (17181.9082)	mem 4918MB
[2022-02-06 22:28:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4550/40036]	eta 1:26:10 lr 0.000007	time 0.1451 (0.1457)	loss 0.2178 (0.2183)	grad_norm 14035.5693 (17179.6367)	mem 4918MB
[2022-02-06 22:28:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4560/40036]	eta 1:26:08 lr 0.000007	time 0.1446 (0.1457)	loss 0.2169 (0.2183)	grad_norm 16676.2852 (17176.7070)	mem 4918MB
[2022-02-06 22:28:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4570/40036]	eta 1:26:07 lr 0.000007	time 0.1440 (0.1457)	loss 0.2189 (0.2183)	grad_norm 18779.1465 (17175.4648)	mem 4918MB
[2022-02-06 22:28:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4580/40036]	eta 1:26:05 lr 0.000007	time 0.1439 (0.1457)	loss 0.2188 (0.2183)	grad_norm 14829.0781 (17173.4727)	mem 4918MB
[2022-02-06 22:28:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4590/40036]	eta 1:26:04 lr 0.000007	time 0.1451 (0.1457)	loss 0.2186 (0.2183)	grad_norm 18468.1582 (17173.5566)	mem 4918MB
[2022-02-06 22:28:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4600/40036]	eta 1:26:03 lr 0.000007	time 0.1452 (0.1457)	loss 0.2164 (0.2183)	grad_norm 15479.6240 (17172.5117)	mem 4918MB
[2022-02-06 22:28:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4610/40036]	eta 1:26:01 lr 0.000007	time 0.1457 (0.1457)	loss 0.2167 (0.2182)	grad_norm 16125.9258 (17172.5254)	mem 4918MB
[2022-02-06 22:28:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4620/40036]	eta 1:26:00 lr 0.000007	time 0.1448 (0.1457)	loss 0.2189 (0.2182)	grad_norm 19392.7461 (17172.8750)	mem 4918MB
[2022-02-06 22:28:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4630/40036]	eta 1:25:58 lr 0.000007	time 0.1453 (0.1457)	loss 0.2236 (0.2182)	grad_norm 19100.9551 (17171.3398)	mem 4918MB
[2022-02-06 22:28:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4640/40036]	eta 1:25:57 lr 0.000007	time 0.1439 (0.1457)	loss 0.2194 (0.2183)	grad_norm 17457.4863 (17169.2305)	mem 4918MB
[2022-02-06 22:28:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4650/40036]	eta 1:25:56 lr 0.000007	time 0.1444 (0.1457)	loss 0.2155 (0.2182)	grad_norm 16076.5781 (17168.8340)	mem 4918MB
[2022-02-06 22:28:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4660/40036]	eta 1:25:54 lr 0.000007	time 0.1446 (0.1457)	loss 0.2163 (0.2182)	grad_norm 14715.1885 (17165.6055)	mem 4918MB
[2022-02-06 22:28:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4670/40036]	eta 1:25:53 lr 0.000007	time 0.1452 (0.1457)	loss 0.2177 (0.2182)	grad_norm 17701.1387 (17165.2070)	mem 4918MB
[2022-02-06 22:29:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4680/40036]	eta 1:25:51 lr 0.000007	time 0.1452 (0.1457)	loss 0.2167 (0.2182)	grad_norm 19407.7637 (17165.7617)	mem 4918MB
[2022-02-06 22:29:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4690/40036]	eta 1:25:50 lr 0.000007	time 0.1448 (0.1457)	loss 0.2187 (0.2182)	grad_norm 14369.7021 (17162.2461)	mem 4918MB
[2022-02-06 22:29:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4700/40036]	eta 1:25:48 lr 0.000007	time 0.1458 (0.1457)	loss 0.2178 (0.2182)	grad_norm 17340.1992 (17161.5430)	mem 4918MB
[2022-02-06 22:29:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4710/40036]	eta 1:25:47 lr 0.000007	time 0.1448 (0.1457)	loss 0.2171 (0.2182)	grad_norm 17700.3477 (17161.4727)	mem 4918MB
[2022-02-06 22:29:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4720/40036]	eta 1:25:46 lr 0.000007	time 0.1450 (0.1457)	loss 0.2159 (0.2182)	grad_norm 13422.1533 (17157.5312)	mem 4918MB
[2022-02-06 22:29:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4730/40036]	eta 1:25:44 lr 0.000007	time 0.1448 (0.1457)	loss 0.2141 (0.2182)	grad_norm 19217.9551 (17158.2246)	mem 4918MB
[2022-02-06 22:29:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4740/40036]	eta 1:25:43 lr 0.000007	time 0.1451 (0.1457)	loss 0.2177 (0.2182)	grad_norm 14880.0684 (17154.7246)	mem 4918MB
[2022-02-06 22:29:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4750/40036]	eta 1:25:41 lr 0.000007	time 0.1451 (0.1457)	loss 0.2195 (0.2182)	grad_norm 17645.7715 (17153.9316)	mem 4918MB
[2022-02-06 22:29:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4760/40036]	eta 1:25:40 lr 0.000007	time 0.1457 (0.1457)	loss 0.2181 (0.2182)	grad_norm 17864.6816 (17153.1406)	mem 4918MB
[2022-02-06 22:29:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4770/40036]	eta 1:25:39 lr 0.000007	time 0.1460 (0.1457)	loss 0.2184 (0.2182)	grad_norm 17234.1953 (17153.7578)	mem 4918MB
[2022-02-06 22:29:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4780/40036]	eta 1:25:37 lr 0.000007	time 0.1448 (0.1457)	loss 0.2161 (0.2182)	grad_norm 14421.4883 (17152.4180)	mem 4918MB
[2022-02-06 22:29:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4790/40036]	eta 1:25:36 lr 0.000007	time 0.1456 (0.1457)	loss 0.2195 (0.2182)	grad_norm 16136.1484 (17149.6504)	mem 4918MB
[2022-02-06 22:29:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4800/40036]	eta 1:25:34 lr 0.000007	time 0.1454 (0.1457)	loss 0.2188 (0.2182)	grad_norm 17030.6152 (17149.1934)	mem 4918MB
[2022-02-06 22:29:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4810/40036]	eta 1:25:33 lr 0.000007	time 0.1457 (0.1457)	loss 0.2161 (0.2182)	grad_norm 17196.4902 (17148.4355)	mem 4918MB
[2022-02-06 22:29:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4820/40036]	eta 1:25:31 lr 0.000007	time 0.1449 (0.1457)	loss 0.2190 (0.2182)	grad_norm 20693.5469 (17147.5664)	mem 4918MB
[2022-02-06 22:29:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4830/40036]	eta 1:25:30 lr 0.000007	time 0.1450 (0.1457)	loss 0.2174 (0.2182)	grad_norm 15921.8389 (17147.4355)	mem 4918MB
[2022-02-06 22:29:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4840/40036]	eta 1:25:29 lr 0.000007	time 0.1450 (0.1457)	loss 0.2205 (0.2182)	grad_norm 15148.1025 (17145.2441)	mem 4918MB
[2022-02-06 22:29:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4850/40036]	eta 1:25:27 lr 0.000007	time 0.1439 (0.1457)	loss 0.2166 (0.2182)	grad_norm 15300.2607 (17144.8809)	mem 4918MB
[2022-02-06 22:29:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4860/40036]	eta 1:25:26 lr 0.000007	time 0.1450 (0.1457)	loss 0.2184 (0.2182)	grad_norm 18899.3359 (17145.8887)	mem 4918MB
[2022-02-06 22:29:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4870/40036]	eta 1:25:24 lr 0.000007	time 0.1448 (0.1457)	loss 0.2182 (0.2182)	grad_norm 19777.5684 (17146.3145)	mem 4918MB
[2022-02-06 22:29:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4880/40036]	eta 1:25:23 lr 0.000007	time 0.1447 (0.1457)	loss 0.2195 (0.2182)	grad_norm 16084.7969 (17144.8418)	mem 4918MB
[2022-02-06 22:29:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4890/40036]	eta 1:25:21 lr 0.000007	time 0.1449 (0.1457)	loss 0.2184 (0.2182)	grad_norm 15025.9551 (17142.3965)	mem 4918MB
[2022-02-06 22:29:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4900/40036]	eta 1:25:20 lr 0.000007	time 0.1450 (0.1457)	loss 0.2222 (0.2182)	grad_norm 18348.4375 (17142.4414)	mem 4918MB
[2022-02-06 22:29:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4910/40036]	eta 1:25:19 lr 0.000007	time 0.1450 (0.1457)	loss 0.2170 (0.2182)	grad_norm 18574.1602 (17143.5117)	mem 4918MB
[2022-02-06 22:29:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4920/40036]	eta 1:25:17 lr 0.000007	time 0.1452 (0.1457)	loss 0.2169 (0.2182)	grad_norm 16938.4844 (17140.9102)	mem 4918MB
[2022-02-06 22:29:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4930/40036]	eta 1:25:16 lr 0.000007	time 0.1456 (0.1457)	loss 0.2184 (0.2182)	grad_norm 14632.4492 (17139.9414)	mem 4918MB
[2022-02-06 22:29:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4940/40036]	eta 1:25:14 lr 0.000007	time 0.1457 (0.1457)	loss 0.2187 (0.2182)	grad_norm 16958.5156 (17138.9336)	mem 4918MB
[2022-02-06 22:29:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4950/40036]	eta 1:25:13 lr 0.000007	time 0.1456 (0.1457)	loss 0.2184 (0.2182)	grad_norm 16899.5645 (17138.7773)	mem 4918MB
[2022-02-06 22:29:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4960/40036]	eta 1:25:12 lr 0.000007	time 0.1449 (0.1457)	loss 0.2181 (0.2182)	grad_norm 17110.1777 (17137.4766)	mem 4918MB
[2022-02-06 22:29:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4970/40036]	eta 1:25:10 lr 0.000007	time 0.1461 (0.1457)	loss 0.2166 (0.2182)	grad_norm 17264.7734 (17137.1016)	mem 4918MB
[2022-02-06 22:29:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4980/40036]	eta 1:25:09 lr 0.000007	time 0.1448 (0.1457)	loss 0.2166 (0.2182)	grad_norm 13997.5439 (17132.8262)	mem 4918MB
[2022-02-06 22:29:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][4990/40036]	eta 1:25:07 lr 0.000007	time 0.1455 (0.1457)	loss 0.2192 (0.2182)	grad_norm 16535.7148 (17130.9258)	mem 4918MB
[2022-02-06 22:29:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5000/40036]	eta 1:25:06 lr 0.000007	time 0.1458 (0.1457)	loss 0.2178 (0.2182)	grad_norm 15888.7539 (17127.9629)	mem 4918MB
[2022-02-06 22:29:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5010/40036]	eta 1:25:04 lr 0.000007	time 0.1454 (0.1457)	loss 0.2176 (0.2182)	grad_norm 15997.2900 (17126.1523)	mem 4918MB
[2022-02-06 22:29:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5020/40036]	eta 1:25:03 lr 0.000007	time 0.1462 (0.1457)	loss 0.2191 (0.2182)	grad_norm 18550.8926 (17125.2754)	mem 4918MB
[2022-02-06 22:29:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5030/40036]	eta 1:25:02 lr 0.000007	time 0.1456 (0.1458)	loss 0.2212 (0.2182)	grad_norm 18724.9648 (17125.8027)	mem 4918MB
[2022-02-06 22:29:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5040/40036]	eta 1:25:00 lr 0.000007	time 0.1453 (0.1458)	loss 0.2147 (0.2182)	grad_norm 17338.1582 (17124.7871)	mem 4918MB
[2022-02-06 22:29:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5050/40036]	eta 1:24:59 lr 0.000007	time 0.1454 (0.1458)	loss 0.2197 (0.2182)	grad_norm 19531.0762 (17123.9902)	mem 4918MB
[2022-02-06 22:29:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5060/40036]	eta 1:24:58 lr 0.000007	time 0.1444 (0.1458)	loss 0.2175 (0.2182)	grad_norm 14849.2891 (17122.6152)	mem 4918MB
[2022-02-06 22:29:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5070/40036]	eta 1:24:56 lr 0.000007	time 0.1456 (0.1458)	loss 0.2153 (0.2182)	grad_norm 18211.3809 (17122.4141)	mem 4918MB
[2022-02-06 22:29:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5080/40036]	eta 1:24:55 lr 0.000007	time 0.1458 (0.1458)	loss 0.2199 (0.2182)	grad_norm 14245.0273 (17119.9258)	mem 4918MB
[2022-02-06 22:30:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5090/40036]	eta 1:24:53 lr 0.000007	time 0.1459 (0.1458)	loss 0.2154 (0.2182)	grad_norm 14758.7549 (17118.9414)	mem 4918MB
[2022-02-06 22:30:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5100/40036]	eta 1:24:52 lr 0.000007	time 0.1459 (0.1458)	loss 0.2197 (0.2182)	grad_norm 16170.6436 (17118.8379)	mem 4918MB
[2022-02-06 22:30:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5110/40036]	eta 1:24:51 lr 0.000007	time 0.1457 (0.1458)	loss 0.2192 (0.2182)	grad_norm 19745.2227 (17117.7695)	mem 4918MB
[2022-02-06 22:30:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5120/40036]	eta 1:24:49 lr 0.000007	time 0.1440 (0.1458)	loss 0.2182 (0.2182)	grad_norm 17155.5156 (17117.4805)	mem 4918MB
[2022-02-06 22:30:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5130/40036]	eta 1:24:48 lr 0.000007	time 0.1447 (0.1458)	loss 0.2188 (0.2182)	grad_norm 15273.5977 (17116.1250)	mem 4918MB
[2022-02-06 22:30:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5140/40036]	eta 1:24:46 lr 0.000007	time 0.1456 (0.1458)	loss 0.2156 (0.2182)	grad_norm 12867.4834 (17113.3867)	mem 4918MB
[2022-02-06 22:30:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5150/40036]	eta 1:24:45 lr 0.000007	time 0.1444 (0.1458)	loss 0.2154 (0.2182)	grad_norm 14104.5625 (17112.4355)	mem 4918MB
[2022-02-06 22:30:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5160/40036]	eta 1:24:44 lr 0.000007	time 0.1453 (0.1458)	loss 0.2169 (0.2182)	grad_norm 14933.0615 (17110.2676)	mem 4918MB
[2022-02-06 22:30:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5170/40036]	eta 1:24:42 lr 0.000007	time 0.1461 (0.1458)	loss 0.2204 (0.2182)	grad_norm 18360.4980 (17109.8535)	mem 4918MB
[2022-02-06 22:30:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5180/40036]	eta 1:24:41 lr 0.000007	time 0.1459 (0.1458)	loss 0.2215 (0.2182)	grad_norm 16840.0508 (17108.3789)	mem 4918MB
[2022-02-06 22:30:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5190/40036]	eta 1:24:39 lr 0.000007	time 0.1450 (0.1458)	loss 0.2183 (0.2182)	grad_norm 14216.8271 (17107.4609)	mem 4918MB
[2022-02-06 22:30:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5200/40036]	eta 1:24:38 lr 0.000007	time 0.1462 (0.1458)	loss 0.2162 (0.2182)	grad_norm 17013.6738 (17105.3477)	mem 4918MB
[2022-02-06 22:30:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5210/40036]	eta 1:24:36 lr 0.000007	time 0.1452 (0.1458)	loss 0.2190 (0.2182)	grad_norm 14109.9043 (17103.6152)	mem 4918MB
[2022-02-06 22:30:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5220/40036]	eta 1:24:35 lr 0.000008	time 0.1443 (0.1458)	loss 0.2164 (0.2182)	grad_norm 16351.5205 (17102.5293)	mem 4918MB
[2022-02-06 22:30:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5230/40036]	eta 1:24:34 lr 0.000008	time 0.1455 (0.1458)	loss 0.2148 (0.2182)	grad_norm 14761.1064 (17100.8809)	mem 4918MB
[2022-02-06 22:30:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5240/40036]	eta 1:24:32 lr 0.000008	time 0.1459 (0.1458)	loss 0.2165 (0.2182)	grad_norm 14522.6953 (17098.2070)	mem 4918MB
[2022-02-06 22:30:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5250/40036]	eta 1:24:31 lr 0.000008	time 0.1458 (0.1458)	loss 0.2209 (0.2182)	grad_norm 16936.0020 (17097.8301)	mem 4918MB
[2022-02-06 22:30:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5260/40036]	eta 1:24:30 lr 0.000008	time 0.1454 (0.1458)	loss 0.2147 (0.2182)	grad_norm 14929.7461 (17095.3125)	mem 4918MB
[2022-02-06 22:30:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5270/40036]	eta 1:24:28 lr 0.000008	time 0.1446 (0.1458)	loss 0.2212 (0.2182)	grad_norm 15988.0322 (17093.6035)	mem 4918MB
[2022-02-06 22:30:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5280/40036]	eta 1:24:27 lr 0.000008	time 0.1451 (0.1458)	loss 0.2201 (0.2182)	grad_norm 14402.3252 (17091.3574)	mem 4918MB
[2022-02-06 22:30:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5290/40036]	eta 1:24:25 lr 0.000008	time 0.1457 (0.1458)	loss 0.2189 (0.2182)	grad_norm 15213.7734 (17090.1426)	mem 4918MB
[2022-02-06 22:30:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5300/40036]	eta 1:24:24 lr 0.000008	time 0.1449 (0.1458)	loss 0.2149 (0.2182)	grad_norm 14485.7842 (17088.5723)	mem 4918MB
[2022-02-06 22:30:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5310/40036]	eta 1:24:22 lr 0.000008	time 0.1457 (0.1458)	loss 0.2190 (0.2182)	grad_norm 16842.0137 (17087.1543)	mem 4918MB
[2022-02-06 22:30:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5320/40036]	eta 1:24:21 lr 0.000008	time 0.1453 (0.1458)	loss 0.2157 (0.2182)	grad_norm 17439.6816 (17087.0996)	mem 4918MB
[2022-02-06 22:30:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5330/40036]	eta 1:24:20 lr 0.000008	time 0.1457 (0.1458)	loss 0.2179 (0.2182)	grad_norm 16260.7715 (17085.8457)	mem 4918MB
[2022-02-06 22:30:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5340/40036]	eta 1:24:18 lr 0.000008	time 0.1454 (0.1458)	loss 0.2171 (0.2182)	grad_norm 14116.5156 (17085.0176)	mem 4918MB
[2022-02-06 22:30:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5350/40036]	eta 1:24:17 lr 0.000008	time 0.1448 (0.1458)	loss 0.2192 (0.2182)	grad_norm 14545.5938 (17083.0273)	mem 4918MB
[2022-02-06 22:30:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5360/40036]	eta 1:24:16 lr 0.000008	time 0.1463 (0.1458)	loss 0.2174 (0.2182)	grad_norm 17285.3887 (17082.7891)	mem 4918MB
[2022-02-06 22:30:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5370/40036]	eta 1:24:14 lr 0.000008	time 0.1453 (0.1458)	loss 0.2172 (0.2182)	grad_norm 15318.7822 (17081.5957)	mem 4918MB
[2022-02-06 22:30:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5380/40036]	eta 1:24:13 lr 0.000008	time 0.1453 (0.1458)	loss 0.2178 (0.2182)	grad_norm 16832.4492 (17081.0957)	mem 4918MB
[2022-02-06 22:30:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5390/40036]	eta 1:24:11 lr 0.000008	time 0.1452 (0.1458)	loss 0.2186 (0.2182)	grad_norm 17934.9473 (17080.3047)	mem 4918MB
[2022-02-06 22:30:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5400/40036]	eta 1:24:10 lr 0.000008	time 0.1445 (0.1458)	loss 0.2187 (0.2182)	grad_norm 15657.1162 (17077.3867)	mem 4918MB
[2022-02-06 22:30:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5410/40036]	eta 1:24:09 lr 0.000008	time 0.1440 (0.1458)	loss 0.2164 (0.2182)	grad_norm 16586.6426 (17076.6211)	mem 4918MB
[2022-02-06 22:30:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5420/40036]	eta 1:24:07 lr 0.000008	time 0.1462 (0.1458)	loss 0.2203 (0.2182)	grad_norm 18560.4199 (17074.6621)	mem 4918MB
[2022-02-06 22:30:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5430/40036]	eta 1:24:06 lr 0.000008	time 0.1455 (0.1458)	loss 0.2179 (0.2182)	grad_norm 15867.7021 (17074.7988)	mem 4918MB
[2022-02-06 22:30:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5440/40036]	eta 1:24:04 lr 0.000008	time 0.1442 (0.1458)	loss 0.2175 (0.2182)	grad_norm 14331.3740 (17072.5137)	mem 4918MB
[2022-02-06 22:30:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5450/40036]	eta 1:24:03 lr 0.000008	time 0.1463 (0.1458)	loss 0.2188 (0.2182)	grad_norm 16255.1611 (17070.0098)	mem 4918MB
[2022-02-06 22:30:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5460/40036]	eta 1:24:01 lr 0.000008	time 0.1458 (0.1458)	loss 0.2190 (0.2182)	grad_norm 15919.8486 (17068.9824)	mem 4918MB
[2022-02-06 22:30:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5470/40036]	eta 1:24:00 lr 0.000008	time 0.1454 (0.1458)	loss 0.2157 (0.2182)	grad_norm 15807.2754 (17065.8047)	mem 4918MB
[2022-02-06 22:30:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5480/40036]	eta 1:23:59 lr 0.000008	time 0.1461 (0.1458)	loss 0.2193 (0.2182)	grad_norm 16935.7402 (17064.0332)	mem 4918MB
[2022-02-06 22:30:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5490/40036]	eta 1:23:57 lr 0.000008	time 0.1449 (0.1458)	loss 0.2157 (0.2182)	grad_norm 14670.8291 (17062.4160)	mem 4918MB
[2022-02-06 22:31:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5500/40036]	eta 1:23:56 lr 0.000008	time 0.1453 (0.1458)	loss 0.2167 (0.2182)	grad_norm 17324.2949 (17061.0762)	mem 4918MB
[2022-02-06 22:31:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5510/40036]	eta 1:23:54 lr 0.000008	time 0.1454 (0.1458)	loss 0.2145 (0.2182)	grad_norm 16416.8613 (17060.4199)	mem 4918MB
[2022-02-06 22:31:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5520/40036]	eta 1:23:53 lr 0.000008	time 0.1453 (0.1458)	loss 0.2182 (0.2182)	grad_norm 14636.9531 (17059.0312)	mem 4918MB
[2022-02-06 22:31:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5530/40036]	eta 1:23:52 lr 0.000008	time 0.1456 (0.1458)	loss 0.2181 (0.2182)	grad_norm 14422.5342 (17058.2070)	mem 4918MB
[2022-02-06 22:31:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5540/40036]	eta 1:23:50 lr 0.000008	time 0.1453 (0.1458)	loss 0.2189 (0.2182)	grad_norm 17471.6172 (17057.6641)	mem 4918MB
[2022-02-06 22:31:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5550/40036]	eta 1:23:49 lr 0.000008	time 0.1448 (0.1458)	loss 0.2193 (0.2182)	grad_norm 14656.4990 (17056.5176)	mem 4918MB
[2022-02-06 22:31:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5560/40036]	eta 1:23:47 lr 0.000008	time 0.1448 (0.1458)	loss 0.2167 (0.2182)	grad_norm 17898.8789 (17056.6445)	mem 4918MB
[2022-02-06 22:31:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5570/40036]	eta 1:23:46 lr 0.000008	time 0.1456 (0.1458)	loss 0.2168 (0.2182)	grad_norm 19252.9844 (17055.9570)	mem 4918MB
[2022-02-06 22:31:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5580/40036]	eta 1:23:45 lr 0.000008	time 0.1451 (0.1458)	loss 0.2209 (0.2182)	grad_norm 16020.2998 (17055.4453)	mem 4918MB
[2022-02-06 22:31:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5590/40036]	eta 1:23:43 lr 0.000008	time 0.1454 (0.1458)	loss 0.2165 (0.2182)	grad_norm 17931.1816 (17053.0742)	mem 4918MB
[2022-02-06 22:31:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5600/40036]	eta 1:23:42 lr 0.000008	time 0.1449 (0.1458)	loss 0.2194 (0.2182)	grad_norm 18681.1270 (17053.0977)	mem 4918MB
[2022-02-06 22:31:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5610/40036]	eta 1:23:40 lr 0.000008	time 0.1446 (0.1458)	loss 0.2194 (0.2182)	grad_norm 18404.3398 (17052.9121)	mem 4918MB
[2022-02-06 22:31:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5620/40036]	eta 1:23:39 lr 0.000008	time 0.1454 (0.1458)	loss 0.2184 (0.2182)	grad_norm 16750.1250 (17051.8027)	mem 4918MB
[2022-02-06 22:31:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5630/40036]	eta 1:23:37 lr 0.000008	time 0.1457 (0.1458)	loss 0.2175 (0.2182)	grad_norm 16055.5059 (17050.8359)	mem 4918MB
[2022-02-06 22:31:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5640/40036]	eta 1:23:36 lr 0.000008	time 0.1457 (0.1458)	loss 0.2200 (0.2182)	grad_norm 14787.5811 (17048.6562)	mem 4918MB
[2022-02-06 22:31:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5650/40036]	eta 1:23:35 lr 0.000008	time 0.1452 (0.1458)	loss 0.2173 (0.2182)	grad_norm 15886.6572 (17048.5215)	mem 4918MB
[2022-02-06 22:31:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5660/40036]	eta 1:23:33 lr 0.000008	time 0.1453 (0.1459)	loss 0.2171 (0.2182)	grad_norm 16601.6738 (17047.1250)	mem 4918MB
[2022-02-06 22:31:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5670/40036]	eta 1:23:32 lr 0.000008	time 0.1451 (0.1459)	loss 0.2180 (0.2182)	grad_norm 15290.4131 (17046.9922)	mem 4918MB
[2022-02-06 22:31:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5680/40036]	eta 1:23:30 lr 0.000008	time 0.1456 (0.1459)	loss 0.2180 (0.2182)	grad_norm 16393.8711 (17047.0098)	mem 4918MB
[2022-02-06 22:31:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5690/40036]	eta 1:23:29 lr 0.000008	time 0.1455 (0.1459)	loss 0.2172 (0.2182)	grad_norm 15777.5967 (17044.9902)	mem 4918MB
[2022-02-06 22:31:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5700/40036]	eta 1:23:28 lr 0.000008	time 0.1444 (0.1459)	loss 0.2152 (0.2182)	grad_norm 16395.2539 (17043.6055)	mem 4918MB
[2022-02-06 22:31:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5710/40036]	eta 1:23:26 lr 0.000008	time 0.1447 (0.1459)	loss 0.2154 (0.2182)	grad_norm 15883.8779 (17042.7148)	mem 4918MB
[2022-02-06 22:31:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5720/40036]	eta 1:23:25 lr 0.000008	time 0.1452 (0.1459)	loss 0.2173 (0.2182)	grad_norm 13881.6455 (17041.8984)	mem 4918MB
[2022-02-06 22:31:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5730/40036]	eta 1:23:23 lr 0.000008	time 0.1446 (0.1459)	loss 0.2201 (0.2182)	grad_norm 17787.8906 (17040.9707)	mem 4918MB
[2022-02-06 22:31:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5740/40036]	eta 1:23:22 lr 0.000008	time 0.1456 (0.1459)	loss 0.2201 (0.2182)	grad_norm 17080.4824 (17039.0371)	mem 4918MB
[2022-02-06 22:31:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5750/40036]	eta 1:23:21 lr 0.000008	time 0.1464 (0.1459)	loss 0.2166 (0.2182)	grad_norm 15542.3174 (17038.2812)	mem 4918MB
[2022-02-06 22:31:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5760/40036]	eta 1:23:19 lr 0.000008	time 0.1446 (0.1459)	loss 0.2199 (0.2182)	grad_norm 15164.2988 (17036.7168)	mem 4918MB
[2022-02-06 22:31:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5770/40036]	eta 1:23:18 lr 0.000008	time 0.1448 (0.1459)	loss 0.2168 (0.2182)	grad_norm 17627.4668 (17034.8652)	mem 4918MB
[2022-02-06 22:31:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5780/40036]	eta 1:23:16 lr 0.000008	time 0.1462 (0.1459)	loss 0.2197 (0.2182)	grad_norm 18371.0684 (17033.5371)	mem 4918MB
[2022-02-06 22:31:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5790/40036]	eta 1:23:15 lr 0.000008	time 0.1448 (0.1459)	loss 0.2204 (0.2182)	grad_norm 16779.0117 (17031.6660)	mem 4918MB
[2022-02-06 22:31:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5800/40036]	eta 1:23:13 lr 0.000008	time 0.1451 (0.1459)	loss 0.2190 (0.2182)	grad_norm 15808.5020 (17030.4668)	mem 4918MB
[2022-02-06 22:31:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5810/40036]	eta 1:23:12 lr 0.000008	time 0.1454 (0.1459)	loss 0.2169 (0.2182)	grad_norm 14523.7168 (17029.1816)	mem 4918MB
[2022-02-06 22:31:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5820/40036]	eta 1:23:11 lr 0.000008	time 0.1450 (0.1459)	loss 0.2177 (0.2182)	grad_norm 15362.5488 (17028.0332)	mem 4918MB
[2022-02-06 22:31:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5830/40036]	eta 1:23:09 lr 0.000008	time 0.1452 (0.1459)	loss 0.2153 (0.2182)	grad_norm 16297.8516 (17026.0410)	mem 4918MB
[2022-02-06 22:31:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5840/40036]	eta 1:23:08 lr 0.000008	time 0.1450 (0.1459)	loss 0.2169 (0.2182)	grad_norm 16347.9219 (17024.6621)	mem 4918MB
[2022-02-06 22:31:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5850/40036]	eta 1:23:06 lr 0.000008	time 0.1456 (0.1459)	loss 0.2184 (0.2182)	grad_norm 18457.7949 (17023.4141)	mem 4918MB
[2022-02-06 22:31:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5860/40036]	eta 1:23:05 lr 0.000008	time 0.1460 (0.1459)	loss 0.2149 (0.2182)	grad_norm 15111.9277 (17020.8398)	mem 4918MB
[2022-02-06 22:31:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5870/40036]	eta 1:23:04 lr 0.000008	time 0.1450 (0.1459)	loss 0.2172 (0.2182)	grad_norm 17594.3359 (17019.0000)	mem 4918MB
[2022-02-06 22:31:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5880/40036]	eta 1:23:02 lr 0.000008	time 0.1455 (0.1459)	loss 0.2176 (0.2182)	grad_norm 15895.9062 (17017.5234)	mem 4918MB
[2022-02-06 22:31:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5890/40036]	eta 1:23:01 lr 0.000008	time 0.1455 (0.1459)	loss 0.2195 (0.2182)	grad_norm 14736.8945 (17015.3418)	mem 4918MB
[2022-02-06 22:31:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5900/40036]	eta 1:22:59 lr 0.000008	time 0.1456 (0.1459)	loss 0.2153 (0.2182)	grad_norm 15768.5566 (17016.1523)	mem 4918MB
[2022-02-06 22:32:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5910/40036]	eta 1:22:58 lr 0.000008	time 0.1463 (0.1459)	loss 0.2177 (0.2182)	grad_norm 16927.3906 (17014.7676)	mem 4918MB
[2022-02-06 22:32:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5920/40036]	eta 1:22:56 lr 0.000008	time 0.1450 (0.1459)	loss 0.2173 (0.2182)	grad_norm 18033.9707 (17013.9688)	mem 4918MB
[2022-02-06 22:32:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5930/40036]	eta 1:22:55 lr 0.000008	time 0.1450 (0.1459)	loss 0.2168 (0.2182)	grad_norm 14928.3145 (17013.4883)	mem 4918MB
[2022-02-06 22:32:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5940/40036]	eta 1:22:54 lr 0.000008	time 0.1462 (0.1459)	loss 0.2179 (0.2182)	grad_norm 17842.7559 (17010.6973)	mem 4918MB
[2022-02-06 22:32:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5950/40036]	eta 1:22:52 lr 0.000008	time 0.1455 (0.1459)	loss 0.2179 (0.2182)	grad_norm 13775.0664 (17008.1016)	mem 4918MB
[2022-02-06 22:32:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5960/40036]	eta 1:22:51 lr 0.000008	time 0.1452 (0.1459)	loss 0.2228 (0.2182)	grad_norm 16919.8145 (17006.8027)	mem 4918MB
[2022-02-06 22:32:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5970/40036]	eta 1:22:49 lr 0.000008	time 0.1459 (0.1459)	loss 0.2169 (0.2182)	grad_norm 17799.9492 (17006.2832)	mem 4918MB
[2022-02-06 22:32:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5980/40036]	eta 1:22:48 lr 0.000008	time 0.1454 (0.1459)	loss 0.2160 (0.2182)	grad_norm 15145.2480 (17004.9883)	mem 4918MB
[2022-02-06 22:32:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][5990/40036]	eta 1:22:47 lr 0.000008	time 0.1446 (0.1459)	loss 0.2194 (0.2182)	grad_norm 16949.0391 (17003.4219)	mem 4918MB
[2022-02-06 22:32:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6000/40036]	eta 1:22:45 lr 0.000008	time 0.1456 (0.1459)	loss 0.2159 (0.2182)	grad_norm 14916.8320 (17002.0938)	mem 4918MB
[2022-02-06 22:32:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6010/40036]	eta 1:22:44 lr 0.000008	time 0.1453 (0.1459)	loss 0.2174 (0.2182)	grad_norm 17288.2129 (16999.7539)	mem 4918MB
[2022-02-06 22:32:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6020/40036]	eta 1:22:42 lr 0.000009	time 0.1452 (0.1459)	loss 0.2182 (0.2182)	grad_norm 13642.6914 (16998.5918)	mem 4918MB
[2022-02-06 22:32:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6030/40036]	eta 1:22:41 lr 0.000009	time 0.1447 (0.1459)	loss 0.2177 (0.2182)	grad_norm 19068.5879 (16997.9355)	mem 4918MB
[2022-02-06 22:32:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6040/40036]	eta 1:22:39 lr 0.000009	time 0.1453 (0.1459)	loss 0.2165 (0.2182)	grad_norm 17947.7480 (16997.6816)	mem 4918MB
[2022-02-06 22:32:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6050/40036]	eta 1:22:38 lr 0.000009	time 0.1450 (0.1459)	loss 0.2160 (0.2182)	grad_norm 15247.7871 (16995.8789)	mem 4918MB
[2022-02-06 22:32:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6060/40036]	eta 1:22:37 lr 0.000009	time 0.1451 (0.1459)	loss 0.2159 (0.2182)	grad_norm 14035.8545 (16994.1289)	mem 4918MB
[2022-02-06 22:32:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6070/40036]	eta 1:22:35 lr 0.000009	time 0.1457 (0.1459)	loss 0.2207 (0.2182)	grad_norm 16255.4082 (16992.9980)	mem 4918MB
[2022-02-06 22:32:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6080/40036]	eta 1:22:34 lr 0.000009	time 0.1445 (0.1459)	loss 0.2170 (0.2182)	grad_norm 14507.8848 (16991.4062)	mem 4918MB
[2022-02-06 22:32:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6090/40036]	eta 1:22:32 lr 0.000009	time 0.1457 (0.1459)	loss 0.2180 (0.2182)	grad_norm 15626.8477 (16989.2148)	mem 4918MB
[2022-02-06 22:32:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6100/40036]	eta 1:22:31 lr 0.000009	time 0.1459 (0.1459)	loss 0.2175 (0.2182)	grad_norm 17094.6758 (16989.0176)	mem 4918MB
[2022-02-06 22:32:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6110/40036]	eta 1:22:29 lr 0.000009	time 0.1455 (0.1459)	loss 0.2202 (0.2182)	grad_norm 16266.8447 (16988.3125)	mem 4918MB
[2022-02-06 22:32:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6120/40036]	eta 1:22:28 lr 0.000009	time 0.1454 (0.1459)	loss 0.2147 (0.2182)	grad_norm 16076.0674 (16987.0039)	mem 4918MB
[2022-02-06 22:32:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6130/40036]	eta 1:22:27 lr 0.000009	time 0.1463 (0.1459)	loss 0.2182 (0.2182)	grad_norm 17625.2656 (16985.8789)	mem 4918MB
[2022-02-06 22:32:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6140/40036]	eta 1:22:25 lr 0.000009	time 0.1452 (0.1459)	loss 0.2187 (0.2182)	grad_norm 18293.8965 (16985.7676)	mem 4918MB
[2022-02-06 22:32:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6150/40036]	eta 1:22:24 lr 0.000009	time 0.1453 (0.1459)	loss 0.2196 (0.2182)	grad_norm 15945.3613 (16984.7285)	mem 4918MB
[2022-02-06 22:32:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6160/40036]	eta 1:22:22 lr 0.000009	time 0.1460 (0.1459)	loss 0.2161 (0.2182)	grad_norm 17733.6426 (16984.1289)	mem 4918MB
[2022-02-06 22:32:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6170/40036]	eta 1:22:21 lr 0.000009	time 0.1450 (0.1459)	loss 0.2199 (0.2182)	grad_norm 13886.4756 (16981.3789)	mem 4918MB
[2022-02-06 22:32:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6180/40036]	eta 1:22:20 lr 0.000009	time 0.1456 (0.1459)	loss 0.2198 (0.2182)	grad_norm 15132.6562 (16979.4492)	mem 4918MB
[2022-02-06 22:32:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6190/40036]	eta 1:22:18 lr 0.000009	time 0.1452 (0.1459)	loss 0.2188 (0.2182)	grad_norm 13919.1104 (16977.5156)	mem 4918MB
[2022-02-06 22:32:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6200/40036]	eta 1:22:17 lr 0.000009	time 0.1441 (0.1459)	loss 0.2178 (0.2182)	grad_norm 14918.0566 (16975.4336)	mem 4918MB
[2022-02-06 22:32:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6210/40036]	eta 1:22:15 lr 0.000009	time 0.1446 (0.1459)	loss 0.2171 (0.2182)	grad_norm 17086.8223 (16975.2754)	mem 4918MB
[2022-02-06 22:32:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6220/40036]	eta 1:22:14 lr 0.000009	time 0.1457 (0.1459)	loss 0.2168 (0.2182)	grad_norm 14355.1631 (16973.9316)	mem 4918MB
[2022-02-06 22:32:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6230/40036]	eta 1:22:12 lr 0.000009	time 0.1453 (0.1459)	loss 0.2162 (0.2182)	grad_norm 16618.8438 (16971.6758)	mem 4918MB
[2022-02-06 22:32:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6240/40036]	eta 1:22:11 lr 0.000009	time 0.1442 (0.1459)	loss 0.2171 (0.2182)	grad_norm 15729.8887 (16970.1953)	mem 4918MB
[2022-02-06 22:32:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6250/40036]	eta 1:22:09 lr 0.000009	time 0.1466 (0.1459)	loss 0.2188 (0.2182)	grad_norm 16953.0645 (16968.7441)	mem 4918MB
[2022-02-06 22:32:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6260/40036]	eta 1:22:08 lr 0.000009	time 0.1444 (0.1459)	loss 0.2184 (0.2182)	grad_norm 15708.5996 (16966.9141)	mem 4918MB
[2022-02-06 22:32:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6270/40036]	eta 1:22:07 lr 0.000009	time 0.1449 (0.1459)	loss 0.2148 (0.2182)	grad_norm 16701.6035 (16964.7109)	mem 4918MB
[2022-02-06 22:32:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6280/40036]	eta 1:22:05 lr 0.000009	time 0.1449 (0.1459)	loss 0.2186 (0.2182)	grad_norm 15431.6953 (16962.3906)	mem 4918MB
[2022-02-06 22:32:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6290/40036]	eta 1:22:04 lr 0.000009	time 0.1447 (0.1459)	loss 0.2201 (0.2182)	grad_norm 17673.7773 (16961.1660)	mem 4918MB
[2022-02-06 22:32:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6300/40036]	eta 1:22:02 lr 0.000009	time 0.1451 (0.1459)	loss 0.2157 (0.2182)	grad_norm 13949.4170 (16958.1797)	mem 4918MB
[2022-02-06 22:32:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6310/40036]	eta 1:22:01 lr 0.000009	time 0.1454 (0.1459)	loss 0.2200 (0.2182)	grad_norm 15373.4248 (16956.1113)	mem 4918MB
[2022-02-06 22:33:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6320/40036]	eta 1:21:59 lr 0.000009	time 0.1444 (0.1459)	loss 0.2169 (0.2182)	grad_norm 16633.2285 (16954.5449)	mem 4918MB
[2022-02-06 22:33:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6330/40036]	eta 1:21:58 lr 0.000009	time 0.1450 (0.1459)	loss 0.2166 (0.2182)	grad_norm 15213.2266 (16953.6348)	mem 4918MB
[2022-02-06 22:33:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6340/40036]	eta 1:21:56 lr 0.000009	time 0.1448 (0.1459)	loss 0.2200 (0.2182)	grad_norm 19320.8359 (16952.3398)	mem 4918MB
[2022-02-06 22:33:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6350/40036]	eta 1:21:55 lr 0.000009	time 0.1447 (0.1459)	loss 0.2168 (0.2182)	grad_norm 17221.5586 (16950.6953)	mem 4918MB
[2022-02-06 22:33:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6360/40036]	eta 1:21:54 lr 0.000009	time 0.1464 (0.1459)	loss 0.2194 (0.2182)	grad_norm 17050.0723 (16949.4160)	mem 4918MB
[2022-02-06 22:33:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6370/40036]	eta 1:21:52 lr 0.000009	time 0.1453 (0.1459)	loss 0.2169 (0.2182)	grad_norm 13766.4971 (16947.2441)	mem 4918MB
[2022-02-06 22:33:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6380/40036]	eta 1:21:51 lr 0.000009	time 0.1453 (0.1459)	loss 0.2200 (0.2182)	grad_norm 15783.6162 (16945.1055)	mem 4918MB
[2022-02-06 22:33:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6390/40036]	eta 1:21:49 lr 0.000009	time 0.1458 (0.1459)	loss 0.2183 (0.2182)	grad_norm 15676.2979 (16944.1465)	mem 4918MB
[2022-02-06 22:33:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6400/40036]	eta 1:21:48 lr 0.000009	time 0.1444 (0.1459)	loss 0.2188 (0.2182)	grad_norm 20234.6719 (16943.1172)	mem 4918MB
[2022-02-06 22:33:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6410/40036]	eta 1:21:46 lr 0.000009	time 0.1450 (0.1459)	loss 0.2188 (0.2182)	grad_norm 17328.5078 (16941.9863)	mem 4918MB
[2022-02-06 22:33:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6420/40036]	eta 1:21:45 lr 0.000009	time 0.1451 (0.1459)	loss 0.2192 (0.2182)	grad_norm 16154.9053 (16939.6758)	mem 4918MB
[2022-02-06 22:33:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6430/40036]	eta 1:21:43 lr 0.000009	time 0.1448 (0.1459)	loss 0.2206 (0.2182)	grad_norm 15072.2666 (16938.4609)	mem 4918MB
[2022-02-06 22:33:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6440/40036]	eta 1:21:42 lr 0.000009	time 0.1451 (0.1459)	loss 0.2164 (0.2182)	grad_norm 16309.0713 (16936.5332)	mem 4918MB
[2022-02-06 22:33:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6450/40036]	eta 1:21:41 lr 0.000009	time 0.1448 (0.1459)	loss 0.2176 (0.2182)	grad_norm 17795.3516 (16936.0586)	mem 4918MB
[2022-02-06 22:33:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6460/40036]	eta 1:21:39 lr 0.000009	time 0.1446 (0.1459)	loss 0.2173 (0.2182)	grad_norm 16558.5645 (16935.7852)	mem 4918MB
[2022-02-06 22:33:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6470/40036]	eta 1:21:38 lr 0.000009	time 0.1454 (0.1459)	loss 0.2184 (0.2182)	grad_norm 16198.4883 (16934.9531)	mem 4918MB
[2022-02-06 22:33:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6480/40036]	eta 1:21:36 lr 0.000009	time 0.1447 (0.1459)	loss 0.2182 (0.2182)	grad_norm 15784.0225 (16933.1738)	mem 4918MB
[2022-02-06 22:33:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6490/40036]	eta 1:21:35 lr 0.000009	time 0.1450 (0.1459)	loss 0.2188 (0.2182)	grad_norm 15024.4629 (16931.1758)	mem 4918MB
[2022-02-06 22:33:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6500/40036]	eta 1:21:33 lr 0.000009	time 0.1449 (0.1459)	loss 0.2167 (0.2182)	grad_norm 15694.5596 (16929.8086)	mem 4918MB
[2022-02-06 22:33:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6510/40036]	eta 1:21:32 lr 0.000009	time 0.1447 (0.1459)	loss 0.2172 (0.2182)	grad_norm 15500.4023 (16927.8066)	mem 4918MB
[2022-02-06 22:33:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6520/40036]	eta 1:21:30 lr 0.000009	time 0.1452 (0.1459)	loss 0.2172 (0.2182)	grad_norm 17745.0977 (16926.5879)	mem 4918MB
[2022-02-06 22:33:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6530/40036]	eta 1:21:29 lr 0.000009	time 0.1452 (0.1459)	loss 0.2151 (0.2182)	grad_norm 17745.3418 (16925.5039)	mem 4918MB
[2022-02-06 22:33:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6540/40036]	eta 1:21:28 lr 0.000009	time 0.1451 (0.1459)	loss 0.2167 (0.2182)	grad_norm 15789.4941 (16922.9473)	mem 4918MB
[2022-02-06 22:33:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6550/40036]	eta 1:21:26 lr 0.000009	time 0.1452 (0.1459)	loss 0.2195 (0.2182)	grad_norm 15255.1924 (16921.0938)	mem 4918MB
[2022-02-06 22:33:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6560/40036]	eta 1:21:25 lr 0.000009	time 0.1450 (0.1459)	loss 0.2159 (0.2182)	grad_norm 16713.8223 (16920.8691)	mem 4918MB
[2022-02-06 22:33:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6570/40036]	eta 1:21:23 lr 0.000009	time 0.1450 (0.1459)	loss 0.2205 (0.2182)	grad_norm 14614.7852 (16919.1758)	mem 4918MB
[2022-02-06 22:33:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6580/40036]	eta 1:21:22 lr 0.000009	time 0.1446 (0.1459)	loss 0.2166 (0.2182)	grad_norm 19010.0215 (16918.0605)	mem 4918MB
[2022-02-06 22:33:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6590/40036]	eta 1:21:20 lr 0.000009	time 0.1449 (0.1459)	loss 0.2182 (0.2182)	grad_norm 15330.4268 (16915.9512)	mem 4918MB
[2022-02-06 22:33:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6600/40036]	eta 1:21:19 lr 0.000009	time 0.1445 (0.1459)	loss 0.2193 (0.2182)	grad_norm 18367.7227 (16914.6641)	mem 4918MB
[2022-02-06 22:33:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6610/40036]	eta 1:21:17 lr 0.000009	time 0.1456 (0.1459)	loss 0.2165 (0.2182)	grad_norm 14083.4697 (16913.6152)	mem 4918MB
[2022-02-06 22:33:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6620/40036]	eta 1:21:16 lr 0.000009	time 0.1449 (0.1459)	loss 0.2183 (0.2182)	grad_norm 16854.0078 (16911.2656)	mem 4918MB
[2022-02-06 22:33:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6630/40036]	eta 1:21:15 lr 0.000009	time 0.1450 (0.1459)	loss 0.2174 (0.2182)	grad_norm 17750.3828 (16911.0547)	mem 4918MB
[2022-02-06 22:33:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6640/40036]	eta 1:21:13 lr 0.000009	time 0.1445 (0.1459)	loss 0.2185 (0.2182)	grad_norm 16934.3535 (16908.5742)	mem 4918MB
[2022-02-06 22:33:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6650/40036]	eta 1:21:12 lr 0.000009	time 0.1449 (0.1459)	loss 0.2186 (0.2182)	grad_norm 16375.5986 (16907.0820)	mem 4918MB
[2022-02-06 22:33:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6660/40036]	eta 1:21:10 lr 0.000009	time 0.1452 (0.1459)	loss 0.2177 (0.2182)	grad_norm 16841.4355 (16905.0781)	mem 4918MB
[2022-02-06 22:33:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6670/40036]	eta 1:21:09 lr 0.000009	time 0.1445 (0.1459)	loss 0.2217 (0.2182)	grad_norm 18039.6172 (16904.8848)	mem 4918MB
[2022-02-06 22:33:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6680/40036]	eta 1:21:07 lr 0.000009	time 0.1446 (0.1459)	loss 0.2166 (0.2182)	grad_norm 15029.8828 (16903.7227)	mem 4918MB
[2022-02-06 22:33:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6690/40036]	eta 1:21:06 lr 0.000009	time 0.1451 (0.1459)	loss 0.2164 (0.2182)	grad_norm 19018.0254 (16902.1152)	mem 4918MB
[2022-02-06 22:33:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6700/40036]	eta 1:21:04 lr 0.000009	time 0.1446 (0.1459)	loss 0.2185 (0.2182)	grad_norm 18676.4922 (16900.5996)	mem 4918MB
[2022-02-06 22:33:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6710/40036]	eta 1:21:03 lr 0.000009	time 0.1447 (0.1459)	loss 0.2169 (0.2182)	grad_norm 14836.9482 (16897.9746)	mem 4918MB
[2022-02-06 22:33:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6720/40036]	eta 1:21:02 lr 0.000009	time 0.1441 (0.1459)	loss 0.2199 (0.2182)	grad_norm 13858.8057 (16895.6016)	mem 4918MB
[2022-02-06 22:34:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6730/40036]	eta 1:21:00 lr 0.000009	time 0.1457 (0.1459)	loss 0.2175 (0.2182)	grad_norm 13475.3613 (16892.6660)	mem 4918MB
[2022-02-06 22:34:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6740/40036]	eta 1:20:59 lr 0.000009	time 0.1453 (0.1459)	loss 0.2205 (0.2182)	grad_norm 15735.0518 (16891.2285)	mem 4918MB
[2022-02-06 22:34:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6750/40036]	eta 1:20:57 lr 0.000009	time 0.1448 (0.1459)	loss 0.2180 (0.2182)	grad_norm 15193.1270 (16891.4395)	mem 4918MB
[2022-02-06 22:34:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6760/40036]	eta 1:20:56 lr 0.000009	time 0.1447 (0.1459)	loss 0.2180 (0.2182)	grad_norm 14336.3311 (16889.4102)	mem 4918MB
[2022-02-06 22:34:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6770/40036]	eta 1:20:54 lr 0.000009	time 0.1445 (0.1459)	loss 0.2199 (0.2182)	grad_norm 16658.6191 (16888.9922)	mem 4918MB
[2022-02-06 22:34:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6780/40036]	eta 1:20:53 lr 0.000009	time 0.1446 (0.1459)	loss 0.2214 (0.2182)	grad_norm 13571.1768 (16886.3145)	mem 4918MB
[2022-02-06 22:34:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6790/40036]	eta 1:20:51 lr 0.000009	time 0.1446 (0.1459)	loss 0.2205 (0.2182)	grad_norm 16490.2910 (16884.6445)	mem 4918MB
[2022-02-06 22:34:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6800/40036]	eta 1:20:50 lr 0.000009	time 0.1456 (0.1459)	loss 0.2189 (0.2182)	grad_norm 16703.9043 (16882.4512)	mem 4918MB
[2022-02-06 22:34:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6810/40036]	eta 1:20:48 lr 0.000009	time 0.1454 (0.1459)	loss 0.2200 (0.2182)	grad_norm 16446.3242 (16881.0742)	mem 4918MB
[2022-02-06 22:34:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6820/40036]	eta 1:20:47 lr 0.000010	time 0.1455 (0.1459)	loss 0.2166 (0.2182)	grad_norm 15912.0254 (16881.0762)	mem 4918MB
[2022-02-06 22:34:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6830/40036]	eta 1:20:46 lr 0.000010	time 0.1449 (0.1459)	loss 0.2165 (0.2182)	grad_norm 15260.1709 (16879.0000)	mem 4918MB
[2022-02-06 22:34:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6840/40036]	eta 1:20:44 lr 0.000010	time 0.1445 (0.1459)	loss 0.2196 (0.2182)	grad_norm 17256.5371 (16878.6660)	mem 4918MB
[2022-02-06 22:34:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6850/40036]	eta 1:20:43 lr 0.000010	time 0.1447 (0.1459)	loss 0.2183 (0.2182)	grad_norm 18810.6445 (16878.1484)	mem 4918MB
[2022-02-06 22:34:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6860/40036]	eta 1:20:41 lr 0.000010	time 0.1454 (0.1459)	loss 0.2160 (0.2182)	grad_norm 16121.4092 (16877.6328)	mem 4918MB
[2022-02-06 22:34:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6870/40036]	eta 1:20:40 lr 0.000010	time 0.1450 (0.1459)	loss 0.2151 (0.2182)	grad_norm 15621.7236 (16875.6562)	mem 4918MB
[2022-02-06 22:34:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6880/40036]	eta 1:20:38 lr 0.000010	time 0.1445 (0.1459)	loss 0.2194 (0.2182)	grad_norm 13591.8262 (16874.4902)	mem 4918MB
[2022-02-06 22:34:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6890/40036]	eta 1:20:37 lr 0.000010	time 0.1451 (0.1459)	loss 0.2182 (0.2182)	grad_norm 15670.9150 (16873.6934)	mem 4918MB
[2022-02-06 22:34:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6900/40036]	eta 1:20:35 lr 0.000010	time 0.1449 (0.1459)	loss 0.2189 (0.2182)	grad_norm 17488.2129 (16872.1602)	mem 4918MB
[2022-02-06 22:34:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6910/40036]	eta 1:20:34 lr 0.000010	time 0.1470 (0.1459)	loss 0.2197 (0.2182)	grad_norm 18028.6641 (16870.9043)	mem 4918MB
[2022-02-06 22:34:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6920/40036]	eta 1:20:33 lr 0.000010	time 0.1450 (0.1459)	loss 0.2198 (0.2182)	grad_norm 13435.3311 (16869.4590)	mem 4918MB
[2022-02-06 22:34:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6930/40036]	eta 1:20:31 lr 0.000010	time 0.1446 (0.1459)	loss 0.2159 (0.2182)	grad_norm 17239.8672 (16868.9043)	mem 4918MB
[2022-02-06 22:34:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6940/40036]	eta 1:20:30 lr 0.000010	time 0.1450 (0.1459)	loss 0.2178 (0.2182)	grad_norm 17303.4277 (16868.2188)	mem 4918MB
[2022-02-06 22:34:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6950/40036]	eta 1:20:28 lr 0.000010	time 0.1449 (0.1459)	loss 0.2180 (0.2182)	grad_norm 14376.0078 (16866.7480)	mem 4918MB
[2022-02-06 22:34:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6960/40036]	eta 1:20:27 lr 0.000010	time 0.1453 (0.1459)	loss 0.2207 (0.2182)	grad_norm 16202.2822 (16866.5723)	mem 4918MB
[2022-02-06 22:34:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6970/40036]	eta 1:20:25 lr 0.000010	time 0.1450 (0.1459)	loss 0.2187 (0.2182)	grad_norm 14823.2939 (16864.1406)	mem 4918MB
[2022-02-06 22:34:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6980/40036]	eta 1:20:24 lr 0.000010	time 0.1453 (0.1459)	loss 0.2164 (0.2182)	grad_norm 17970.7188 (16861.8730)	mem 4918MB
[2022-02-06 22:34:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][6990/40036]	eta 1:20:23 lr 0.000010	time 0.1455 (0.1459)	loss 0.2172 (0.2182)	grad_norm 15676.3037 (16861.1895)	mem 4918MB
[2022-02-06 22:34:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7000/40036]	eta 1:20:21 lr 0.000010	time 0.1450 (0.1459)	loss 0.2186 (0.2181)	grad_norm 18187.5254 (16860.9277)	mem 4918MB
[2022-02-06 22:34:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7010/40036]	eta 1:20:20 lr 0.000010	time 0.1451 (0.1460)	loss 0.2169 (0.2181)	grad_norm 12931.0488 (16859.9922)	mem 4918MB
[2022-02-06 22:34:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7020/40036]	eta 1:20:18 lr 0.000010	time 0.1469 (0.1460)	loss 0.2206 (0.2181)	grad_norm 16408.0918 (16857.6582)	mem 4918MB
[2022-02-06 22:34:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7030/40036]	eta 1:20:17 lr 0.000010	time 0.1452 (0.1459)	loss 0.2192 (0.2182)	grad_norm 17147.7207 (16856.1777)	mem 4918MB
[2022-02-06 22:34:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7040/40036]	eta 1:20:15 lr 0.000010	time 0.1450 (0.1460)	loss 0.2172 (0.2181)	grad_norm 14915.6504 (16854.8203)	mem 4918MB
[2022-02-06 22:34:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7050/40036]	eta 1:20:14 lr 0.000010	time 0.1446 (0.1460)	loss 0.2192 (0.2181)	grad_norm 18324.6641 (16852.6504)	mem 4918MB
[2022-02-06 22:34:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7060/40036]	eta 1:20:12 lr 0.000010	time 0.1450 (0.1460)	loss 0.2191 (0.2181)	grad_norm 14158.1455 (16850.1074)	mem 4918MB
[2022-02-06 22:34:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7070/40036]	eta 1:20:11 lr 0.000010	time 0.1446 (0.1460)	loss 0.2180 (0.2181)	grad_norm 14123.4951 (16847.9277)	mem 4918MB
[2022-02-06 22:34:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7080/40036]	eta 1:20:10 lr 0.000010	time 0.1456 (0.1460)	loss 0.2184 (0.2181)	grad_norm 15280.8672 (16847.4043)	mem 4918MB
[2022-02-06 22:34:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7090/40036]	eta 1:20:08 lr 0.000010	time 0.1450 (0.1460)	loss 0.2193 (0.2181)	grad_norm 15973.1475 (16845.1543)	mem 4918MB
[2022-02-06 22:34:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7100/40036]	eta 1:20:07 lr 0.000010	time 0.1447 (0.1460)	loss 0.2187 (0.2181)	grad_norm 14909.7686 (16842.9883)	mem 4918MB
[2022-02-06 22:34:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7110/40036]	eta 1:20:05 lr 0.000010	time 0.1447 (0.1460)	loss 0.2157 (0.2181)	grad_norm 15932.2324 (16841.5449)	mem 4918MB
[2022-02-06 22:34:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7120/40036]	eta 1:20:04 lr 0.000010	time 0.1448 (0.1460)	loss 0.2173 (0.2181)	grad_norm 15189.6582 (16839.2949)	mem 4918MB
[2022-02-06 22:34:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7130/40036]	eta 1:20:02 lr 0.000010	time 0.1453 (0.1460)	loss 0.2157 (0.2181)	grad_norm 15046.4053 (16837.3164)	mem 4918MB
[2022-02-06 22:35:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7140/40036]	eta 1:20:01 lr 0.000010	time 0.1451 (0.1460)	loss 0.2158 (0.2181)	grad_norm 16743.0137 (16835.7422)	mem 4918MB
[2022-02-06 22:35:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7150/40036]	eta 1:19:59 lr 0.000010	time 0.1444 (0.1460)	loss 0.2190 (0.2181)	grad_norm 16324.0693 (16833.5684)	mem 4918MB
[2022-02-06 22:35:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7160/40036]	eta 1:19:58 lr 0.000010	time 0.1450 (0.1460)	loss 0.2174 (0.2181)	grad_norm 13925.4512 (16830.6738)	mem 4918MB
[2022-02-06 22:35:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7170/40036]	eta 1:19:57 lr 0.000010	time 0.1453 (0.1460)	loss 0.2179 (0.2181)	grad_norm 15727.6348 (16828.1523)	mem 4918MB
[2022-02-06 22:35:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7180/40036]	eta 1:19:55 lr 0.000010	time 0.1451 (0.1460)	loss 0.2179 (0.2181)	grad_norm 13900.7529 (16827.1973)	mem 4918MB
[2022-02-06 22:35:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7190/40036]	eta 1:19:54 lr 0.000010	time 0.1452 (0.1460)	loss 0.2152 (0.2181)	grad_norm 16704.1660 (16826.0742)	mem 4918MB
[2022-02-06 22:35:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7200/40036]	eta 1:19:52 lr 0.000010	time 0.1443 (0.1460)	loss 0.2157 (0.2181)	grad_norm 14171.5010 (16823.8945)	mem 4918MB
[2022-02-06 22:35:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7210/40036]	eta 1:19:51 lr 0.000010	time 0.1450 (0.1460)	loss 0.2160 (0.2181)	grad_norm 16078.9482 (16822.2598)	mem 4918MB
[2022-02-06 22:35:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7220/40036]	eta 1:19:49 lr 0.000010	time 0.1448 (0.1460)	loss 0.2170 (0.2181)	grad_norm 14561.5410 (16820.1094)	mem 4918MB
[2022-02-06 22:35:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7230/40036]	eta 1:19:48 lr 0.000010	time 0.1446 (0.1460)	loss 0.2156 (0.2181)	grad_norm 15604.9980 (16818.2930)	mem 4918MB
[2022-02-06 22:35:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7240/40036]	eta 1:19:46 lr 0.000010	time 0.1443 (0.1460)	loss 0.2180 (0.2181)	grad_norm 13608.9980 (16815.5918)	mem 4918MB
[2022-02-06 22:35:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7250/40036]	eta 1:19:45 lr 0.000010	time 0.1447 (0.1460)	loss 0.2186 (0.2181)	grad_norm 16724.1699 (16814.5430)	mem 4918MB
[2022-02-06 22:35:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7260/40036]	eta 1:19:43 lr 0.000010	time 0.1453 (0.1460)	loss 0.2196 (0.2181)	grad_norm 16957.8184 (16813.1445)	mem 4918MB
[2022-02-06 22:35:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7270/40036]	eta 1:19:42 lr 0.000010	time 0.1452 (0.1460)	loss 0.2226 (0.2181)	grad_norm 15594.8652 (16811.2168)	mem 4918MB
[2022-02-06 22:35:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7280/40036]	eta 1:19:41 lr 0.000010	time 0.1450 (0.1460)	loss 0.2175 (0.2181)	grad_norm 16997.1895 (16810.0234)	mem 4918MB
[2022-02-06 22:35:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7290/40036]	eta 1:19:39 lr 0.000010	time 0.1450 (0.1460)	loss 0.2171 (0.2181)	grad_norm 14040.9189 (16807.8184)	mem 4918MB
[2022-02-06 22:35:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7300/40036]	eta 1:19:38 lr 0.000010	time 0.1449 (0.1460)	loss 0.2155 (0.2181)	grad_norm 16031.6992 (16806.3848)	mem 4918MB
[2022-02-06 22:35:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7310/40036]	eta 1:19:36 lr 0.000010	time 0.1449 (0.1460)	loss 0.2169 (0.2181)	grad_norm 15169.4648 (16804.2266)	mem 4918MB
[2022-02-06 22:35:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7320/40036]	eta 1:19:35 lr 0.000010	time 0.1457 (0.1460)	loss 0.2186 (0.2181)	grad_norm 16825.8027 (16803.2695)	mem 4918MB
[2022-02-06 22:35:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7330/40036]	eta 1:19:33 lr 0.000010	time 0.1456 (0.1460)	loss 0.2196 (0.2181)	grad_norm 14409.1279 (16802.1777)	mem 4918MB
[2022-02-06 22:35:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7340/40036]	eta 1:19:32 lr 0.000010	time 0.1454 (0.1460)	loss 0.2189 (0.2181)	grad_norm 14964.3926 (16799.7070)	mem 4918MB
[2022-02-06 22:35:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7350/40036]	eta 1:19:30 lr 0.000010	time 0.1449 (0.1460)	loss 0.2204 (0.2181)	grad_norm 13758.3887 (16798.0234)	mem 4918MB
[2022-02-06 22:35:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7360/40036]	eta 1:19:29 lr 0.000010	time 0.1447 (0.1460)	loss 0.2187 (0.2181)	grad_norm 15780.1211 (16796.8926)	mem 4918MB
[2022-02-06 22:35:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7370/40036]	eta 1:19:28 lr 0.000010	time 0.1453 (0.1460)	loss 0.2186 (0.2181)	grad_norm 12962.1270 (16794.5723)	mem 4918MB
[2022-02-06 22:35:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7380/40036]	eta 1:19:26 lr 0.000010	time 0.1453 (0.1460)	loss 0.2197 (0.2181)	grad_norm 17418.7617 (16793.5039)	mem 4918MB
[2022-02-06 22:35:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7390/40036]	eta 1:19:25 lr 0.000010	time 0.1448 (0.1460)	loss 0.2144 (0.2181)	grad_norm 14671.0225 (16792.3594)	mem 4918MB
[2022-02-06 22:35:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7400/40036]	eta 1:19:23 lr 0.000010	time 0.1448 (0.1460)	loss 0.2157 (0.2181)	grad_norm 12756.5703 (16790.9824)	mem 4918MB
[2022-02-06 22:35:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7410/40036]	eta 1:19:22 lr 0.000010	time 0.1450 (0.1460)	loss 0.2184 (0.2181)	grad_norm 14825.5293 (16788.8438)	mem 4918MB
[2022-02-06 22:35:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7420/40036]	eta 1:19:20 lr 0.000010	time 0.1452 (0.1460)	loss 0.2156 (0.2181)	grad_norm 14591.9141 (16786.5820)	mem 4918MB
[2022-02-06 22:35:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7430/40036]	eta 1:19:19 lr 0.000010	time 0.1450 (0.1460)	loss 0.2165 (0.2181)	grad_norm 13771.9492 (16784.8652)	mem 4918MB
[2022-02-06 22:35:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7440/40036]	eta 1:19:17 lr 0.000010	time 0.1449 (0.1460)	loss 0.2172 (0.2181)	grad_norm 15308.0039 (16783.6426)	mem 4918MB
[2022-02-06 22:35:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7450/40036]	eta 1:19:16 lr 0.000010	time 0.1456 (0.1460)	loss 0.2174 (0.2181)	grad_norm 17434.9023 (16783.0566)	mem 4918MB
[2022-02-06 22:35:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7460/40036]	eta 1:19:15 lr 0.000010	time 0.1454 (0.1460)	loss 0.2178 (0.2181)	grad_norm 14973.8018 (16781.3555)	mem 4918MB
[2022-02-06 22:35:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7470/40036]	eta 1:19:13 lr 0.000010	time 0.1453 (0.1460)	loss 0.2235 (0.2181)	grad_norm 17522.1211 (16779.9961)	mem 4918MB
[2022-02-06 22:35:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7480/40036]	eta 1:19:12 lr 0.000010	time 0.1444 (0.1460)	loss 0.2179 (0.2181)	grad_norm 15731.6875 (16777.6973)	mem 4918MB
[2022-02-06 22:35:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7490/40036]	eta 1:19:10 lr 0.000010	time 0.1449 (0.1460)	loss 0.2190 (0.2181)	grad_norm 14304.8311 (16776.2949)	mem 4918MB
[2022-02-06 22:35:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7500/40036]	eta 1:19:09 lr 0.000010	time 0.1449 (0.1460)	loss 0.2192 (0.2181)	grad_norm 15311.8145 (16774.2168)	mem 4918MB
[2022-02-06 22:35:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7510/40036]	eta 1:19:07 lr 0.000010	time 0.1445 (0.1460)	loss 0.2170 (0.2181)	grad_norm 13210.0605 (16772.1953)	mem 4918MB
[2022-02-06 22:35:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7520/40036]	eta 1:19:06 lr 0.000010	time 0.1439 (0.1460)	loss 0.2186 (0.2181)	grad_norm 15385.3955 (16769.3711)	mem 4918MB
[2022-02-06 22:35:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7530/40036]	eta 1:19:04 lr 0.000010	time 0.1455 (0.1460)	loss 0.2176 (0.2181)	grad_norm 14368.1260 (16767.4453)	mem 4918MB
[2022-02-06 22:35:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7540/40036]	eta 1:19:03 lr 0.000010	time 0.1451 (0.1460)	loss 0.2163 (0.2181)	grad_norm 16227.2324 (16765.7168)	mem 4918MB
[2022-02-06 22:36:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7550/40036]	eta 1:19:01 lr 0.000010	time 0.1446 (0.1460)	loss 0.2170 (0.2181)	grad_norm 13987.7988 (16763.7559)	mem 4918MB
[2022-02-06 22:36:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7560/40036]	eta 1:19:00 lr 0.000010	time 0.1454 (0.1460)	loss 0.2168 (0.2181)	grad_norm 15001.6416 (16762.0742)	mem 4918MB
[2022-02-06 22:36:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7570/40036]	eta 1:18:59 lr 0.000010	time 0.1448 (0.1460)	loss 0.2166 (0.2181)	grad_norm 16858.1387 (16760.4590)	mem 4918MB
[2022-02-06 22:36:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7580/40036]	eta 1:18:57 lr 0.000010	time 0.1457 (0.1460)	loss 0.2209 (0.2181)	grad_norm 16198.4365 (16758.0098)	mem 4918MB
[2022-02-06 22:36:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7590/40036]	eta 1:18:56 lr 0.000010	time 0.1452 (0.1460)	loss 0.2203 (0.2181)	grad_norm 14088.1211 (16756.2031)	mem 4918MB
[2022-02-06 22:36:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7600/40036]	eta 1:18:54 lr 0.000010	time 0.1448 (0.1460)	loss 0.2186 (0.2181)	grad_norm 13162.7637 (16755.2734)	mem 4918MB
[2022-02-06 22:36:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7610/40036]	eta 1:18:53 lr 0.000010	time 0.1454 (0.1460)	loss 0.2168 (0.2181)	grad_norm 13432.5557 (16753.6270)	mem 4918MB
[2022-02-06 22:36:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7620/40036]	eta 1:18:51 lr 0.000011	time 0.1448 (0.1460)	loss 0.2194 (0.2181)	grad_norm 13769.0469 (16753.0332)	mem 4918MB
[2022-02-06 22:36:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7630/40036]	eta 1:18:50 lr 0.000011	time 0.1453 (0.1460)	loss 0.2154 (0.2181)	grad_norm 15608.9756 (16750.8574)	mem 4918MB
[2022-02-06 22:36:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7640/40036]	eta 1:18:48 lr 0.000011	time 0.1450 (0.1460)	loss 0.2183 (0.2181)	grad_norm 14998.3779 (16749.2832)	mem 4918MB
[2022-02-06 22:36:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7650/40036]	eta 1:18:47 lr 0.000011	time 0.1449 (0.1460)	loss 0.2172 (0.2181)	grad_norm 16750.8711 (16746.9961)	mem 4918MB
[2022-02-06 22:36:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7660/40036]	eta 1:18:46 lr 0.000011	time 0.1450 (0.1460)	loss 0.2207 (0.2181)	grad_norm 15510.7373 (16744.9531)	mem 4918MB
[2022-02-06 22:36:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7670/40036]	eta 1:18:44 lr 0.000011	time 0.1454 (0.1460)	loss 0.2183 (0.2181)	grad_norm 17587.0371 (16743.1309)	mem 4918MB
[2022-02-06 22:36:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7680/40036]	eta 1:18:43 lr 0.000011	time 0.1447 (0.1460)	loss 0.2180 (0.2181)	grad_norm 15043.1445 (16740.7422)	mem 4918MB
[2022-02-06 22:36:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7690/40036]	eta 1:18:41 lr 0.000011	time 0.1455 (0.1460)	loss 0.2168 (0.2181)	grad_norm 14132.7295 (16738.6250)	mem 4918MB
[2022-02-06 22:36:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7700/40036]	eta 1:18:40 lr 0.000011	time 0.1447 (0.1460)	loss 0.2166 (0.2181)	grad_norm 13718.9785 (16736.1152)	mem 4918MB
[2022-02-06 22:36:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7710/40036]	eta 1:18:38 lr 0.000011	time 0.1454 (0.1460)	loss 0.2206 (0.2181)	grad_norm 15068.0918 (16733.4688)	mem 4918MB
[2022-02-06 22:36:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7720/40036]	eta 1:18:37 lr 0.000011	time 0.1451 (0.1460)	loss 0.2191 (0.2181)	grad_norm 17963.7617 (16733.6543)	mem 4918MB
[2022-02-06 22:36:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7730/40036]	eta 1:18:35 lr 0.000011	time 0.1450 (0.1460)	loss 0.2178 (0.2181)	grad_norm 14663.0254 (16731.9590)	mem 4918MB
[2022-02-06 22:36:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7740/40036]	eta 1:18:34 lr 0.000011	time 0.1447 (0.1460)	loss 0.2156 (0.2181)	grad_norm 15422.0332 (16730.6777)	mem 4918MB
[2022-02-06 22:36:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7750/40036]	eta 1:18:33 lr 0.000011	time 0.1447 (0.1460)	loss 0.2171 (0.2181)	grad_norm 16535.2715 (16729.1777)	mem 4918MB
[2022-02-06 22:36:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7760/40036]	eta 1:18:31 lr 0.000011	time 0.1447 (0.1460)	loss 0.2184 (0.2181)	grad_norm 15869.6914 (16726.8672)	mem 4918MB
[2022-02-06 22:36:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7770/40036]	eta 1:18:30 lr 0.000011	time 0.1450 (0.1460)	loss 0.2182 (0.2181)	grad_norm 15341.9844 (16725.0156)	mem 4918MB
[2022-02-06 22:36:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7780/40036]	eta 1:18:28 lr 0.000011	time 0.1447 (0.1460)	loss 0.2179 (0.2181)	grad_norm 14315.3203 (16723.8535)	mem 4918MB
[2022-02-06 22:36:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7790/40036]	eta 1:18:27 lr 0.000011	time 0.1451 (0.1460)	loss 0.2190 (0.2181)	grad_norm 16454.1523 (16722.2871)	mem 4918MB
[2022-02-06 22:36:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7800/40036]	eta 1:18:25 lr 0.000011	time 0.1447 (0.1460)	loss 0.2159 (0.2181)	grad_norm 16178.9043 (16721.0645)	mem 4918MB
[2022-02-06 22:36:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7810/40036]	eta 1:18:24 lr 0.000011	time 0.1452 (0.1460)	loss 0.2191 (0.2181)	grad_norm 14076.6260 (16719.1758)	mem 4918MB
[2022-02-06 22:36:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7820/40036]	eta 1:18:22 lr 0.000011	time 0.1449 (0.1460)	loss 0.2170 (0.2181)	grad_norm 11339.0586 (16716.8555)	mem 4918MB
[2022-02-06 22:36:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7830/40036]	eta 1:18:21 lr 0.000011	time 0.1444 (0.1460)	loss 0.2203 (0.2181)	grad_norm 16543.7012 (16715.3516)	mem 4918MB
[2022-02-06 22:36:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7840/40036]	eta 1:18:20 lr 0.000011	time 0.1449 (0.1460)	loss 0.2166 (0.2181)	grad_norm 15477.1924 (16714.6367)	mem 4918MB
[2022-02-06 22:36:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7850/40036]	eta 1:18:18 lr 0.000011	time 0.1452 (0.1460)	loss 0.2158 (0.2181)	grad_norm 16069.8594 (16712.1406)	mem 4918MB
[2022-02-06 22:36:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7860/40036]	eta 1:18:17 lr 0.000011	time 0.1451 (0.1460)	loss 0.2177 (0.2181)	grad_norm 17084.9609 (16710.6914)	mem 4918MB
[2022-02-06 22:36:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7870/40036]	eta 1:18:15 lr 0.000011	time 0.1447 (0.1460)	loss 0.2159 (0.2181)	grad_norm 13861.8496 (16708.6016)	mem 4918MB
[2022-02-06 22:36:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7880/40036]	eta 1:18:14 lr 0.000011	time 0.1453 (0.1460)	loss 0.2139 (0.2181)	grad_norm 15044.8389 (16707.0098)	mem 4918MB
[2022-02-06 22:36:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7890/40036]	eta 1:18:12 lr 0.000011	time 0.1449 (0.1460)	loss 0.2213 (0.2181)	grad_norm 13096.6318 (16704.5430)	mem 4918MB
[2022-02-06 22:36:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7900/40036]	eta 1:18:11 lr 0.000011	time 0.1453 (0.1460)	loss 0.2172 (0.2181)	grad_norm 13385.5713 (16702.7617)	mem 4918MB
[2022-02-06 22:36:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7910/40036]	eta 1:18:09 lr 0.000011	time 0.1455 (0.1460)	loss 0.2173 (0.2181)	grad_norm 16310.1016 (16700.5234)	mem 4918MB
[2022-02-06 22:36:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7920/40036]	eta 1:18:08 lr 0.000011	time 0.1448 (0.1460)	loss 0.2172 (0.2181)	grad_norm 15878.1475 (16698.5898)	mem 4918MB
[2022-02-06 22:36:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7930/40036]	eta 1:18:06 lr 0.000011	time 0.1447 (0.1460)	loss 0.2182 (0.2181)	grad_norm 15183.8037 (16697.2051)	mem 4918MB
[2022-02-06 22:36:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7940/40036]	eta 1:18:05 lr 0.000011	time 0.1446 (0.1460)	loss 0.2192 (0.2181)	grad_norm 14852.5322 (16695.2539)	mem 4918MB
[2022-02-06 22:36:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7950/40036]	eta 1:18:04 lr 0.000011	time 0.1450 (0.1460)	loss 0.2153 (0.2181)	grad_norm 14646.1094 (16692.8945)	mem 4918MB
[2022-02-06 22:37:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7960/40036]	eta 1:18:02 lr 0.000011	time 0.1451 (0.1460)	loss 0.2188 (0.2181)	grad_norm 13430.2920 (16691.0801)	mem 4918MB
[2022-02-06 22:37:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7970/40036]	eta 1:18:01 lr 0.000011	time 0.1456 (0.1460)	loss 0.2159 (0.2181)	grad_norm 17329.4570 (16689.4473)	mem 4918MB
[2022-02-06 22:37:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7980/40036]	eta 1:17:59 lr 0.000011	time 0.1444 (0.1460)	loss 0.2184 (0.2181)	grad_norm 14613.1113 (16687.5176)	mem 4918MB
[2022-02-06 22:37:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][7990/40036]	eta 1:17:58 lr 0.000011	time 0.1451 (0.1460)	loss 0.2161 (0.2181)	grad_norm 17795.7383 (16685.1758)	mem 4918MB
[2022-02-06 22:37:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8000/40036]	eta 1:17:56 lr 0.000011	time 0.1444 (0.1460)	loss 0.2223 (0.2181)	grad_norm 14972.9473 (16683.7910)	mem 4918MB
[2022-02-06 22:37:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8010/40036]	eta 1:17:55 lr 0.000011	time 0.1451 (0.1460)	loss 0.2179 (0.2181)	grad_norm 13719.5938 (16682.1445)	mem 4918MB
[2022-02-06 22:37:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8020/40036]	eta 1:17:53 lr 0.000011	time 0.1451 (0.1460)	loss 0.2211 (0.2181)	grad_norm 16051.1846 (16680.7617)	mem 4918MB
[2022-02-06 22:37:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8030/40036]	eta 1:17:52 lr 0.000011	time 0.1446 (0.1460)	loss 0.2140 (0.2181)	grad_norm 16212.0986 (16679.4590)	mem 4918MB
[2022-02-06 22:37:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8040/40036]	eta 1:17:51 lr 0.000011	time 0.1451 (0.1460)	loss 0.2208 (0.2181)	grad_norm 14991.4219 (16677.2285)	mem 4918MB
[2022-02-06 22:37:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8050/40036]	eta 1:17:49 lr 0.000011	time 0.1455 (0.1460)	loss 0.2160 (0.2181)	grad_norm 15472.2021 (16676.4883)	mem 4918MB
[2022-02-06 22:37:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8060/40036]	eta 1:17:48 lr 0.000011	time 0.1454 (0.1460)	loss 0.2190 (0.2181)	grad_norm 15835.8887 (16674.6641)	mem 4918MB
[2022-02-06 22:37:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8070/40036]	eta 1:17:46 lr 0.000011	time 0.1452 (0.1460)	loss 0.2176 (0.2181)	grad_norm 15522.8564 (16671.9492)	mem 4918MB
[2022-02-06 22:37:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8080/40036]	eta 1:17:45 lr 0.000011	time 0.1447 (0.1460)	loss 0.2172 (0.2181)	grad_norm 14759.3760 (16670.3945)	mem 4918MB
[2022-02-06 22:37:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8090/40036]	eta 1:17:43 lr 0.000011	time 0.1455 (0.1460)	loss 0.2181 (0.2181)	grad_norm 16410.0078 (16667.9434)	mem 4918MB
[2022-02-06 22:37:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8100/40036]	eta 1:17:42 lr 0.000011	time 0.1448 (0.1460)	loss 0.2182 (0.2181)	grad_norm 13489.4941 (16666.3125)	mem 4918MB
[2022-02-06 22:37:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8110/40036]	eta 1:17:40 lr 0.000011	time 0.1458 (0.1460)	loss 0.2210 (0.2181)	grad_norm 16261.0010 (16664.8613)	mem 4918MB
[2022-02-06 22:37:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8120/40036]	eta 1:17:39 lr 0.000011	time 0.1452 (0.1460)	loss 0.2168 (0.2181)	grad_norm 13259.6865 (16663.1680)	mem 4918MB
[2022-02-06 22:37:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8130/40036]	eta 1:17:38 lr 0.000011	time 0.1449 (0.1460)	loss 0.2160 (0.2181)	grad_norm 14704.0107 (16660.8145)	mem 4918MB
[2022-02-06 22:37:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8140/40036]	eta 1:17:36 lr 0.000011	time 0.1451 (0.1460)	loss 0.2197 (0.2181)	grad_norm 14450.4404 (16659.2090)	mem 4918MB
[2022-02-06 22:37:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8150/40036]	eta 1:17:35 lr 0.000011	time 0.1446 (0.1460)	loss 0.2158 (0.2181)	grad_norm 14499.8652 (16656.8496)	mem 4918MB
[2022-02-06 22:37:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8160/40036]	eta 1:17:33 lr 0.000011	time 0.1443 (0.1460)	loss 0.2179 (0.2181)	grad_norm 14127.1895 (16655.2285)	mem 4918MB
[2022-02-06 22:37:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8170/40036]	eta 1:17:32 lr 0.000011	time 0.1450 (0.1460)	loss 0.2167 (0.2181)	grad_norm 14689.1230 (16653.2266)	mem 4918MB
[2022-02-06 22:37:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8180/40036]	eta 1:17:30 lr 0.000011	time 0.1457 (0.1460)	loss 0.2169 (0.2181)	grad_norm 13044.4229 (16651.7617)	mem 4918MB
[2022-02-06 22:37:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8190/40036]	eta 1:17:29 lr 0.000011	time 0.1452 (0.1460)	loss 0.2171 (0.2181)	grad_norm 13935.7695 (16650.2480)	mem 4918MB
[2022-02-06 22:37:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8200/40036]	eta 1:17:27 lr 0.000011	time 0.1455 (0.1460)	loss 0.2176 (0.2181)	grad_norm 13587.2910 (16648.4551)	mem 4918MB
[2022-02-06 22:37:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8210/40036]	eta 1:17:26 lr 0.000011	time 0.1448 (0.1460)	loss 0.2183 (0.2181)	grad_norm 15413.2529 (16647.0527)	mem 4918MB
[2022-02-06 22:37:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8220/40036]	eta 1:17:24 lr 0.000011	time 0.1445 (0.1460)	loss 0.2179 (0.2181)	grad_norm 12871.8311 (16645.3770)	mem 4918MB
[2022-02-06 22:37:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8230/40036]	eta 1:17:23 lr 0.000011	time 0.1444 (0.1460)	loss 0.2165 (0.2181)	grad_norm 15614.5371 (16643.6758)	mem 4918MB
[2022-02-06 22:37:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8240/40036]	eta 1:17:21 lr 0.000011	time 0.1453 (0.1460)	loss 0.2181 (0.2181)	grad_norm 14941.5508 (16642.6680)	mem 4918MB
[2022-02-06 22:37:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8250/40036]	eta 1:17:20 lr 0.000011	time 0.1454 (0.1460)	loss 0.2188 (0.2181)	grad_norm 13517.3965 (16640.8477)	mem 4918MB
[2022-02-06 22:37:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8260/40036]	eta 1:17:19 lr 0.000011	time 0.1452 (0.1460)	loss 0.2162 (0.2181)	grad_norm 14581.6299 (16638.5449)	mem 4918MB
[2022-02-06 22:37:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8270/40036]	eta 1:17:17 lr 0.000011	time 0.1444 (0.1460)	loss 0.2177 (0.2181)	grad_norm 13318.0771 (16636.4570)	mem 4918MB
[2022-02-06 22:37:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8280/40036]	eta 1:17:16 lr 0.000011	time 0.1451 (0.1460)	loss 0.2161 (0.2181)	grad_norm 16368.5732 (16635.1035)	mem 4918MB
[2022-02-06 22:37:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8290/40036]	eta 1:17:14 lr 0.000011	time 0.1447 (0.1460)	loss 0.2173 (0.2181)	grad_norm 13087.5254 (16633.3535)	mem 4918MB
[2022-02-06 22:37:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8300/40036]	eta 1:17:13 lr 0.000011	time 0.1456 (0.1460)	loss 0.2153 (0.2181)	grad_norm 15496.7480 (16631.6660)	mem 4918MB
[2022-02-06 22:37:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8310/40036]	eta 1:17:11 lr 0.000011	time 0.1454 (0.1460)	loss 0.2203 (0.2181)	grad_norm 13797.1475 (16629.5273)	mem 4918MB
[2022-02-06 22:37:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8320/40036]	eta 1:17:10 lr 0.000011	time 0.1447 (0.1460)	loss 0.2174 (0.2181)	grad_norm 14418.7402 (16628.0000)	mem 4918MB
[2022-02-06 22:37:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8330/40036]	eta 1:17:08 lr 0.000011	time 0.1449 (0.1460)	loss 0.2163 (0.2181)	grad_norm 15651.1953 (16626.1816)	mem 4918MB
[2022-02-06 22:37:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8340/40036]	eta 1:17:07 lr 0.000011	time 0.1453 (0.1460)	loss 0.2163 (0.2181)	grad_norm 16104.4619 (16624.8203)	mem 4918MB
[2022-02-06 22:37:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8350/40036]	eta 1:17:06 lr 0.000011	time 0.1455 (0.1460)	loss 0.2152 (0.2181)	grad_norm 13631.2549 (16622.7422)	mem 4918MB
[2022-02-06 22:37:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8360/40036]	eta 1:17:04 lr 0.000011	time 0.1452 (0.1460)	loss 0.2166 (0.2181)	grad_norm 15471.1436 (16620.8418)	mem 4918MB
[2022-02-06 22:38:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8370/40036]	eta 1:17:03 lr 0.000011	time 0.1447 (0.1460)	loss 0.2176 (0.2181)	grad_norm 16110.0498 (16619.7812)	mem 4918MB
[2022-02-06 22:38:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8380/40036]	eta 1:17:01 lr 0.000011	time 0.1445 (0.1460)	loss 0.2190 (0.2181)	grad_norm 15794.1572 (16617.9766)	mem 4918MB
[2022-02-06 22:38:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8390/40036]	eta 1:17:00 lr 0.000011	time 0.1451 (0.1460)	loss 0.2188 (0.2181)	grad_norm 16004.6895 (16616.1699)	mem 4918MB
[2022-02-06 22:38:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8400/40036]	eta 1:16:58 lr 0.000011	time 0.1454 (0.1460)	loss 0.2162 (0.2181)	grad_norm 17328.4902 (16615.0996)	mem 4918MB
[2022-02-06 22:38:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8410/40036]	eta 1:16:57 lr 0.000011	time 0.1451 (0.1460)	loss 0.2178 (0.2181)	grad_norm 16448.4883 (16613.1113)	mem 4918MB
[2022-02-06 22:38:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8420/40036]	eta 1:16:55 lr 0.000011	time 0.1449 (0.1460)	loss 0.2214 (0.2181)	grad_norm 14275.0186 (16612.0000)	mem 4918MB
[2022-02-06 22:38:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8430/40036]	eta 1:16:54 lr 0.000011	time 0.1450 (0.1460)	loss 0.2193 (0.2181)	grad_norm 14498.4619 (16608.9824)	mem 4918MB
[2022-02-06 22:38:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8440/40036]	eta 1:16:53 lr 0.000011	time 0.1451 (0.1460)	loss 0.2166 (0.2181)	grad_norm 13140.3115 (16607.1367)	mem 4918MB
[2022-02-06 22:38:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8450/40036]	eta 1:16:51 lr 0.000012	time 0.1452 (0.1460)	loss 0.2187 (0.2181)	grad_norm 16305.6602 (16605.3672)	mem 4918MB
[2022-02-06 22:38:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8460/40036]	eta 1:16:50 lr 0.000012	time 0.1456 (0.1460)	loss 0.2184 (0.2181)	grad_norm 14880.9014 (16602.9746)	mem 4918MB
[2022-02-06 22:38:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8470/40036]	eta 1:16:48 lr 0.000012	time 0.1450 (0.1460)	loss 0.2182 (0.2181)	grad_norm 16519.1836 (16600.9258)	mem 4918MB
[2022-02-06 22:38:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8480/40036]	eta 1:16:47 lr 0.000012	time 0.1441 (0.1460)	loss 0.2167 (0.2181)	grad_norm 13947.7178 (16598.5352)	mem 4918MB
[2022-02-06 22:38:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8490/40036]	eta 1:16:45 lr 0.000012	time 0.1454 (0.1460)	loss 0.2166 (0.2181)	grad_norm 15781.7764 (16596.7129)	mem 4918MB
[2022-02-06 22:38:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8500/40036]	eta 1:16:44 lr 0.000012	time 0.1451 (0.1460)	loss 0.2180 (0.2181)	grad_norm 12929.1963 (16594.1875)	mem 4918MB
[2022-02-06 22:38:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8510/40036]	eta 1:16:42 lr 0.000012	time 0.1456 (0.1460)	loss 0.2158 (0.2181)	grad_norm 15468.3545 (16591.9883)	mem 4918MB
[2022-02-06 22:38:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8520/40036]	eta 1:16:41 lr 0.000012	time 0.1454 (0.1460)	loss 0.2194 (0.2181)	grad_norm 13322.1484 (16590.4688)	mem 4918MB
[2022-02-06 22:38:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8530/40036]	eta 1:16:40 lr 0.000012	time 0.1446 (0.1460)	loss 0.2182 (0.2181)	grad_norm 14703.0420 (16587.8184)	mem 4918MB
[2022-02-06 22:38:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8540/40036]	eta 1:16:38 lr 0.000012	time 0.1454 (0.1460)	loss 0.2192 (0.2181)	grad_norm 14184.9609 (16585.5371)	mem 4918MB
[2022-02-06 22:38:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8550/40036]	eta 1:16:37 lr 0.000012	time 0.1456 (0.1460)	loss 0.2188 (0.2181)	grad_norm 13394.8340 (16583.2793)	mem 4918MB
[2022-02-06 22:38:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8560/40036]	eta 1:16:35 lr 0.000012	time 0.1453 (0.1460)	loss 0.2192 (0.2181)	grad_norm 14352.1201 (16581.8633)	mem 4918MB
[2022-02-06 22:38:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8570/40036]	eta 1:16:34 lr 0.000012	time 0.1450 (0.1460)	loss 0.2172 (0.2181)	grad_norm 12952.7568 (16579.5625)	mem 4918MB
[2022-02-06 22:38:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8580/40036]	eta 1:16:32 lr 0.000012	time 0.1441 (0.1460)	loss 0.2182 (0.2181)	grad_norm 15867.2207 (16577.4980)	mem 4918MB
[2022-02-06 22:38:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8590/40036]	eta 1:16:31 lr 0.000012	time 0.1453 (0.1460)	loss 0.2192 (0.2181)	grad_norm 14361.0742 (16575.8535)	mem 4918MB
[2022-02-06 22:38:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8600/40036]	eta 1:16:29 lr 0.000012	time 0.1449 (0.1460)	loss 0.2165 (0.2181)	grad_norm 14760.8936 (16573.6699)	mem 4918MB
[2022-02-06 22:38:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8610/40036]	eta 1:16:28 lr 0.000012	time 0.1462 (0.1460)	loss 0.2171 (0.2181)	grad_norm 16098.0820 (16572.1875)	mem 4918MB
[2022-02-06 22:38:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8620/40036]	eta 1:16:26 lr 0.000012	time 0.1447 (0.1460)	loss 0.2191 (0.2181)	grad_norm 13738.8330 (16569.7793)	mem 4918MB
[2022-02-06 22:38:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8630/40036]	eta 1:16:25 lr 0.000012	time 0.1448 (0.1460)	loss 0.2162 (0.2181)	grad_norm 13994.7158 (16567.6934)	mem 4918MB
[2022-02-06 22:38:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8640/40036]	eta 1:16:24 lr 0.000012	time 0.1443 (0.1460)	loss 0.2174 (0.2181)	grad_norm 13441.9189 (16565.9297)	mem 4918MB
[2022-02-06 22:38:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8650/40036]	eta 1:16:22 lr 0.000012	time 0.1458 (0.1460)	loss 0.2201 (0.2181)	grad_norm 15900.1738 (16564.2344)	mem 4918MB
[2022-02-06 22:38:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8660/40036]	eta 1:16:21 lr 0.000012	time 0.1446 (0.1460)	loss 0.2160 (0.2181)	grad_norm 15827.5166 (16562.8340)	mem 4918MB
[2022-02-06 22:38:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8670/40036]	eta 1:16:19 lr 0.000012	time 0.1447 (0.1460)	loss 0.2174 (0.2181)	grad_norm 16888.8203 (16561.2637)	mem 4918MB
[2022-02-06 22:38:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8680/40036]	eta 1:16:18 lr 0.000012	time 0.1448 (0.1460)	loss 0.2176 (0.2181)	grad_norm 15251.9971 (16559.1855)	mem 4918MB
[2022-02-06 22:38:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8690/40036]	eta 1:16:16 lr 0.000012	time 0.1456 (0.1460)	loss 0.2177 (0.2181)	grad_norm 14887.4990 (16557.6543)	mem 4918MB
[2022-02-06 22:38:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8700/40036]	eta 1:16:15 lr 0.000012	time 0.1455 (0.1460)	loss 0.2182 (0.2181)	grad_norm 15846.3682 (16555.9629)	mem 4918MB
[2022-02-06 22:38:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8710/40036]	eta 1:16:13 lr 0.000012	time 0.1448 (0.1460)	loss 0.2191 (0.2181)	grad_norm 14471.8838 (16553.3066)	mem 4918MB
[2022-02-06 22:38:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8720/40036]	eta 1:16:12 lr 0.000012	time 0.1449 (0.1460)	loss 0.2187 (0.2181)	grad_norm 13159.8789 (16550.8066)	mem 4918MB
[2022-02-06 22:38:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8730/40036]	eta 1:16:11 lr 0.000012	time 0.1451 (0.1460)	loss 0.2191 (0.2181)	grad_norm 15119.1045 (16548.9824)	mem 4918MB
[2022-02-06 22:38:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8740/40036]	eta 1:16:09 lr 0.000012	time 0.1453 (0.1460)	loss 0.2195 (0.2181)	grad_norm 15866.0557 (16546.9043)	mem 4918MB
[2022-02-06 22:38:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8750/40036]	eta 1:16:08 lr 0.000012	time 0.1451 (0.1460)	loss 0.2173 (0.2181)	grad_norm 14309.8447 (16544.4941)	mem 4918MB
[2022-02-06 22:38:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8760/40036]	eta 1:16:06 lr 0.000012	time 0.1450 (0.1460)	loss 0.2190 (0.2181)	grad_norm 13610.9512 (16541.6934)	mem 4918MB
[2022-02-06 22:38:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8770/40036]	eta 1:16:05 lr 0.000012	time 0.1447 (0.1460)	loss 0.2203 (0.2181)	grad_norm 14867.0186 (16540.1914)	mem 4918MB
[2022-02-06 22:39:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8780/40036]	eta 1:16:03 lr 0.000012	time 0.1450 (0.1460)	loss 0.2173 (0.2181)	grad_norm 12992.6045 (16537.5215)	mem 4918MB
[2022-02-06 22:39:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8790/40036]	eta 1:16:02 lr 0.000012	time 0.1450 (0.1460)	loss 0.2162 (0.2181)	grad_norm 13229.2793 (16535.0703)	mem 4918MB
[2022-02-06 22:39:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8800/40036]	eta 1:16:00 lr 0.000012	time 0.1448 (0.1460)	loss 0.2170 (0.2181)	grad_norm 14711.1807 (16533.1465)	mem 4918MB
[2022-02-06 22:39:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8810/40036]	eta 1:15:59 lr 0.000012	time 0.1446 (0.1460)	loss 0.2200 (0.2181)	grad_norm 14030.9883 (16531.7832)	mem 4918MB
[2022-02-06 22:39:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8820/40036]	eta 1:15:58 lr 0.000012	time 0.1448 (0.1460)	loss 0.2181 (0.2181)	grad_norm 15182.8535 (16529.6875)	mem 4918MB
[2022-02-06 22:39:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8830/40036]	eta 1:15:56 lr 0.000012	time 0.1452 (0.1460)	loss 0.2166 (0.2181)	grad_norm 14982.4863 (16527.1641)	mem 4918MB
[2022-02-06 22:39:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8840/40036]	eta 1:15:55 lr 0.000012	time 0.1455 (0.1460)	loss 0.2180 (0.2181)	grad_norm 14313.3213 (16524.1133)	mem 4918MB
[2022-02-06 22:39:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8850/40036]	eta 1:15:53 lr 0.000012	time 0.1451 (0.1460)	loss 0.2171 (0.2181)	grad_norm 14675.1553 (16522.0137)	mem 4918MB
[2022-02-06 22:39:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8860/40036]	eta 1:15:52 lr 0.000012	time 0.1453 (0.1460)	loss 0.2153 (0.2181)	grad_norm 13166.7607 (16519.6445)	mem 4918MB
[2022-02-06 22:39:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8870/40036]	eta 1:15:50 lr 0.000012	time 0.1451 (0.1460)	loss 0.2178 (0.2181)	grad_norm 14193.2490 (16518.2207)	mem 4918MB
[2022-02-06 22:39:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8880/40036]	eta 1:15:49 lr 0.000012	time 0.1446 (0.1460)	loss 0.2177 (0.2181)	grad_norm 17348.6660 (16516.1191)	mem 4918MB
[2022-02-06 22:39:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8890/40036]	eta 1:15:47 lr 0.000012	time 0.1446 (0.1460)	loss 0.2171 (0.2181)	grad_norm 16756.7070 (16513.9062)	mem 4918MB
[2022-02-06 22:39:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8900/40036]	eta 1:15:46 lr 0.000012	time 0.1446 (0.1460)	loss 0.2190 (0.2181)	grad_norm 12282.5557 (16511.6055)	mem 4918MB
[2022-02-06 22:39:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8910/40036]	eta 1:15:44 lr 0.000012	time 0.1444 (0.1460)	loss 0.2153 (0.2181)	grad_norm 14050.1787 (16509.9961)	mem 4918MB
[2022-02-06 22:39:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8920/40036]	eta 1:15:43 lr 0.000012	time 0.1455 (0.1460)	loss 0.2176 (0.2181)	grad_norm 14944.8633 (16507.7949)	mem 4918MB
[2022-02-06 22:39:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8930/40036]	eta 1:15:42 lr 0.000012	time 0.1451 (0.1460)	loss 0.2191 (0.2181)	grad_norm 15022.3770 (16505.2422)	mem 4918MB
[2022-02-06 22:39:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8940/40036]	eta 1:15:40 lr 0.000012	time 0.1449 (0.1460)	loss 0.2198 (0.2181)	grad_norm 13920.6709 (16502.9043)	mem 4918MB
[2022-02-06 22:39:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8950/40036]	eta 1:15:39 lr 0.000012	time 0.1453 (0.1460)	loss 0.2159 (0.2181)	grad_norm 14709.7842 (16500.9082)	mem 4918MB
[2022-02-06 22:39:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8960/40036]	eta 1:15:37 lr 0.000012	time 0.1451 (0.1460)	loss 0.2141 (0.2181)	grad_norm 15472.6582 (16498.7910)	mem 4918MB
[2022-02-06 22:39:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8970/40036]	eta 1:15:36 lr 0.000012	time 0.1454 (0.1460)	loss 0.2138 (0.2181)	grad_norm 13667.2939 (16495.7773)	mem 4918MB
[2022-02-06 22:39:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8980/40036]	eta 1:15:34 lr 0.000012	time 0.1450 (0.1460)	loss 0.2185 (0.2181)	grad_norm 15113.7002 (16494.1484)	mem 4918MB
[2022-02-06 22:39:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][8990/40036]	eta 1:15:33 lr 0.000012	time 0.1444 (0.1460)	loss 0.2167 (0.2181)	grad_norm 13858.8271 (16492.5137)	mem 4918MB
[2022-02-06 22:39:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9000/40036]	eta 1:15:31 lr 0.000012	time 0.1451 (0.1460)	loss 0.2184 (0.2181)	grad_norm 17604.5488 (16491.4414)	mem 4918MB
[2022-02-06 22:39:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9010/40036]	eta 1:15:30 lr 0.000012	time 0.1447 (0.1460)	loss 0.2185 (0.2181)	grad_norm 14065.8789 (16489.8340)	mem 4918MB
[2022-02-06 22:39:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9020/40036]	eta 1:15:28 lr 0.000012	time 0.1454 (0.1460)	loss 0.2180 (0.2181)	grad_norm 14463.7139 (16487.6426)	mem 4918MB
[2022-02-06 22:39:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9030/40036]	eta 1:15:27 lr 0.000012	time 0.1450 (0.1460)	loss 0.2188 (0.2181)	grad_norm 12409.3613 (16486.0137)	mem 4918MB
[2022-02-06 22:39:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9040/40036]	eta 1:15:26 lr 0.000012	time 0.1458 (0.1460)	loss 0.2180 (0.2181)	grad_norm 14727.4434 (16483.7793)	mem 4918MB
[2022-02-06 22:39:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9050/40036]	eta 1:15:24 lr 0.000012	time 0.1448 (0.1460)	loss 0.2186 (0.2181)	grad_norm 15304.0059 (16482.4434)	mem 4918MB
[2022-02-06 22:39:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9060/40036]	eta 1:15:23 lr 0.000012	time 0.1496 (0.1460)	loss 0.2179 (0.2181)	grad_norm 16515.7754 (16480.5352)	mem 4918MB
[2022-02-06 22:39:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9070/40036]	eta 1:15:22 lr 0.000012	time 0.1479 (0.1460)	loss 0.2161 (0.2181)	grad_norm 15890.9775 (16478.0977)	mem 4918MB
[2022-02-06 22:39:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9080/40036]	eta 1:15:20 lr 0.000012	time 0.1459 (0.1460)	loss 0.2185 (0.2181)	grad_norm 14457.2051 (16475.8984)	mem 4918MB
[2022-02-06 22:39:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9090/40036]	eta 1:15:19 lr 0.000012	time 0.1455 (0.1460)	loss 0.2158 (0.2181)	grad_norm 15520.9424 (16474.0352)	mem 4918MB
[2022-02-06 22:39:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9100/40036]	eta 1:15:17 lr 0.000012	time 0.1462 (0.1460)	loss 0.2164 (0.2181)	grad_norm 15002.4502 (16472.3379)	mem 4918MB
[2022-02-06 22:39:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9110/40036]	eta 1:15:16 lr 0.000012	time 0.1450 (0.1460)	loss 0.2160 (0.2181)	grad_norm 12844.5859 (16469.2168)	mem 4918MB
[2022-02-06 22:39:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9120/40036]	eta 1:15:15 lr 0.000012	time 0.1448 (0.1460)	loss 0.2170 (0.2181)	grad_norm 14055.0576 (16467.3203)	mem 4918MB
[2022-02-06 22:39:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9130/40036]	eta 1:15:13 lr 0.000012	time 0.1454 (0.1460)	loss 0.2210 (0.2181)	grad_norm 16046.0811 (16465.1016)	mem 4918MB
[2022-02-06 22:39:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9140/40036]	eta 1:15:12 lr 0.000012	time 0.1457 (0.1460)	loss 0.2181 (0.2181)	grad_norm 13596.4570 (16463.1367)	mem 4918MB
[2022-02-06 22:39:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9150/40036]	eta 1:15:10 lr 0.000012	time 0.1459 (0.1460)	loss 0.2184 (0.2181)	grad_norm 14879.9473 (16461.1113)	mem 4918MB
[2022-02-06 22:39:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9160/40036]	eta 1:15:09 lr 0.000012	time 0.1458 (0.1460)	loss 0.2172 (0.2181)	grad_norm 15395.7949 (16459.4648)	mem 4918MB
[2022-02-06 22:39:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9170/40036]	eta 1:15:07 lr 0.000012	time 0.1453 (0.1460)	loss 0.2152 (0.2181)	grad_norm 12066.7412 (16456.9297)	mem 4918MB
[2022-02-06 22:39:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9180/40036]	eta 1:15:06 lr 0.000012	time 0.1456 (0.1460)	loss 0.2177 (0.2181)	grad_norm 15553.6221 (16455.1465)	mem 4918MB
[2022-02-06 22:40:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9190/40036]	eta 1:15:05 lr 0.000012	time 0.1509 (0.1461)	loss 0.2214 (0.2181)	grad_norm 15945.6250 (16452.7773)	mem 4918MB
[2022-02-06 22:40:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9200/40036]	eta 1:15:03 lr 0.000012	time 0.1460 (0.1461)	loss 0.2219 (0.2181)	grad_norm 14860.7842 (16450.9238)	mem 4918MB
[2022-02-06 22:40:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9210/40036]	eta 1:15:02 lr 0.000012	time 0.1526 (0.1461)	loss 0.2179 (0.2181)	grad_norm 13539.8936 (16448.7910)	mem 4918MB
[2022-02-06 22:40:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9220/40036]	eta 1:15:01 lr 0.000012	time 0.1576 (0.1461)	loss 0.2190 (0.2181)	grad_norm 14580.6230 (16446.3613)	mem 4918MB
[2022-02-06 22:40:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9230/40036]	eta 1:14:59 lr 0.000012	time 0.1461 (0.1461)	loss 0.2204 (0.2181)	grad_norm 15694.9570 (16443.9863)	mem 4918MB
[2022-02-06 22:40:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9240/40036]	eta 1:14:58 lr 0.000012	time 0.1547 (0.1461)	loss 0.2196 (0.2181)	grad_norm 12961.6465 (16441.6445)	mem 4918MB
[2022-02-06 22:40:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9250/40036]	eta 1:14:57 lr 0.000013	time 0.1459 (0.1461)	loss 0.2183 (0.2181)	grad_norm 11867.5195 (16439.1777)	mem 4918MB
[2022-02-06 22:40:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9260/40036]	eta 1:14:55 lr 0.000013	time 0.1449 (0.1461)	loss 0.2195 (0.2181)	grad_norm 13451.5879 (16436.9727)	mem 4918MB
[2022-02-06 22:40:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9270/40036]	eta 1:14:54 lr 0.000013	time 0.1449 (0.1461)	loss 0.2177 (0.2181)	grad_norm 15659.7207 (16435.2070)	mem 4918MB
[2022-02-06 22:40:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9280/40036]	eta 1:14:52 lr 0.000013	time 0.1446 (0.1461)	loss 0.2187 (0.2181)	grad_norm 13376.2197 (16432.9258)	mem 4918MB
[2022-02-06 22:40:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9290/40036]	eta 1:14:51 lr 0.000013	time 0.1459 (0.1461)	loss 0.2176 (0.2181)	grad_norm 16941.2715 (16431.1797)	mem 4918MB
[2022-02-06 22:40:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9300/40036]	eta 1:14:50 lr 0.000013	time 0.1459 (0.1461)	loss 0.2185 (0.2181)	grad_norm 13443.0986 (16428.8887)	mem 4918MB
[2022-02-06 22:40:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9310/40036]	eta 1:14:48 lr 0.000013	time 0.1447 (0.1461)	loss 0.2160 (0.2181)	grad_norm 12904.1436 (16427.0215)	mem 4918MB
[2022-02-06 22:40:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9320/40036]	eta 1:14:47 lr 0.000013	time 0.1457 (0.1461)	loss 0.2153 (0.2181)	grad_norm 13147.0371 (16425.1270)	mem 4918MB
[2022-02-06 22:40:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9330/40036]	eta 1:14:45 lr 0.000013	time 0.1456 (0.1461)	loss 0.2228 (0.2181)	grad_norm 14900.5801 (16423.3887)	mem 4918MB
[2022-02-06 22:40:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9340/40036]	eta 1:14:44 lr 0.000013	time 0.1451 (0.1461)	loss 0.2138 (0.2181)	grad_norm 15634.9307 (16420.8652)	mem 4918MB
[2022-02-06 22:40:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9350/40036]	eta 1:14:42 lr 0.000013	time 0.1454 (0.1461)	loss 0.2174 (0.2181)	grad_norm 12868.8086 (16418.5684)	mem 4918MB
[2022-02-06 22:40:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9360/40036]	eta 1:14:41 lr 0.000013	time 0.1457 (0.1461)	loss 0.2153 (0.2181)	grad_norm 14110.1514 (16416.4609)	mem 4918MB
[2022-02-06 22:40:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9370/40036]	eta 1:14:40 lr 0.000013	time 0.1461 (0.1461)	loss 0.2161 (0.2181)	grad_norm 12412.3936 (16414.0000)	mem 4918MB
[2022-02-06 22:40:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9380/40036]	eta 1:14:38 lr 0.000013	time 0.1474 (0.1461)	loss 0.2185 (0.2181)	grad_norm 15119.9043 (16411.6270)	mem 4918MB
[2022-02-06 22:40:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9390/40036]	eta 1:14:37 lr 0.000013	time 0.1457 (0.1461)	loss 0.2174 (0.2181)	grad_norm 13578.6787 (16409.5176)	mem 4918MB
[2022-02-06 22:40:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9400/40036]	eta 1:14:35 lr 0.000013	time 0.1456 (0.1461)	loss 0.2173 (0.2181)	grad_norm 12095.6084 (16407.1191)	mem 4918MB
[2022-02-06 22:40:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9410/40036]	eta 1:14:34 lr 0.000013	time 0.1457 (0.1461)	loss 0.2148 (0.2181)	grad_norm 13930.6533 (16405.0762)	mem 4918MB
[2022-02-06 22:40:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9420/40036]	eta 1:14:32 lr 0.000013	time 0.1447 (0.1461)	loss 0.2126 (0.2181)	grad_norm 12802.3838 (16402.8320)	mem 4918MB
[2022-02-06 22:40:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9430/40036]	eta 1:14:31 lr 0.000013	time 0.1463 (0.1461)	loss 0.2170 (0.2181)	grad_norm 14043.6777 (16400.4121)	mem 4918MB
[2022-02-06 22:40:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9440/40036]	eta 1:14:30 lr 0.000013	time 0.1451 (0.1461)	loss 0.2198 (0.2181)	grad_norm 14352.7578 (16397.9316)	mem 4918MB
[2022-02-06 22:40:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9450/40036]	eta 1:14:28 lr 0.000013	time 0.1452 (0.1461)	loss 0.2190 (0.2181)	grad_norm 15016.1689 (16395.5723)	mem 4918MB
[2022-02-06 22:40:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9460/40036]	eta 1:14:27 lr 0.000013	time 0.1458 (0.1461)	loss 0.2163 (0.2181)	grad_norm 15134.8691 (16393.2109)	mem 4918MB
[2022-02-06 22:40:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9470/40036]	eta 1:14:25 lr 0.000013	time 0.1458 (0.1461)	loss 0.2192 (0.2181)	grad_norm 12941.5537 (16390.2773)	mem 4918MB
[2022-02-06 22:40:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9480/40036]	eta 1:14:24 lr 0.000013	time 0.1454 (0.1461)	loss 0.2173 (0.2181)	grad_norm 14232.2930 (16388.4434)	mem 4918MB
[2022-02-06 22:40:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9490/40036]	eta 1:14:22 lr 0.000013	time 0.1461 (0.1461)	loss 0.2179 (0.2181)	grad_norm 14787.4473 (16385.7227)	mem 4918MB
[2022-02-06 22:40:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9500/40036]	eta 1:14:21 lr 0.000013	time 0.1448 (0.1461)	loss 0.2165 (0.2181)	grad_norm 16040.5488 (16384.0449)	mem 4918MB
[2022-02-06 22:40:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9510/40036]	eta 1:14:20 lr 0.000013	time 0.1459 (0.1461)	loss 0.2181 (0.2181)	grad_norm 14321.9717 (16383.1172)	mem 4918MB
[2022-02-06 22:40:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9520/40036]	eta 1:14:18 lr 0.000013	time 0.1455 (0.1461)	loss 0.2204 (0.2181)	grad_norm 15755.0781 (16381.6045)	mem 4918MB
[2022-02-06 22:40:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9530/40036]	eta 1:14:17 lr 0.000013	time 0.1456 (0.1461)	loss 0.2177 (0.2181)	grad_norm 12565.8896 (16378.8682)	mem 4918MB
[2022-02-06 22:40:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9540/40036]	eta 1:14:15 lr 0.000013	time 0.1461 (0.1461)	loss 0.2192 (0.2181)	grad_norm 11164.0947 (16376.6865)	mem 4918MB
[2022-02-06 22:40:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9550/40036]	eta 1:14:14 lr 0.000013	time 0.1452 (0.1461)	loss 0.2177 (0.2181)	grad_norm 12147.7266 (16374.6455)	mem 4918MB
[2022-02-06 22:40:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9560/40036]	eta 1:14:12 lr 0.000013	time 0.1454 (0.1461)	loss 0.2180 (0.2181)	grad_norm 15465.9873 (16372.5049)	mem 4918MB
[2022-02-06 22:40:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9570/40036]	eta 1:14:11 lr 0.000013	time 0.1454 (0.1461)	loss 0.2158 (0.2181)	grad_norm 14322.9277 (16370.2285)	mem 4918MB
[2022-02-06 22:40:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9580/40036]	eta 1:14:09 lr 0.000013	time 0.1458 (0.1461)	loss 0.2163 (0.2181)	grad_norm 15526.6221 (16368.6514)	mem 4918MB
[2022-02-06 22:40:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9590/40036]	eta 1:14:08 lr 0.000013	time 0.1465 (0.1461)	loss 0.2177 (0.2181)	grad_norm 15062.2031 (16366.6553)	mem 4918MB
[2022-02-06 22:41:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9600/40036]	eta 1:14:07 lr 0.000013	time 0.1443 (0.1461)	loss 0.2181 (0.2181)	grad_norm 14286.6211 (16365.1807)	mem 4918MB
[2022-02-06 22:41:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9610/40036]	eta 1:14:05 lr 0.000013	time 0.1453 (0.1461)	loss 0.2218 (0.2181)	grad_norm 14995.2441 (16362.9229)	mem 4918MB
[2022-02-06 22:41:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9620/40036]	eta 1:14:04 lr 0.000013	time 0.1463 (0.1461)	loss 0.2192 (0.2181)	grad_norm 13218.7539 (16361.2539)	mem 4918MB
[2022-02-06 22:41:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9630/40036]	eta 1:14:02 lr 0.000013	time 0.1454 (0.1461)	loss 0.2164 (0.2181)	grad_norm 12356.1641 (16359.2236)	mem 4918MB
[2022-02-06 22:41:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9640/40036]	eta 1:14:01 lr 0.000013	time 0.1458 (0.1461)	loss 0.2141 (0.2181)	grad_norm 14812.2393 (16357.5781)	mem 4918MB
[2022-02-06 22:41:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9650/40036]	eta 1:13:59 lr 0.000013	time 0.1455 (0.1461)	loss 0.2166 (0.2181)	grad_norm 13738.6553 (16355.0869)	mem 4918MB
[2022-02-06 22:41:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9660/40036]	eta 1:13:58 lr 0.000013	time 0.1454 (0.1461)	loss 0.2177 (0.2181)	grad_norm 12404.6787 (16352.7949)	mem 4918MB
[2022-02-06 22:41:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9670/40036]	eta 1:13:57 lr 0.000013	time 0.1445 (0.1461)	loss 0.2183 (0.2181)	grad_norm 12845.4707 (16349.6240)	mem 4918MB
[2022-02-06 22:41:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9680/40036]	eta 1:13:55 lr 0.000013	time 0.1455 (0.1461)	loss 0.2140 (0.2181)	grad_norm 14667.1572 (16346.8662)	mem 4918MB
[2022-02-06 22:41:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9690/40036]	eta 1:13:54 lr 0.000013	time 0.1454 (0.1461)	loss 0.2184 (0.2181)	grad_norm 16198.2725 (16344.7822)	mem 4918MB
[2022-02-06 22:41:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9700/40036]	eta 1:13:52 lr 0.000013	time 0.1458 (0.1461)	loss 0.2177 (0.2181)	grad_norm 15051.9404 (16342.1328)	mem 4918MB
[2022-02-06 22:41:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9710/40036]	eta 1:13:51 lr 0.000013	time 0.1456 (0.1461)	loss 0.2154 (0.2181)	grad_norm 13933.4141 (16339.5645)	mem 4918MB
[2022-02-06 22:41:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9720/40036]	eta 1:13:49 lr 0.000013	time 0.1466 (0.1461)	loss 0.2174 (0.2181)	grad_norm 14509.0078 (16336.8652)	mem 4918MB
[2022-02-06 22:41:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9730/40036]	eta 1:13:48 lr 0.000013	time 0.1453 (0.1461)	loss 0.2173 (0.2181)	grad_norm 13542.1357 (16334.5244)	mem 4918MB
[2022-02-06 22:41:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9740/40036]	eta 1:13:47 lr 0.000013	time 0.1444 (0.1461)	loss 0.2159 (0.2181)	grad_norm 12774.0713 (16332.1797)	mem 4918MB
[2022-02-06 22:41:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9750/40036]	eta 1:13:45 lr 0.000013	time 0.1453 (0.1461)	loss 0.2164 (0.2181)	grad_norm 14847.6309 (16329.6562)	mem 4918MB
[2022-02-06 22:41:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9760/40036]	eta 1:13:44 lr 0.000013	time 0.1448 (0.1461)	loss 0.2186 (0.2181)	grad_norm 13222.2646 (16327.1104)	mem 4918MB
[2022-02-06 22:41:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9770/40036]	eta 1:13:42 lr 0.000013	time 0.1450 (0.1461)	loss 0.2187 (0.2181)	grad_norm 13402.5752 (16325.3467)	mem 4918MB
[2022-02-06 22:41:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9780/40036]	eta 1:13:41 lr 0.000013	time 0.1458 (0.1461)	loss 0.2169 (0.2181)	grad_norm 13926.3320 (16322.8857)	mem 4918MB
[2022-02-06 22:41:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9790/40036]	eta 1:13:39 lr 0.000013	time 0.1466 (0.1461)	loss 0.2191 (0.2181)	grad_norm 14302.7842 (16320.5000)	mem 4918MB
[2022-02-06 22:41:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9800/40036]	eta 1:13:38 lr 0.000013	time 0.1470 (0.1461)	loss 0.2214 (0.2181)	grad_norm 13373.7178 (16318.0918)	mem 4918MB
[2022-02-06 22:41:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9810/40036]	eta 1:13:36 lr 0.000013	time 0.1453 (0.1461)	loss 0.2190 (0.2181)	grad_norm 16411.8965 (16316.5332)	mem 4918MB
[2022-02-06 22:41:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9820/40036]	eta 1:13:35 lr 0.000013	time 0.1461 (0.1461)	loss 0.2170 (0.2181)	grad_norm 14492.3604 (16314.7490)	mem 4918MB
[2022-02-06 22:41:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9830/40036]	eta 1:13:34 lr 0.000013	time 0.1455 (0.1461)	loss 0.2191 (0.2181)	grad_norm 14115.9062 (16312.6338)	mem 4918MB
[2022-02-06 22:41:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9840/40036]	eta 1:13:32 lr 0.000013	time 0.1453 (0.1461)	loss 0.2181 (0.2181)	grad_norm 13398.8428 (16310.7227)	mem 4918MB
[2022-02-06 22:41:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9850/40036]	eta 1:13:31 lr 0.000013	time 0.1453 (0.1461)	loss 0.2185 (0.2181)	grad_norm 13335.8281 (16308.0264)	mem 4918MB
[2022-02-06 22:41:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9860/40036]	eta 1:13:29 lr 0.000013	time 0.1461 (0.1461)	loss 0.2167 (0.2181)	grad_norm 13147.0801 (16305.7969)	mem 4918MB
[2022-02-06 22:41:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9870/40036]	eta 1:13:28 lr 0.000013	time 0.1445 (0.1461)	loss 0.2190 (0.2181)	grad_norm 15124.3652 (16304.0137)	mem 4918MB
[2022-02-06 22:41:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9880/40036]	eta 1:13:26 lr 0.000013	time 0.1459 (0.1461)	loss 0.2196 (0.2181)	grad_norm 12047.5127 (16301.2158)	mem 4918MB
[2022-02-06 22:41:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9890/40036]	eta 1:13:25 lr 0.000013	time 0.1459 (0.1461)	loss 0.2155 (0.2181)	grad_norm 15648.3535 (16298.5508)	mem 4918MB
[2022-02-06 22:41:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9900/40036]	eta 1:13:24 lr 0.000013	time 0.1447 (0.1461)	loss 0.2179 (0.2181)	grad_norm 12064.8047 (16296.3506)	mem 4918MB
[2022-02-06 22:41:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9910/40036]	eta 1:13:22 lr 0.000013	time 0.1450 (0.1461)	loss 0.2163 (0.2181)	grad_norm 13941.2969 (16294.3193)	mem 4918MB
[2022-02-06 22:41:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9920/40036]	eta 1:13:21 lr 0.000013	time 0.1440 (0.1461)	loss 0.2175 (0.2181)	grad_norm 14275.3574 (16292.3193)	mem 4918MB
[2022-02-06 22:41:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9930/40036]	eta 1:13:19 lr 0.000013	time 0.1458 (0.1461)	loss 0.2150 (0.2181)	grad_norm 12645.7236 (16290.1025)	mem 4918MB
[2022-02-06 22:41:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9940/40036]	eta 1:13:18 lr 0.000013	time 0.1460 (0.1461)	loss 0.2167 (0.2181)	grad_norm 13595.1484 (16287.4404)	mem 4918MB
[2022-02-06 22:41:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9950/40036]	eta 1:13:16 lr 0.000013	time 0.1456 (0.1461)	loss 0.2193 (0.2181)	grad_norm 13076.7217 (16284.6426)	mem 4918MB
[2022-02-06 22:41:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9960/40036]	eta 1:13:15 lr 0.000013	time 0.1452 (0.1461)	loss 0.2132 (0.2181)	grad_norm 15780.0010 (16282.8047)	mem 4918MB
[2022-02-06 22:41:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9970/40036]	eta 1:13:13 lr 0.000013	time 0.1458 (0.1461)	loss 0.2164 (0.2181)	grad_norm 13270.7217 (16280.4463)	mem 4918MB
[2022-02-06 22:41:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9980/40036]	eta 1:13:12 lr 0.000013	time 0.1451 (0.1461)	loss 0.2189 (0.2181)	grad_norm 13370.8350 (16277.9180)	mem 4918MB
[2022-02-06 22:41:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][9990/40036]	eta 1:13:11 lr 0.000013	time 0.1453 (0.1461)	loss 0.2167 (0.2181)	grad_norm 12465.6201 (16275.5664)	mem 4918MB
[2022-02-06 22:42:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10000/40036]	eta 1:13:09 lr 0.000013	time 0.1461 (0.1461)	loss 0.2170 (0.2181)	grad_norm 11752.4229 (16273.5195)	mem 4918MB
[2022-02-06 22:42:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10010/40036]	eta 1:13:08 lr 0.000013	time 0.1452 (0.1461)	loss 0.2177 (0.2181)	grad_norm 14209.9502 (16271.2334)	mem 4918MB
[2022-02-06 22:42:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10020/40036]	eta 1:13:06 lr 0.000013	time 0.1465 (0.1461)	loss 0.2145 (0.2181)	grad_norm 13543.9434 (16268.9189)	mem 4918MB
[2022-02-06 22:42:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10030/40036]	eta 1:13:05 lr 0.000013	time 0.1456 (0.1461)	loss 0.2172 (0.2181)	grad_norm 14485.7627 (16266.5527)	mem 4918MB
[2022-02-06 22:42:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10040/40036]	eta 1:13:03 lr 0.000013	time 0.1455 (0.1461)	loss 0.2176 (0.2181)	grad_norm 12240.8525 (16263.9902)	mem 4918MB
[2022-02-06 22:42:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10050/40036]	eta 1:13:02 lr 0.000014	time 0.1443 (0.1462)	loss 0.2185 (0.2181)	grad_norm 14796.9209 (16262.6934)	mem 4918MB
[2022-02-06 22:42:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10060/40036]	eta 1:13:01 lr 0.000014	time 0.1448 (0.1462)	loss 0.2197 (0.2181)	grad_norm 13368.1982 (16259.9805)	mem 4918MB
[2022-02-06 22:42:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10070/40036]	eta 1:12:59 lr 0.000014	time 0.1457 (0.1462)	loss 0.2144 (0.2181)	grad_norm 14649.5195 (16257.7080)	mem 4918MB
[2022-02-06 22:42:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10080/40036]	eta 1:12:58 lr 0.000014	time 0.1467 (0.1462)	loss 0.2181 (0.2181)	grad_norm 14855.2676 (16255.7842)	mem 4918MB
[2022-02-06 22:42:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10090/40036]	eta 1:12:56 lr 0.000014	time 0.1448 (0.1462)	loss 0.2194 (0.2181)	grad_norm 14236.4434 (16253.3379)	mem 4918MB
[2022-02-06 22:42:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10100/40036]	eta 1:12:55 lr 0.000014	time 0.1464 (0.1462)	loss 0.2149 (0.2181)	grad_norm 16347.7461 (16251.4824)	mem 4918MB
[2022-02-06 22:42:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10110/40036]	eta 1:12:53 lr 0.000014	time 0.1449 (0.1462)	loss 0.2141 (0.2181)	grad_norm 13924.7520 (16249.2920)	mem 4918MB
[2022-02-06 22:42:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10120/40036]	eta 1:12:52 lr 0.000014	time 0.1459 (0.1462)	loss 0.2174 (0.2181)	grad_norm 13843.8623 (16246.8086)	mem 4918MB
[2022-02-06 22:42:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10130/40036]	eta 1:12:50 lr 0.000014	time 0.1461 (0.1462)	loss 0.2174 (0.2181)	grad_norm 13087.2383 (16244.9395)	mem 4918MB
[2022-02-06 22:42:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10140/40036]	eta 1:12:49 lr 0.000014	time 0.1449 (0.1462)	loss 0.2197 (0.2181)	grad_norm 14363.7686 (16242.6367)	mem 4918MB
[2022-02-06 22:42:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10150/40036]	eta 1:12:48 lr 0.000014	time 0.1452 (0.1462)	loss 0.2183 (0.2181)	grad_norm 13891.6904 (16240.9414)	mem 4918MB
[2022-02-06 22:42:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10160/40036]	eta 1:12:46 lr 0.000014	time 0.1461 (0.1462)	loss 0.2206 (0.2181)	grad_norm 13657.0391 (16238.7217)	mem 4918MB
[2022-02-06 22:42:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10170/40036]	eta 1:12:45 lr 0.000014	time 0.1444 (0.1462)	loss 0.2184 (0.2181)	grad_norm 14110.3232 (16236.6631)	mem 4918MB
[2022-02-06 22:42:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10180/40036]	eta 1:12:43 lr 0.000014	time 0.1460 (0.1462)	loss 0.2187 (0.2181)	grad_norm 13348.7207 (16233.9648)	mem 4918MB
[2022-02-06 22:42:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10190/40036]	eta 1:12:42 lr 0.000014	time 0.1468 (0.1462)	loss 0.2207 (0.2181)	grad_norm 14885.4883 (16232.0166)	mem 4918MB
[2022-02-06 22:42:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10200/40036]	eta 1:12:40 lr 0.000014	time 0.1468 (0.1462)	loss 0.2193 (0.2181)	grad_norm 14422.4922 (16229.2764)	mem 4918MB
[2022-02-06 22:42:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10210/40036]	eta 1:12:39 lr 0.000014	time 0.1459 (0.1462)	loss 0.2182 (0.2181)	grad_norm 12400.2861 (16226.9756)	mem 4918MB
[2022-02-06 22:42:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10220/40036]	eta 1:12:37 lr 0.000014	time 0.1455 (0.1462)	loss 0.2210 (0.2181)	grad_norm 15464.1963 (16225.4453)	mem 4918MB
[2022-02-06 22:42:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10230/40036]	eta 1:12:36 lr 0.000014	time 0.1456 (0.1462)	loss 0.2175 (0.2181)	grad_norm 14295.8926 (16223.1934)	mem 4918MB
[2022-02-06 22:42:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10240/40036]	eta 1:12:35 lr 0.000014	time 0.1446 (0.1462)	loss 0.2190 (0.2180)	grad_norm 13035.0049 (16220.7441)	mem 4918MB
[2022-02-06 22:42:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10250/40036]	eta 1:12:33 lr 0.000014	time 0.1463 (0.1462)	loss 0.2196 (0.2180)	grad_norm 12964.3662 (16218.2881)	mem 4918MB
[2022-02-06 22:42:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10260/40036]	eta 1:12:32 lr 0.000014	time 0.1467 (0.1462)	loss 0.2180 (0.2180)	grad_norm 13260.6641 (16215.7139)	mem 4918MB
[2022-02-06 22:42:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10270/40036]	eta 1:12:30 lr 0.000014	time 0.1439 (0.1462)	loss 0.2174 (0.2180)	grad_norm 13544.0449 (16213.1514)	mem 4918MB
[2022-02-06 22:42:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10280/40036]	eta 1:12:29 lr 0.000014	time 0.1449 (0.1462)	loss 0.2171 (0.2180)	grad_norm 12910.1611 (16210.9619)	mem 4918MB
[2022-02-06 22:42:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10290/40036]	eta 1:12:27 lr 0.000014	time 0.1454 (0.1462)	loss 0.2200 (0.2180)	grad_norm 13080.6660 (16208.1270)	mem 4918MB
[2022-02-06 22:42:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10300/40036]	eta 1:12:26 lr 0.000014	time 0.1454 (0.1462)	loss 0.2195 (0.2180)	grad_norm 16131.5400 (16206.1338)	mem 4918MB
[2022-02-06 22:42:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10310/40036]	eta 1:12:25 lr 0.000014	time 0.1451 (0.1462)	loss 0.2171 (0.2180)	grad_norm 13995.6768 (16204.0615)	mem 4918MB
[2022-02-06 22:42:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10320/40036]	eta 1:12:23 lr 0.000014	time 0.1456 (0.1462)	loss 0.2176 (0.2180)	grad_norm 14000.2822 (16202.0117)	mem 4918MB
[2022-02-06 22:42:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10330/40036]	eta 1:12:22 lr 0.000014	time 0.1456 (0.1462)	loss 0.2186 (0.2180)	grad_norm 12159.1562 (16199.7285)	mem 4918MB
[2022-02-06 22:42:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10340/40036]	eta 1:12:20 lr 0.000014	time 0.1461 (0.1462)	loss 0.2165 (0.2180)	grad_norm 12511.4248 (16197.5225)	mem 4918MB
[2022-02-06 22:42:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10350/40036]	eta 1:12:19 lr 0.000014	time 0.1457 (0.1462)	loss 0.2175 (0.2180)	grad_norm 13186.4971 (16195.0762)	mem 4918MB
[2022-02-06 22:42:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10360/40036]	eta 1:12:17 lr 0.000014	time 0.1467 (0.1462)	loss 0.2178 (0.2180)	grad_norm 12540.4014 (16192.1484)	mem 4918MB
[2022-02-06 22:42:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10370/40036]	eta 1:12:16 lr 0.000014	time 0.1466 (0.1462)	loss 0.2201 (0.2180)	grad_norm 15610.0781 (16189.7686)	mem 4918MB
[2022-02-06 22:42:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10380/40036]	eta 1:12:14 lr 0.000014	time 0.1447 (0.1462)	loss 0.2184 (0.2180)	grad_norm 12870.9170 (16188.0400)	mem 4918MB
[2022-02-06 22:42:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10390/40036]	eta 1:12:13 lr 0.000014	time 0.1451 (0.1462)	loss 0.2169 (0.2180)	grad_norm 14718.2051 (16186.3428)	mem 4918MB
[2022-02-06 22:42:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10400/40036]	eta 1:12:12 lr 0.000014	time 0.1441 (0.1462)	loss 0.2164 (0.2180)	grad_norm 14739.1992 (16184.8252)	mem 4918MB
[2022-02-06 22:43:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10410/40036]	eta 1:12:10 lr 0.000014	time 0.1454 (0.1462)	loss 0.2138 (0.2180)	grad_norm 13315.4717 (16183.0879)	mem 4918MB
[2022-02-06 22:43:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10420/40036]	eta 1:12:09 lr 0.000014	time 0.1461 (0.1462)	loss 0.2156 (0.2180)	grad_norm 12959.7188 (16180.6367)	mem 4918MB
[2022-02-06 22:43:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10430/40036]	eta 1:12:07 lr 0.000014	time 0.1455 (0.1462)	loss 0.2146 (0.2180)	grad_norm 11234.3584 (16178.5303)	mem 4918MB
[2022-02-06 22:43:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10440/40036]	eta 1:12:06 lr 0.000014	time 0.1455 (0.1462)	loss 0.2158 (0.2180)	grad_norm 12644.6797 (16176.0908)	mem 4918MB
[2022-02-06 22:43:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10450/40036]	eta 1:12:04 lr 0.000014	time 0.1459 (0.1462)	loss 0.2183 (0.2180)	grad_norm 12894.8721 (16173.8164)	mem 4918MB
[2022-02-06 22:43:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10460/40036]	eta 1:12:03 lr 0.000014	time 0.1450 (0.1462)	loss 0.2180 (0.2180)	grad_norm 14625.4990 (16171.8301)	mem 4918MB
[2022-02-06 22:43:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10470/40036]	eta 1:12:02 lr 0.000014	time 0.1453 (0.1462)	loss 0.2168 (0.2180)	grad_norm 12026.9375 (16168.9863)	mem 4918MB
[2022-02-06 22:43:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10480/40036]	eta 1:12:00 lr 0.000014	time 0.1447 (0.1462)	loss 0.2142 (0.2180)	grad_norm 17237.3340 (16166.9932)	mem 4918MB
[2022-02-06 22:43:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10490/40036]	eta 1:11:59 lr 0.000014	time 0.1456 (0.1462)	loss 0.2170 (0.2180)	grad_norm 13869.0449 (16164.6680)	mem 4918MB
[2022-02-06 22:43:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10500/40036]	eta 1:11:57 lr 0.000014	time 0.1461 (0.1462)	loss 0.2170 (0.2180)	grad_norm 14235.5361 (16162.6943)	mem 4918MB
[2022-02-06 22:43:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10510/40036]	eta 1:11:56 lr 0.000014	time 0.1457 (0.1462)	loss 0.2182 (0.2180)	grad_norm 12433.1904 (16160.2773)	mem 4918MB
[2022-02-06 22:43:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10520/40036]	eta 1:11:54 lr 0.000014	time 0.1459 (0.1462)	loss 0.2170 (0.2180)	grad_norm 12456.6885 (16157.4033)	mem 4918MB
[2022-02-06 22:43:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10530/40036]	eta 1:11:53 lr 0.000014	time 0.1461 (0.1462)	loss 0.2183 (0.2180)	grad_norm 14872.0908 (16155.0938)	mem 4918MB
[2022-02-06 22:43:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10540/40036]	eta 1:11:51 lr 0.000014	time 0.1448 (0.1462)	loss 0.2156 (0.2180)	grad_norm 11380.6396 (16152.9209)	mem 4918MB
[2022-02-06 22:43:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10550/40036]	eta 1:11:50 lr 0.000014	time 0.1483 (0.1462)	loss 0.2169 (0.2180)	grad_norm 13334.4395 (16150.5371)	mem 4918MB
[2022-02-06 22:43:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10560/40036]	eta 1:11:49 lr 0.000014	time 0.1447 (0.1462)	loss 0.2169 (0.2180)	grad_norm 14501.1699 (16148.2295)	mem 4918MB
[2022-02-06 22:43:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10570/40036]	eta 1:11:47 lr 0.000014	time 0.1459 (0.1462)	loss 0.2191 (0.2180)	grad_norm 14348.4141 (16146.5938)	mem 4918MB
[2022-02-06 22:43:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10580/40036]	eta 1:11:46 lr 0.000014	time 0.1461 (0.1462)	loss 0.2146 (0.2180)	grad_norm 11917.1982 (16144.2588)	mem 4918MB
[2022-02-06 22:43:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10590/40036]	eta 1:11:44 lr 0.000014	time 0.1458 (0.1462)	loss 0.2191 (0.2180)	grad_norm 15461.6562 (16142.1729)	mem 4918MB
[2022-02-06 22:43:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10600/40036]	eta 1:11:43 lr 0.000014	time 0.1458 (0.1462)	loss 0.2175 (0.2180)	grad_norm 14655.8994 (16140.0410)	mem 4918MB
[2022-02-06 22:43:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10610/40036]	eta 1:11:41 lr 0.000014	time 0.1446 (0.1462)	loss 0.2188 (0.2180)	grad_norm 15144.5137 (16137.0615)	mem 4918MB
[2022-02-06 22:43:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10620/40036]	eta 1:11:40 lr 0.000014	time 0.1454 (0.1462)	loss 0.2182 (0.2180)	grad_norm 14124.0459 (16134.7617)	mem 4918MB
[2022-02-06 22:43:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10630/40036]	eta 1:11:38 lr 0.000014	time 0.1458 (0.1462)	loss 0.2175 (0.2180)	grad_norm 13923.5566 (16132.0479)	mem 4918MB
[2022-02-06 22:43:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10640/40036]	eta 1:11:37 lr 0.000014	time 0.1455 (0.1462)	loss 0.2177 (0.2180)	grad_norm 14293.8105 (16129.8867)	mem 4918MB
[2022-02-06 22:43:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10650/40036]	eta 1:11:36 lr 0.000014	time 0.1451 (0.1462)	loss 0.2199 (0.2180)	grad_norm 13154.5918 (16127.8701)	mem 4918MB
[2022-02-06 22:43:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10660/40036]	eta 1:11:34 lr 0.000014	time 0.1455 (0.1462)	loss 0.2193 (0.2180)	grad_norm 11465.1279 (16125.5410)	mem 4918MB
[2022-02-06 22:43:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10670/40036]	eta 1:11:33 lr 0.000014	time 0.1451 (0.1462)	loss 0.2170 (0.2180)	grad_norm 11866.4297 (16122.5654)	mem 4918MB
[2022-02-06 22:43:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10680/40036]	eta 1:11:31 lr 0.000014	time 0.1455 (0.1462)	loss 0.2187 (0.2180)	grad_norm 12223.9717 (16120.2705)	mem 4918MB
[2022-02-06 22:43:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10690/40036]	eta 1:11:30 lr 0.000014	time 0.1459 (0.1462)	loss 0.2154 (0.2180)	grad_norm 13752.2012 (16117.9805)	mem 4918MB
[2022-02-06 22:43:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10700/40036]	eta 1:11:28 lr 0.000014	time 0.1454 (0.1462)	loss 0.2172 (0.2180)	grad_norm 12694.7441 (16115.2871)	mem 4918MB
[2022-02-06 22:43:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10710/40036]	eta 1:11:27 lr 0.000014	time 0.1447 (0.1462)	loss 0.2198 (0.2180)	grad_norm 12289.8408 (16113.0879)	mem 4918MB
[2022-02-06 22:43:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10720/40036]	eta 1:11:26 lr 0.000014	time 0.1510 (0.1462)	loss 0.2177 (0.2180)	grad_norm 12693.4707 (16110.8408)	mem 4918MB
[2022-02-06 22:43:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10730/40036]	eta 1:11:24 lr 0.000014	time 0.1575 (0.1462)	loss 0.2158 (0.2180)	grad_norm 13815.2998 (16108.3994)	mem 4918MB
[2022-02-06 22:43:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10740/40036]	eta 1:11:23 lr 0.000014	time 0.1456 (0.1462)	loss 0.2208 (0.2180)	grad_norm 13644.8545 (16106.1523)	mem 4918MB
[2022-02-06 22:43:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10750/40036]	eta 1:11:22 lr 0.000014	time 0.1558 (0.1462)	loss 0.2180 (0.2180)	grad_norm 13603.9707 (16103.7119)	mem 4918MB
[2022-02-06 22:43:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10760/40036]	eta 1:11:20 lr 0.000014	time 0.1485 (0.1462)	loss 0.2184 (0.2180)	grad_norm 13798.1689 (16101.3184)	mem 4918MB
[2022-02-06 22:43:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10770/40036]	eta 1:11:19 lr 0.000014	time 0.1460 (0.1462)	loss 0.2169 (0.2180)	grad_norm 12991.0342 (16098.9033)	mem 4918MB
[2022-02-06 22:43:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10780/40036]	eta 1:11:17 lr 0.000014	time 0.1464 (0.1462)	loss 0.2153 (0.2180)	grad_norm 13823.6553 (16096.6406)	mem 4918MB
[2022-02-06 22:43:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10790/40036]	eta 1:11:16 lr 0.000014	time 0.1454 (0.1462)	loss 0.2189 (0.2180)	grad_norm 11961.8594 (16093.9189)	mem 4918MB
[2022-02-06 22:43:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10800/40036]	eta 1:11:15 lr 0.000014	time 0.1446 (0.1462)	loss 0.2141 (0.2180)	grad_norm 11536.6660 (16091.2646)	mem 4918MB
[2022-02-06 22:43:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10810/40036]	eta 1:11:13 lr 0.000014	time 0.1466 (0.1462)	loss 0.2179 (0.2180)	grad_norm 11941.8691 (16088.6211)	mem 4918MB
[2022-02-06 22:44:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10820/40036]	eta 1:11:12 lr 0.000014	time 0.1459 (0.1462)	loss 0.2135 (0.2180)	grad_norm 12003.9111 (16086.4287)	mem 4918MB
[2022-02-06 22:44:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10830/40036]	eta 1:11:10 lr 0.000014	time 0.1458 (0.1462)	loss 0.2195 (0.2180)	grad_norm 12404.4951 (16084.4307)	mem 4918MB
[2022-02-06 22:44:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10840/40036]	eta 1:11:09 lr 0.000014	time 0.1452 (0.1462)	loss 0.2158 (0.2180)	grad_norm 13039.2100 (16081.7695)	mem 4918MB
[2022-02-06 22:44:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10850/40036]	eta 1:11:07 lr 0.000015	time 0.1457 (0.1462)	loss 0.2185 (0.2180)	grad_norm 11949.2178 (16079.1357)	mem 4918MB
[2022-02-06 22:44:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10860/40036]	eta 1:11:06 lr 0.000015	time 0.1462 (0.1462)	loss 0.2188 (0.2180)	grad_norm 13873.6670 (16076.7646)	mem 4918MB
[2022-02-06 22:44:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10870/40036]	eta 1:11:04 lr 0.000015	time 0.1528 (0.1462)	loss 0.2168 (0.2180)	grad_norm 16290.8594 (16074.4619)	mem 4918MB
[2022-02-06 22:44:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10880/40036]	eta 1:11:03 lr 0.000015	time 0.1438 (0.1462)	loss 0.2182 (0.2180)	grad_norm 13862.5996 (16071.7393)	mem 4918MB
[2022-02-06 22:44:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10890/40036]	eta 1:11:02 lr 0.000015	time 0.1450 (0.1462)	loss 0.2194 (0.2180)	grad_norm 12838.4961 (16069.3623)	mem 4918MB
[2022-02-06 22:44:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10900/40036]	eta 1:11:00 lr 0.000015	time 0.1448 (0.1462)	loss 0.2180 (0.2180)	grad_norm 12323.7910 (16067.6270)	mem 4918MB
[2022-02-06 22:44:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10910/40036]	eta 1:10:59 lr 0.000015	time 0.1461 (0.1462)	loss 0.2195 (0.2180)	grad_norm 15059.8779 (16064.5479)	mem 4918MB
[2022-02-06 22:44:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10920/40036]	eta 1:10:57 lr 0.000015	time 0.1468 (0.1462)	loss 0.2169 (0.2180)	grad_norm 13933.7080 (16062.4277)	mem 4918MB
[2022-02-06 22:44:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10930/40036]	eta 1:10:56 lr 0.000015	time 0.1477 (0.1462)	loss 0.2197 (0.2180)	grad_norm 13921.1611 (16060.1279)	mem 4918MB
[2022-02-06 22:44:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10940/40036]	eta 1:10:55 lr 0.000015	time 0.1534 (0.1462)	loss 0.2157 (0.2180)	grad_norm 12597.3242 (16057.9248)	mem 4918MB
[2022-02-06 22:44:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10950/40036]	eta 1:10:53 lr 0.000015	time 0.1454 (0.1462)	loss 0.2187 (0.2180)	grad_norm 13348.2852 (16055.0928)	mem 4918MB
[2022-02-06 22:44:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10960/40036]	eta 1:10:52 lr 0.000015	time 0.1483 (0.1462)	loss 0.2168 (0.2180)	grad_norm 12394.7871 (16052.7695)	mem 4918MB
[2022-02-06 22:44:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10970/40036]	eta 1:10:50 lr 0.000015	time 0.1458 (0.1462)	loss 0.2202 (0.2180)	grad_norm 14738.3252 (16050.2178)	mem 4918MB
[2022-02-06 22:44:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10980/40036]	eta 1:10:49 lr 0.000015	time 0.1464 (0.1462)	loss 0.2153 (0.2180)	grad_norm 13341.2646 (16047.3994)	mem 4918MB
[2022-02-06 22:44:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][10990/40036]	eta 1:10:48 lr 0.000015	time 0.1482 (0.1463)	loss 0.2206 (0.2180)	grad_norm 14793.6650 (16045.1709)	mem 4918MB
[2022-02-06 22:44:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11000/40036]	eta 1:10:46 lr 0.000015	time 0.1463 (0.1463)	loss 0.2180 (0.2180)	grad_norm 13039.6514 (16042.0420)	mem 4918MB
[2022-02-06 22:44:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11010/40036]	eta 1:10:45 lr 0.000015	time 0.1453 (0.1463)	loss 0.2141 (0.2180)	grad_norm 13250.4492 (16039.1572)	mem 4918MB
[2022-02-06 22:44:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11020/40036]	eta 1:10:43 lr 0.000015	time 0.1453 (0.1463)	loss 0.2197 (0.2180)	grad_norm 13584.4551 (16036.4541)	mem 4918MB
[2022-02-06 22:44:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11030/40036]	eta 1:10:42 lr 0.000015	time 0.1458 (0.1463)	loss 0.2170 (0.2180)	grad_norm 13108.1787 (16033.1494)	mem 4918MB
[2022-02-06 22:44:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11040/40036]	eta 1:10:40 lr 0.000015	time 0.1445 (0.1463)	loss 0.2164 (0.2180)	grad_norm 12241.8359 (16030.4395)	mem 4918MB
[2022-02-06 22:44:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11050/40036]	eta 1:10:39 lr 0.000015	time 0.1460 (0.1463)	loss 0.2199 (0.2180)	grad_norm 14961.6123 (16028.1260)	mem 4918MB
[2022-02-06 22:44:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11060/40036]	eta 1:10:38 lr 0.000015	time 0.1453 (0.1463)	loss 0.2170 (0.2180)	grad_norm 11638.2441 (16025.8652)	mem 4918MB
[2022-02-06 22:44:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11070/40036]	eta 1:10:36 lr 0.000015	time 0.1454 (0.1463)	loss 0.2147 (0.2180)	grad_norm 12650.4512 (16022.7285)	mem 4918MB
[2022-02-06 22:44:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11080/40036]	eta 1:10:35 lr 0.000015	time 0.1459 (0.1463)	loss 0.2177 (0.2180)	grad_norm 13394.8555 (16020.6172)	mem 4918MB
[2022-02-06 22:44:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11090/40036]	eta 1:10:33 lr 0.000015	time 0.1461 (0.1463)	loss 0.2179 (0.2180)	grad_norm 11018.1738 (16018.1680)	mem 4918MB
[2022-02-06 22:44:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11100/40036]	eta 1:10:32 lr 0.000015	time 0.1467 (0.1463)	loss 0.2189 (0.2180)	grad_norm 15001.6172 (16015.3984)	mem 4918MB
[2022-02-06 22:44:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11110/40036]	eta 1:10:30 lr 0.000015	time 0.1449 (0.1463)	loss 0.2181 (0.2180)	grad_norm 13400.7637 (16013.2402)	mem 4918MB
[2022-02-06 22:44:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11120/40036]	eta 1:10:29 lr 0.000015	time 0.1452 (0.1463)	loss 0.2174 (0.2180)	grad_norm 11530.0234 (16010.7529)	mem 4918MB
[2022-02-06 22:44:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11130/40036]	eta 1:10:27 lr 0.000015	time 0.1458 (0.1463)	loss 0.2194 (0.2180)	grad_norm 13816.4600 (16008.1016)	mem 4918MB
[2022-02-06 22:44:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11140/40036]	eta 1:10:26 lr 0.000015	time 0.1455 (0.1463)	loss 0.2171 (0.2180)	grad_norm 13747.0664 (16006.2998)	mem 4918MB
[2022-02-06 22:44:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11150/40036]	eta 1:10:24 lr 0.000015	time 0.1449 (0.1463)	loss 0.2168 (0.2180)	grad_norm 12239.9385 (16003.5166)	mem 4918MB
[2022-02-06 22:44:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11160/40036]	eta 1:10:23 lr 0.000015	time 0.1458 (0.1463)	loss 0.2184 (0.2180)	grad_norm 13380.7285 (16001.4150)	mem 4918MB
[2022-02-06 22:44:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11170/40036]	eta 1:10:22 lr 0.000015	time 0.1458 (0.1463)	loss 0.2136 (0.2180)	grad_norm 13108.1426 (15998.9795)	mem 4918MB
[2022-02-06 22:44:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11180/40036]	eta 1:10:20 lr 0.000015	time 0.1457 (0.1463)	loss 0.2177 (0.2180)	grad_norm 14491.4854 (15996.6543)	mem 4918MB
[2022-02-06 22:44:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11190/40036]	eta 1:10:19 lr 0.000015	time 0.1455 (0.1463)	loss 0.2178 (0.2180)	grad_norm 11715.7188 (15994.2656)	mem 4918MB
[2022-02-06 22:44:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11200/40036]	eta 1:10:17 lr 0.000015	time 0.1447 (0.1463)	loss 0.2174 (0.2180)	grad_norm 13808.2539 (15991.4482)	mem 4918MB
[2022-02-06 22:44:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11210/40036]	eta 1:10:16 lr 0.000015	time 0.1455 (0.1463)	loss 0.2170 (0.2180)	grad_norm 13194.3057 (15989.0312)	mem 4918MB
[2022-02-06 22:44:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11220/40036]	eta 1:10:14 lr 0.000015	time 0.1472 (0.1463)	loss 0.2182 (0.2180)	grad_norm 14998.1270 (15986.9600)	mem 4918MB
[2022-02-06 22:45:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11230/40036]	eta 1:10:13 lr 0.000015	time 0.1466 (0.1463)	loss 0.2170 (0.2180)	grad_norm 13940.8076 (15984.6631)	mem 4918MB
[2022-02-06 22:45:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11240/40036]	eta 1:10:11 lr 0.000015	time 0.1450 (0.1463)	loss 0.2180 (0.2180)	grad_norm 12030.2861 (15982.4268)	mem 4918MB
[2022-02-06 22:45:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11250/40036]	eta 1:10:10 lr 0.000015	time 0.1463 (0.1463)	loss 0.2191 (0.2180)	grad_norm 12769.7910 (15980.1572)	mem 4918MB
[2022-02-06 22:45:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11260/40036]	eta 1:10:08 lr 0.000015	time 0.1453 (0.1463)	loss 0.2179 (0.2180)	grad_norm 13474.9834 (15977.2383)	mem 4918MB
[2022-02-06 22:45:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11270/40036]	eta 1:10:07 lr 0.000015	time 0.1466 (0.1463)	loss 0.2180 (0.2180)	grad_norm 13469.6758 (15974.8525)	mem 4918MB
[2022-02-06 22:45:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11280/40036]	eta 1:10:06 lr 0.000015	time 0.1458 (0.1463)	loss 0.2177 (0.2180)	grad_norm 14844.7520 (15973.0625)	mem 4918MB
[2022-02-06 22:45:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11290/40036]	eta 1:10:04 lr 0.000015	time 0.1464 (0.1463)	loss 0.2191 (0.2180)	grad_norm 13387.6162 (15970.1523)	mem 4918MB
[2022-02-06 22:45:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11300/40036]	eta 1:10:03 lr 0.000015	time 0.1456 (0.1463)	loss 0.2158 (0.2180)	grad_norm 12707.4375 (15967.4395)	mem 4918MB
[2022-02-06 22:45:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11310/40036]	eta 1:10:01 lr 0.000015	time 0.1452 (0.1463)	loss 0.2172 (0.2180)	grad_norm 13107.3369 (15964.7891)	mem 4918MB
[2022-02-06 22:45:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11320/40036]	eta 1:10:00 lr 0.000015	time 0.1452 (0.1463)	loss 0.2195 (0.2180)	grad_norm 13807.3271 (15961.8594)	mem 4918MB
[2022-02-06 22:45:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11330/40036]	eta 1:09:58 lr 0.000015	time 0.1453 (0.1463)	loss 0.2180 (0.2180)	grad_norm 13866.1650 (15958.6846)	mem 4918MB
[2022-02-06 22:45:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11340/40036]	eta 1:09:57 lr 0.000015	time 0.1466 (0.1463)	loss 0.2188 (0.2180)	grad_norm 12229.5088 (15956.2080)	mem 4918MB
[2022-02-06 22:45:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11350/40036]	eta 1:09:55 lr 0.000015	time 0.1455 (0.1463)	loss 0.2166 (0.2180)	grad_norm 12334.2900 (15953.4277)	mem 4918MB
[2022-02-06 22:45:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11360/40036]	eta 1:09:54 lr 0.000015	time 0.1451 (0.1463)	loss 0.2147 (0.2180)	grad_norm 12670.6357 (15950.8174)	mem 4918MB
[2022-02-06 22:45:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11370/40036]	eta 1:09:53 lr 0.000015	time 0.1448 (0.1463)	loss 0.2165 (0.2180)	grad_norm 14476.9717 (15948.4629)	mem 4918MB
[2022-02-06 22:45:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11380/40036]	eta 1:09:51 lr 0.000015	time 0.1471 (0.1463)	loss 0.2163 (0.2180)	grad_norm 13019.5596 (15945.9102)	mem 4918MB
[2022-02-06 22:45:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11390/40036]	eta 1:09:50 lr 0.000015	time 0.1470 (0.1463)	loss 0.2147 (0.2180)	grad_norm 11488.9902 (15943.2354)	mem 4918MB
[2022-02-06 22:45:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11400/40036]	eta 1:09:48 lr 0.000015	time 0.1460 (0.1463)	loss 0.2194 (0.2180)	grad_norm 13452.4082 (15940.8838)	mem 4918MB
[2022-02-06 22:45:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11410/40036]	eta 1:09:47 lr 0.000015	time 0.1448 (0.1463)	loss 0.2189 (0.2180)	grad_norm 12058.8887 (15938.9521)	mem 4918MB
[2022-02-06 22:45:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11420/40036]	eta 1:09:45 lr 0.000015	time 0.1470 (0.1463)	loss 0.2165 (0.2180)	grad_norm 11855.4609 (15936.2646)	mem 4918MB
[2022-02-06 22:45:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11430/40036]	eta 1:09:44 lr 0.000015	time 0.1464 (0.1463)	loss 0.2164 (0.2180)	grad_norm 13527.1055 (15933.9922)	mem 4918MB
[2022-02-06 22:45:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11440/40036]	eta 1:09:42 lr 0.000015	time 0.1455 (0.1463)	loss 0.2187 (0.2180)	grad_norm 13743.9834 (15931.5498)	mem 4918MB
[2022-02-06 22:45:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11450/40036]	eta 1:09:41 lr 0.000015	time 0.1445 (0.1463)	loss 0.2194 (0.2180)	grad_norm 13889.7275 (15928.6074)	mem 4918MB
[2022-02-06 22:45:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11460/40036]	eta 1:09:40 lr 0.000015	time 0.1527 (0.1463)	loss 0.2174 (0.2180)	grad_norm 13281.5977 (15926.5146)	mem 4918MB
[2022-02-06 22:45:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11470/40036]	eta 1:09:38 lr 0.000015	time 0.1464 (0.1463)	loss 0.2156 (0.2180)	grad_norm 11532.7559 (15923.5449)	mem 4918MB
[2022-02-06 22:45:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11480/40036]	eta 1:09:37 lr 0.000015	time 0.1463 (0.1463)	loss 0.2162 (0.2180)	grad_norm 13302.8828 (15920.9707)	mem 4918MB
[2022-02-06 22:45:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11490/40036]	eta 1:09:35 lr 0.000015	time 0.1451 (0.1463)	loss 0.2180 (0.2180)	grad_norm 15142.0518 (15918.2500)	mem 4918MB
[2022-02-06 22:45:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11500/40036]	eta 1:09:34 lr 0.000015	time 0.1461 (0.1463)	loss 0.2183 (0.2180)	grad_norm 13255.6582 (15915.6240)	mem 4918MB
[2022-02-06 22:45:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11510/40036]	eta 1:09:32 lr 0.000015	time 0.1476 (0.1463)	loss 0.2178 (0.2180)	grad_norm 10938.3213 (15912.5625)	mem 4918MB
[2022-02-06 22:45:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11520/40036]	eta 1:09:31 lr 0.000015	time 0.1466 (0.1463)	loss 0.2165 (0.2180)	grad_norm 12410.3174 (15909.7246)	mem 4918MB
[2022-02-06 22:45:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11530/40036]	eta 1:09:30 lr 0.000015	time 0.1478 (0.1463)	loss 0.2189 (0.2180)	grad_norm 14358.5059 (15907.5967)	mem 4918MB
[2022-02-06 22:45:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11540/40036]	eta 1:09:28 lr 0.000015	time 0.1464 (0.1463)	loss 0.2207 (0.2180)	grad_norm 13282.5098 (15904.8398)	mem 4918MB
[2022-02-06 22:45:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11550/40036]	eta 1:09:27 lr 0.000015	time 0.1461 (0.1463)	loss 0.2188 (0.2180)	grad_norm 11632.9678 (15902.0850)	mem 4918MB
[2022-02-06 22:45:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11560/40036]	eta 1:09:25 lr 0.000015	time 0.1539 (0.1463)	loss 0.2185 (0.2180)	grad_norm 14090.7236 (15900.3467)	mem 4918MB
[2022-02-06 22:45:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11570/40036]	eta 1:09:24 lr 0.000015	time 0.1476 (0.1463)	loss 0.2203 (0.2180)	grad_norm 14261.5928 (15898.0811)	mem 4918MB
[2022-02-06 22:45:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11580/40036]	eta 1:09:23 lr 0.000015	time 0.1498 (0.1463)	loss 0.2167 (0.2180)	grad_norm 13575.1719 (15895.8125)	mem 4918MB
[2022-02-06 22:45:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11590/40036]	eta 1:09:21 lr 0.000015	time 0.1542 (0.1463)	loss 0.2176 (0.2180)	grad_norm 13088.6475 (15893.5625)	mem 4918MB
[2022-02-06 22:45:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11600/40036]	eta 1:09:20 lr 0.000015	time 0.1530 (0.1463)	loss 0.2216 (0.2180)	grad_norm 15218.3721 (15891.4023)	mem 4918MB
[2022-02-06 22:45:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11610/40036]	eta 1:09:19 lr 0.000015	time 0.1487 (0.1463)	loss 0.2181 (0.2180)	grad_norm 14263.2236 (15888.4971)	mem 4918MB
[2022-02-06 22:45:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11620/40036]	eta 1:09:18 lr 0.000015	time 0.1488 (0.1463)	loss 0.2169 (0.2180)	grad_norm 14692.2002 (15886.2568)	mem 4918MB
[2022-02-06 22:46:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11630/40036]	eta 1:09:16 lr 0.000015	time 0.1472 (0.1463)	loss 0.2180 (0.2180)	grad_norm 10989.6025 (15884.0684)	mem 4918MB
[2022-02-06 22:46:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11640/40036]	eta 1:09:15 lr 0.000015	time 0.1470 (0.1463)	loss 0.2167 (0.2180)	grad_norm 12583.2764 (15881.7129)	mem 4918MB
[2022-02-06 22:46:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11650/40036]	eta 1:09:13 lr 0.000016	time 0.1462 (0.1463)	loss 0.2186 (0.2180)	grad_norm 14003.8896 (15879.5000)	mem 4918MB
[2022-02-06 22:46:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11660/40036]	eta 1:09:12 lr 0.000016	time 0.1460 (0.1463)	loss 0.2222 (0.2180)	grad_norm 13283.1426 (15877.0771)	mem 4918MB
[2022-02-06 22:46:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11670/40036]	eta 1:09:10 lr 0.000016	time 0.1467 (0.1463)	loss 0.2141 (0.2180)	grad_norm 12268.5137 (15874.8311)	mem 4918MB
[2022-02-06 22:46:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11680/40036]	eta 1:09:09 lr 0.000016	time 0.1446 (0.1463)	loss 0.2176 (0.2180)	grad_norm 10938.2676 (15871.9707)	mem 4918MB
[2022-02-06 22:46:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11690/40036]	eta 1:09:08 lr 0.000016	time 0.1482 (0.1463)	loss 0.2168 (0.2180)	grad_norm 13748.3389 (15869.1719)	mem 4918MB
[2022-02-06 22:46:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11700/40036]	eta 1:09:06 lr 0.000016	time 0.1565 (0.1463)	loss 0.2159 (0.2180)	grad_norm 11605.9434 (15866.3467)	mem 4918MB
[2022-02-06 22:46:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11710/40036]	eta 1:09:05 lr 0.000016	time 0.1508 (0.1463)	loss 0.2187 (0.2180)	grad_norm 12571.4512 (15863.8252)	mem 4918MB
[2022-02-06 22:46:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11720/40036]	eta 1:09:04 lr 0.000016	time 0.1528 (0.1464)	loss 0.2193 (0.2180)	grad_norm 12326.8516 (15862.1143)	mem 4918MB
[2022-02-06 22:46:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11730/40036]	eta 1:09:02 lr 0.000016	time 0.1465 (0.1464)	loss 0.2167 (0.2180)	grad_norm 13168.2920 (15859.9160)	mem 4918MB
[2022-02-06 22:46:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11740/40036]	eta 1:09:01 lr 0.000016	time 0.1463 (0.1464)	loss 0.2184 (0.2180)	grad_norm 13762.4258 (15857.4395)	mem 4918MB
[2022-02-06 22:46:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11750/40036]	eta 1:08:59 lr 0.000016	time 0.1452 (0.1464)	loss 0.2133 (0.2180)	grad_norm 12587.9512 (15855.0049)	mem 4918MB
[2022-02-06 22:46:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11760/40036]	eta 1:08:58 lr 0.000016	time 0.1446 (0.1464)	loss 0.2192 (0.2180)	grad_norm 11464.2822 (15852.1406)	mem 4918MB
[2022-02-06 22:46:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11770/40036]	eta 1:08:56 lr 0.000016	time 0.1442 (0.1464)	loss 0.2182 (0.2180)	grad_norm 13805.6670 (15849.1396)	mem 4918MB
[2022-02-06 22:46:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11780/40036]	eta 1:08:55 lr 0.000016	time 0.1452 (0.1464)	loss 0.2155 (0.2180)	grad_norm 12851.8701 (15846.2100)	mem 4918MB
[2022-02-06 22:46:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11790/40036]	eta 1:08:53 lr 0.000016	time 0.1451 (0.1464)	loss 0.2167 (0.2180)	grad_norm 12600.0840 (15843.4990)	mem 4918MB
[2022-02-06 22:46:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11800/40036]	eta 1:08:52 lr 0.000016	time 0.1457 (0.1464)	loss 0.2170 (0.2180)	grad_norm 13409.5391 (15840.9746)	mem 4918MB
[2022-02-06 22:46:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11810/40036]	eta 1:08:51 lr 0.000016	time 0.1464 (0.1464)	loss 0.2194 (0.2180)	grad_norm 13290.8066 (15838.7559)	mem 4918MB
[2022-02-06 22:46:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11820/40036]	eta 1:08:49 lr 0.000016	time 0.1466 (0.1464)	loss 0.2189 (0.2180)	grad_norm 12239.0088 (15836.6445)	mem 4918MB
[2022-02-06 22:46:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11830/40036]	eta 1:08:48 lr 0.000016	time 0.1464 (0.1464)	loss 0.2165 (0.2180)	grad_norm 10833.2236 (15834.2607)	mem 4918MB
[2022-02-06 22:46:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11840/40036]	eta 1:08:46 lr 0.000016	time 0.1444 (0.1464)	loss 0.2195 (0.2180)	grad_norm 12726.3105 (15831.7217)	mem 4918MB
[2022-02-06 22:46:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11850/40036]	eta 1:08:45 lr 0.000016	time 0.1459 (0.1464)	loss 0.2161 (0.2180)	grad_norm 14308.6309 (15829.1934)	mem 4918MB
[2022-02-06 22:46:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11860/40036]	eta 1:08:43 lr 0.000016	time 0.1451 (0.1464)	loss 0.2157 (0.2180)	grad_norm 13536.5107 (15827.3350)	mem 4918MB
[2022-02-06 22:46:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11870/40036]	eta 1:08:42 lr 0.000016	time 0.1450 (0.1464)	loss 0.2180 (0.2180)	grad_norm 14268.4336 (15825.1299)	mem 4918MB
[2022-02-06 22:46:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11880/40036]	eta 1:08:40 lr 0.000016	time 0.1449 (0.1464)	loss 0.2173 (0.2180)	grad_norm 12661.2061 (15822.8750)	mem 4918MB
[2022-02-06 22:46:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11890/40036]	eta 1:08:39 lr 0.000016	time 0.1445 (0.1464)	loss 0.2193 (0.2180)	grad_norm 12386.7217 (15820.6465)	mem 4918MB
[2022-02-06 22:46:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11900/40036]	eta 1:08:37 lr 0.000016	time 0.1457 (0.1464)	loss 0.2138 (0.2180)	grad_norm 13651.5908 (15818.2725)	mem 4918MB
[2022-02-06 22:46:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11910/40036]	eta 1:08:36 lr 0.000016	time 0.1455 (0.1464)	loss 0.2178 (0.2180)	grad_norm 13031.4473 (15815.6533)	mem 4918MB
[2022-02-06 22:46:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11920/40036]	eta 1:08:34 lr 0.000016	time 0.1448 (0.1464)	loss 0.2194 (0.2180)	grad_norm 13405.3789 (15813.1631)	mem 4918MB
[2022-02-06 22:46:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11930/40036]	eta 1:08:33 lr 0.000016	time 0.1457 (0.1464)	loss 0.2180 (0.2180)	grad_norm 13237.8066 (15810.7207)	mem 4918MB
[2022-02-06 22:46:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11940/40036]	eta 1:08:32 lr 0.000016	time 0.1451 (0.1464)	loss 0.2169 (0.2180)	grad_norm 12104.7549 (15808.1094)	mem 4918MB
[2022-02-06 22:46:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11950/40036]	eta 1:08:30 lr 0.000016	time 0.1459 (0.1464)	loss 0.2161 (0.2180)	grad_norm 14166.9053 (15805.4316)	mem 4918MB
[2022-02-06 22:46:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11960/40036]	eta 1:08:29 lr 0.000016	time 0.1461 (0.1464)	loss 0.2186 (0.2180)	grad_norm 11926.5059 (15802.8076)	mem 4918MB
[2022-02-06 22:46:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11970/40036]	eta 1:08:27 lr 0.000016	time 0.1459 (0.1464)	loss 0.2172 (0.2180)	grad_norm 11979.4639 (15800.4102)	mem 4918MB
[2022-02-06 22:46:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11980/40036]	eta 1:08:26 lr 0.000016	time 0.1446 (0.1464)	loss 0.2151 (0.2180)	grad_norm 12223.7988 (15797.7637)	mem 4918MB
[2022-02-06 22:46:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][11990/40036]	eta 1:08:24 lr 0.000016	time 0.1445 (0.1464)	loss 0.2187 (0.2180)	grad_norm 14533.9668 (15795.2773)	mem 4918MB
[2022-02-06 22:46:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12000/40036]	eta 1:08:23 lr 0.000016	time 0.1440 (0.1464)	loss 0.2179 (0.2180)	grad_norm 11100.4951 (15792.2627)	mem 4918MB
[2022-02-06 22:46:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12010/40036]	eta 1:08:21 lr 0.000016	time 0.1453 (0.1464)	loss 0.2148 (0.2180)	grad_norm 13692.4688 (15789.7383)	mem 4918MB
[2022-02-06 22:46:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12020/40036]	eta 1:08:20 lr 0.000016	time 0.1448 (0.1464)	loss 0.2176 (0.2180)	grad_norm 15487.7070 (15787.7090)	mem 4918MB
[2022-02-06 22:46:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12030/40036]	eta 1:08:18 lr 0.000016	time 0.1456 (0.1464)	loss 0.2204 (0.2180)	grad_norm 13812.7939 (15785.3242)	mem 4918MB
[2022-02-06 22:47:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12040/40036]	eta 1:08:17 lr 0.000016	time 0.1453 (0.1464)	loss 0.2208 (0.2180)	grad_norm 12712.2432 (15782.8516)	mem 4918MB
[2022-02-06 22:47:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12050/40036]	eta 1:08:15 lr 0.000016	time 0.1451 (0.1464)	loss 0.2174 (0.2180)	grad_norm 12008.0303 (15780.5400)	mem 4918MB
[2022-02-06 22:47:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12060/40036]	eta 1:08:14 lr 0.000016	time 0.1457 (0.1464)	loss 0.2138 (0.2180)	grad_norm 12348.9092 (15777.6367)	mem 4918MB
[2022-02-06 22:47:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12070/40036]	eta 1:08:12 lr 0.000016	time 0.1468 (0.1464)	loss 0.2180 (0.2180)	grad_norm 12449.7549 (15775.3975)	mem 4918MB
[2022-02-06 22:47:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12080/40036]	eta 1:08:11 lr 0.000016	time 0.1463 (0.1464)	loss 0.2201 (0.2180)	grad_norm 12850.9932 (15772.6416)	mem 4918MB
[2022-02-06 22:47:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12090/40036]	eta 1:08:10 lr 0.000016	time 0.1452 (0.1464)	loss 0.2177 (0.2180)	grad_norm 12088.5742 (15770.0010)	mem 4918MB
[2022-02-06 22:47:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12100/40036]	eta 1:08:08 lr 0.000016	time 0.1449 (0.1464)	loss 0.2218 (0.2180)	grad_norm 12990.0674 (15767.8955)	mem 4918MB
[2022-02-06 22:47:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12110/40036]	eta 1:08:07 lr 0.000016	time 0.1443 (0.1464)	loss 0.2154 (0.2180)	grad_norm 11960.4932 (15765.3232)	mem 4918MB
[2022-02-06 22:47:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12120/40036]	eta 1:08:05 lr 0.000016	time 0.1447 (0.1464)	loss 0.2184 (0.2180)	grad_norm 12186.3926 (15763.3027)	mem 4918MB
[2022-02-06 22:47:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12130/40036]	eta 1:08:04 lr 0.000016	time 0.1449 (0.1464)	loss 0.2181 (0.2180)	grad_norm 13802.8203 (15760.8125)	mem 4918MB
[2022-02-06 22:47:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12140/40036]	eta 1:08:02 lr 0.000016	time 0.1446 (0.1464)	loss 0.2183 (0.2180)	grad_norm 11579.4082 (15758.8545)	mem 4918MB
[2022-02-06 22:47:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12150/40036]	eta 1:08:01 lr 0.000016	time 0.1455 (0.1464)	loss 0.2170 (0.2180)	grad_norm 12323.1104 (15756.3389)	mem 4918MB
[2022-02-06 22:47:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12160/40036]	eta 1:07:59 lr 0.000016	time 0.1451 (0.1464)	loss 0.2155 (0.2180)	grad_norm 13159.3457 (15754.1309)	mem 4918MB
[2022-02-06 22:47:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12170/40036]	eta 1:07:58 lr 0.000016	time 0.1454 (0.1464)	loss 0.2184 (0.2180)	grad_norm 13062.6650 (15751.7070)	mem 4918MB
[2022-02-06 22:47:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12180/40036]	eta 1:07:56 lr 0.000016	time 0.1451 (0.1464)	loss 0.2177 (0.2180)	grad_norm 12721.5029 (15749.5576)	mem 4918MB
[2022-02-06 22:47:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12190/40036]	eta 1:07:55 lr 0.000016	time 0.1463 (0.1464)	loss 0.2199 (0.2180)	grad_norm 13626.9971 (15747.2197)	mem 4918MB
[2022-02-06 22:47:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12200/40036]	eta 1:07:53 lr 0.000016	time 0.1453 (0.1464)	loss 0.2185 (0.2180)	grad_norm 11726.4238 (15744.5840)	mem 4918MB
[2022-02-06 22:47:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12210/40036]	eta 1:07:52 lr 0.000016	time 0.1462 (0.1464)	loss 0.2173 (0.2180)	grad_norm 11848.6475 (15741.9717)	mem 4918MB
[2022-02-06 22:47:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12220/40036]	eta 1:07:50 lr 0.000016	time 0.1455 (0.1464)	loss 0.2172 (0.2180)	grad_norm 12200.7344 (15739.6748)	mem 4918MB
[2022-02-06 22:47:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12230/40036]	eta 1:07:49 lr 0.000016	time 0.1449 (0.1464)	loss 0.2197 (0.2180)	grad_norm 13196.9375 (15737.0615)	mem 4918MB
[2022-02-06 22:47:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12240/40036]	eta 1:07:48 lr 0.000016	time 0.1454 (0.1464)	loss 0.2167 (0.2180)	grad_norm 13623.7393 (15734.0107)	mem 4918MB
[2022-02-06 22:47:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12250/40036]	eta 1:07:46 lr 0.000016	time 0.1452 (0.1464)	loss 0.2191 (0.2180)	grad_norm 13220.3320 (15731.7549)	mem 4918MB
[2022-02-06 22:47:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12260/40036]	eta 1:07:45 lr 0.000016	time 0.1451 (0.1464)	loss 0.2194 (0.2180)	grad_norm 12684.7861 (15729.2021)	mem 4918MB
[2022-02-06 22:47:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12270/40036]	eta 1:07:43 lr 0.000016	time 0.1459 (0.1464)	loss 0.2141 (0.2180)	grad_norm 14023.7939 (15727.1445)	mem 4918MB
[2022-02-06 22:47:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12280/40036]	eta 1:07:42 lr 0.000016	time 0.1450 (0.1464)	loss 0.2164 (0.2180)	grad_norm 13211.6113 (15724.9072)	mem 4918MB
[2022-02-06 22:47:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12290/40036]	eta 1:07:40 lr 0.000016	time 0.1449 (0.1464)	loss 0.2182 (0.2180)	grad_norm 12675.9629 (15722.7637)	mem 4918MB
[2022-02-06 22:47:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12300/40036]	eta 1:07:39 lr 0.000016	time 0.1449 (0.1464)	loss 0.2193 (0.2180)	grad_norm 11514.0635 (15720.2217)	mem 4918MB
[2022-02-06 22:47:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12310/40036]	eta 1:07:37 lr 0.000016	time 0.1446 (0.1464)	loss 0.2184 (0.2180)	grad_norm 12640.2520 (15717.5527)	mem 4918MB
[2022-02-06 22:47:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12320/40036]	eta 1:07:36 lr 0.000016	time 0.1442 (0.1464)	loss 0.2187 (0.2180)	grad_norm 12035.4014 (15714.9658)	mem 4918MB
[2022-02-06 22:47:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12330/40036]	eta 1:07:34 lr 0.000016	time 0.1469 (0.1464)	loss 0.2164 (0.2180)	grad_norm 14692.2021 (15712.1943)	mem 4918MB
[2022-02-06 22:47:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12340/40036]	eta 1:07:33 lr 0.000016	time 0.1463 (0.1464)	loss 0.2174 (0.2180)	grad_norm 12528.6445 (15709.5781)	mem 4918MB
[2022-02-06 22:47:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12350/40036]	eta 1:07:31 lr 0.000016	time 0.1463 (0.1464)	loss 0.2180 (0.2180)	grad_norm 10803.5557 (15707.0098)	mem 4918MB
[2022-02-06 22:47:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12360/40036]	eta 1:07:30 lr 0.000016	time 0.1451 (0.1464)	loss 0.2163 (0.2180)	grad_norm 12088.7197 (15704.2646)	mem 4918MB
[2022-02-06 22:47:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12370/40036]	eta 1:07:28 lr 0.000016	time 0.1453 (0.1464)	loss 0.2159 (0.2180)	grad_norm 12852.3994 (15702.4648)	mem 4918MB
[2022-02-06 22:47:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12380/40036]	eta 1:07:27 lr 0.000016	time 0.1451 (0.1464)	loss 0.2208 (0.2180)	grad_norm 12918.5312 (15699.7471)	mem 4918MB
[2022-02-06 22:47:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12390/40036]	eta 1:07:26 lr 0.000016	time 0.1451 (0.1464)	loss 0.2163 (0.2180)	grad_norm 14274.3867 (15697.4990)	mem 4918MB
[2022-02-06 22:47:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12400/40036]	eta 1:07:24 lr 0.000016	time 0.1450 (0.1464)	loss 0.2189 (0.2180)	grad_norm 12131.6436 (15694.8545)	mem 4918MB
[2022-02-06 22:47:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12410/40036]	eta 1:07:23 lr 0.000016	time 0.1455 (0.1464)	loss 0.2187 (0.2180)	grad_norm 12266.0322 (15692.3320)	mem 4918MB
[2022-02-06 22:47:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12420/40036]	eta 1:07:21 lr 0.000016	time 0.1457 (0.1464)	loss 0.2176 (0.2180)	grad_norm 10871.3604 (15690.1230)	mem 4918MB
[2022-02-06 22:47:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12430/40036]	eta 1:07:20 lr 0.000016	time 0.1455 (0.1464)	loss 0.2173 (0.2180)	grad_norm 12973.5771 (15687.8867)	mem 4918MB
[2022-02-06 22:47:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12440/40036]	eta 1:07:18 lr 0.000016	time 0.1452 (0.1464)	loss 0.2168 (0.2180)	grad_norm 12178.1914 (15685.4912)	mem 4918MB
[2022-02-06 22:48:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12450/40036]	eta 1:07:17 lr 0.000017	time 0.1467 (0.1464)	loss 0.2188 (0.2180)	grad_norm 13970.2539 (15682.7090)	mem 4918MB
[2022-02-06 22:48:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12460/40036]	eta 1:07:15 lr 0.000017	time 0.1461 (0.1464)	loss 0.2169 (0.2180)	grad_norm 12599.8730 (15680.3086)	mem 4918MB
[2022-02-06 22:48:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12470/40036]	eta 1:07:14 lr 0.000017	time 0.1456 (0.1464)	loss 0.2170 (0.2180)	grad_norm 12664.8506 (15677.7197)	mem 4918MB
[2022-02-06 22:48:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12480/40036]	eta 1:07:12 lr 0.000017	time 0.1452 (0.1464)	loss 0.2189 (0.2180)	grad_norm 12515.1133 (15675.1982)	mem 4918MB
[2022-02-06 22:48:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12490/40036]	eta 1:07:11 lr 0.000017	time 0.1450 (0.1464)	loss 0.2161 (0.2180)	grad_norm 13792.2910 (15673.0654)	mem 4918MB
[2022-02-06 22:48:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12500/40036]	eta 1:07:09 lr 0.000017	time 0.1448 (0.1464)	loss 0.2174 (0.2180)	grad_norm 11584.0049 (15670.8789)	mem 4918MB
[2022-02-06 22:48:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12510/40036]	eta 1:07:08 lr 0.000017	time 0.1457 (0.1464)	loss 0.2173 (0.2180)	grad_norm 12134.6602 (15667.9287)	mem 4918MB
[2022-02-06 22:48:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12520/40036]	eta 1:07:07 lr 0.000017	time 0.1448 (0.1464)	loss 0.2157 (0.2180)	grad_norm 10593.3984 (15665.4092)	mem 4918MB
[2022-02-06 22:48:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12530/40036]	eta 1:07:05 lr 0.000017	time 0.1452 (0.1464)	loss 0.2146 (0.2180)	grad_norm 14304.9941 (15662.9082)	mem 4918MB
[2022-02-06 22:48:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12540/40036]	eta 1:07:04 lr 0.000017	time 0.1451 (0.1464)	loss 0.2187 (0.2180)	grad_norm 13047.4248 (15660.7617)	mem 4918MB
[2022-02-06 22:48:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12550/40036]	eta 1:07:02 lr 0.000017	time 0.1450 (0.1464)	loss 0.2172 (0.2180)	grad_norm 12288.6201 (15658.3223)	mem 4918MB
[2022-02-06 22:48:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12560/40036]	eta 1:07:01 lr 0.000017	time 0.1450 (0.1464)	loss 0.2165 (0.2180)	grad_norm 13347.9971 (15655.9512)	mem 4918MB
[2022-02-06 22:48:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12570/40036]	eta 1:06:59 lr 0.000017	time 0.1453 (0.1464)	loss 0.2166 (0.2180)	grad_norm 12215.6494 (15653.7109)	mem 4918MB
[2022-02-06 22:48:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12580/40036]	eta 1:06:58 lr 0.000017	time 0.1448 (0.1464)	loss 0.2177 (0.2180)	grad_norm 14139.7354 (15651.0332)	mem 4918MB
[2022-02-06 22:48:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12590/40036]	eta 1:06:56 lr 0.000017	time 0.1454 (0.1464)	loss 0.2179 (0.2180)	grad_norm 11173.8574 (15648.2715)	mem 4918MB
[2022-02-06 22:48:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12600/40036]	eta 1:06:55 lr 0.000017	time 0.1459 (0.1464)	loss 0.2155 (0.2180)	grad_norm 12248.2539 (15645.8076)	mem 4918MB
[2022-02-06 22:48:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12610/40036]	eta 1:06:53 lr 0.000017	time 0.1474 (0.1464)	loss 0.2154 (0.2180)	grad_norm 12955.1025 (15643.5469)	mem 4918MB
[2022-02-06 22:48:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12620/40036]	eta 1:06:52 lr 0.000017	time 0.1452 (0.1464)	loss 0.2189 (0.2180)	grad_norm 13436.7822 (15641.0293)	mem 4918MB
[2022-02-06 22:48:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12630/40036]	eta 1:06:50 lr 0.000017	time 0.1454 (0.1464)	loss 0.2172 (0.2180)	grad_norm 12519.2178 (15638.6094)	mem 4918MB
[2022-02-06 22:48:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12640/40036]	eta 1:06:49 lr 0.000017	time 0.1440 (0.1464)	loss 0.2165 (0.2180)	grad_norm 10881.5107 (15636.0352)	mem 4918MB
[2022-02-06 22:48:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12650/40036]	eta 1:06:48 lr 0.000017	time 0.1453 (0.1464)	loss 0.2177 (0.2180)	grad_norm 11274.8652 (15633.3984)	mem 4918MB
[2022-02-06 22:48:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12660/40036]	eta 1:06:46 lr 0.000017	time 0.1454 (0.1464)	loss 0.2189 (0.2180)	grad_norm 11158.8105 (15631.1094)	mem 4918MB
[2022-02-06 22:48:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12670/40036]	eta 1:06:45 lr 0.000017	time 0.1448 (0.1464)	loss 0.2171 (0.2180)	grad_norm 11489.8828 (15628.1963)	mem 4918MB
[2022-02-06 22:48:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12680/40036]	eta 1:06:43 lr 0.000017	time 0.1451 (0.1464)	loss 0.2209 (0.2180)	grad_norm 12567.9160 (15625.4824)	mem 4918MB
[2022-02-06 22:48:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12690/40036]	eta 1:06:42 lr 0.000017	time 0.1445 (0.1464)	loss 0.2167 (0.2180)	grad_norm 13681.6914 (15623.0029)	mem 4918MB
[2022-02-06 22:48:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12700/40036]	eta 1:06:40 lr 0.000017	time 0.1455 (0.1464)	loss 0.2160 (0.2180)	grad_norm 10455.5635 (15620.0928)	mem 4918MB
[2022-02-06 22:48:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12710/40036]	eta 1:06:39 lr 0.000017	time 0.1450 (0.1464)	loss 0.2152 (0.2180)	grad_norm 11755.8311 (15617.6738)	mem 4918MB
[2022-02-06 22:48:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12720/40036]	eta 1:06:37 lr 0.000017	time 0.1466 (0.1464)	loss 0.2203 (0.2180)	grad_norm 13311.1348 (15615.0000)	mem 4918MB
[2022-02-06 22:48:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12730/40036]	eta 1:06:36 lr 0.000017	time 0.1463 (0.1464)	loss 0.2200 (0.2180)	grad_norm 13774.8770 (15612.3838)	mem 4918MB
[2022-02-06 22:48:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12740/40036]	eta 1:06:34 lr 0.000017	time 0.1464 (0.1464)	loss 0.2159 (0.2180)	grad_norm 11252.4717 (15609.8838)	mem 4918MB
[2022-02-06 22:48:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12750/40036]	eta 1:06:33 lr 0.000017	time 0.1457 (0.1464)	loss 0.2181 (0.2180)	grad_norm 12926.5820 (15607.8008)	mem 4918MB
[2022-02-06 22:48:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12760/40036]	eta 1:06:31 lr 0.000017	time 0.1451 (0.1464)	loss 0.2178 (0.2180)	grad_norm 13366.9668 (15605.2002)	mem 4918MB
[2022-02-06 22:48:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12770/40036]	eta 1:06:30 lr 0.000017	time 0.1446 (0.1464)	loss 0.2174 (0.2180)	grad_norm 12768.3457 (15602.9775)	mem 4918MB
[2022-02-06 22:48:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12780/40036]	eta 1:06:28 lr 0.000017	time 0.1449 (0.1464)	loss 0.2183 (0.2180)	grad_norm 12681.4316 (15600.7910)	mem 4918MB
[2022-02-06 22:48:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12790/40036]	eta 1:06:27 lr 0.000017	time 0.1447 (0.1464)	loss 0.2156 (0.2180)	grad_norm 11577.9150 (15598.4082)	mem 4918MB
[2022-02-06 22:48:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12800/40036]	eta 1:06:26 lr 0.000017	time 0.1445 (0.1464)	loss 0.2161 (0.2180)	grad_norm 12062.7217 (15596.4746)	mem 4918MB
[2022-02-06 22:48:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12810/40036]	eta 1:06:24 lr 0.000017	time 0.1451 (0.1464)	loss 0.2162 (0.2180)	grad_norm 11162.8828 (15593.8408)	mem 4918MB
[2022-02-06 22:48:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12820/40036]	eta 1:06:23 lr 0.000017	time 0.1456 (0.1464)	loss 0.2172 (0.2180)	grad_norm 12695.5664 (15591.1768)	mem 4918MB
[2022-02-06 22:48:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12830/40036]	eta 1:06:21 lr 0.000017	time 0.1453 (0.1464)	loss 0.2173 (0.2180)	grad_norm 13447.7061 (15588.9229)	mem 4918MB
[2022-02-06 22:48:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12840/40036]	eta 1:06:20 lr 0.000017	time 0.1457 (0.1464)	loss 0.2187 (0.2180)	grad_norm 12793.3506 (15586.6182)	mem 4918MB
[2022-02-06 22:48:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12850/40036]	eta 1:06:18 lr 0.000017	time 0.1453 (0.1464)	loss 0.2177 (0.2180)	grad_norm 12154.5762 (15583.9590)	mem 4918MB
[2022-02-06 22:49:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12860/40036]	eta 1:06:17 lr 0.000017	time 0.1464 (0.1464)	loss 0.2190 (0.2180)	grad_norm 13340.9688 (15582.0752)	mem 4918MB
[2022-02-06 22:49:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12870/40036]	eta 1:06:15 lr 0.000017	time 0.1466 (0.1464)	loss 0.2164 (0.2180)	grad_norm 10650.1396 (15579.1846)	mem 4918MB
[2022-02-06 22:49:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12880/40036]	eta 1:06:14 lr 0.000017	time 0.1450 (0.1464)	loss 0.2193 (0.2180)	grad_norm 10359.1299 (15576.4395)	mem 4918MB
[2022-02-06 22:49:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12890/40036]	eta 1:06:12 lr 0.000017	time 0.1453 (0.1464)	loss 0.2184 (0.2180)	grad_norm 11431.7891 (15573.9736)	mem 4918MB
[2022-02-06 22:49:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12900/40036]	eta 1:06:11 lr 0.000017	time 0.1452 (0.1464)	loss 0.2157 (0.2180)	grad_norm 12352.0918 (15571.1992)	mem 4918MB
[2022-02-06 22:49:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12910/40036]	eta 1:06:09 lr 0.000017	time 0.1454 (0.1464)	loss 0.2163 (0.2180)	grad_norm 13979.7666 (15568.6846)	mem 4918MB
[2022-02-06 22:49:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12920/40036]	eta 1:06:08 lr 0.000017	time 0.1453 (0.1464)	loss 0.2167 (0.2180)	grad_norm 12353.0225 (15565.9658)	mem 4918MB
[2022-02-06 22:49:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12930/40036]	eta 1:06:07 lr 0.000017	time 0.1453 (0.1464)	loss 0.2188 (0.2180)	grad_norm 11871.3936 (15563.7266)	mem 4918MB
[2022-02-06 22:49:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12940/40036]	eta 1:06:05 lr 0.000017	time 0.1452 (0.1464)	loss 0.2176 (0.2180)	grad_norm 11768.6787 (15561.5283)	mem 4918MB
[2022-02-06 22:49:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12950/40036]	eta 1:06:04 lr 0.000017	time 0.1443 (0.1464)	loss 0.2177 (0.2180)	grad_norm 11394.1670 (15559.2744)	mem 4918MB
[2022-02-06 22:49:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12960/40036]	eta 1:06:02 lr 0.000017	time 0.1440 (0.1464)	loss 0.2176 (0.2180)	grad_norm 10610.1064 (15556.3848)	mem 4918MB
[2022-02-06 22:49:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12970/40036]	eta 1:06:01 lr 0.000017	time 0.1451 (0.1464)	loss 0.2189 (0.2180)	grad_norm 14155.4287 (15553.7305)	mem 4918MB
[2022-02-06 22:49:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12980/40036]	eta 1:05:59 lr 0.000017	time 0.1457 (0.1464)	loss 0.2145 (0.2180)	grad_norm 14878.1523 (15551.7158)	mem 4918MB
[2022-02-06 22:49:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][12990/40036]	eta 1:05:58 lr 0.000017	time 0.1461 (0.1464)	loss 0.2157 (0.2180)	grad_norm 10341.3740 (15549.9238)	mem 4918MB
[2022-02-06 22:49:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13000/40036]	eta 1:05:56 lr 0.000017	time 0.1459 (0.1464)	loss 0.2167 (0.2180)	grad_norm 13055.4717 (15547.7246)	mem 4918MB
[2022-02-06 22:49:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13010/40036]	eta 1:05:55 lr 0.000017	time 0.1452 (0.1464)	loss 0.2199 (0.2180)	grad_norm 11753.0605 (15545.2256)	mem 4918MB
[2022-02-06 22:49:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13020/40036]	eta 1:05:53 lr 0.000017	time 0.1458 (0.1464)	loss 0.2202 (0.2180)	grad_norm 12507.4375 (15542.8711)	mem 4918MB
[2022-02-06 22:49:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13030/40036]	eta 1:05:52 lr 0.000017	time 0.1457 (0.1464)	loss 0.2180 (0.2180)	grad_norm 13765.1016 (15540.4961)	mem 4918MB
[2022-02-06 22:49:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13040/40036]	eta 1:05:50 lr 0.000017	time 0.1452 (0.1464)	loss 0.2190 (0.2180)	grad_norm 12201.4619 (15537.8213)	mem 4918MB
[2022-02-06 22:49:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13050/40036]	eta 1:05:49 lr 0.000017	time 0.1456 (0.1464)	loss 0.2209 (0.2180)	grad_norm 12312.8682 (15535.3018)	mem 4918MB
[2022-02-06 22:49:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13060/40036]	eta 1:05:48 lr 0.000017	time 0.1454 (0.1464)	loss 0.2204 (0.2180)	grad_norm 14337.1631 (15533.1924)	mem 4918MB
[2022-02-06 22:49:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13070/40036]	eta 1:05:46 lr 0.000017	time 0.1454 (0.1464)	loss 0.2186 (0.2180)	grad_norm 12318.2852 (15531.0479)	mem 4918MB
[2022-02-06 22:49:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13080/40036]	eta 1:05:45 lr 0.000017	time 0.1451 (0.1464)	loss 0.2145 (0.2179)	grad_norm 12775.0879 (15528.9131)	mem 4918MB
[2022-02-06 22:49:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13090/40036]	eta 1:05:43 lr 0.000017	time 0.1446 (0.1464)	loss 0.2165 (0.2179)	grad_norm 13407.0947 (15526.6309)	mem 4918MB
[2022-02-06 22:49:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13100/40036]	eta 1:05:42 lr 0.000017	time 0.1452 (0.1464)	loss 0.2166 (0.2179)	grad_norm 13785.8594 (15524.5586)	mem 4918MB
[2022-02-06 22:49:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13110/40036]	eta 1:05:40 lr 0.000017	time 0.1456 (0.1464)	loss 0.2205 (0.2179)	grad_norm 12196.4482 (15522.7793)	mem 4918MB
[2022-02-06 22:49:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13120/40036]	eta 1:05:39 lr 0.000017	time 0.1451 (0.1464)	loss 0.2182 (0.2179)	grad_norm 11705.2412 (15520.6660)	mem 4918MB
[2022-02-06 22:49:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13130/40036]	eta 1:05:37 lr 0.000017	time 0.1457 (0.1464)	loss 0.2150 (0.2179)	grad_norm 13819.2832 (15518.2021)	mem 4918MB
[2022-02-06 22:49:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13140/40036]	eta 1:05:36 lr 0.000017	time 0.1456 (0.1464)	loss 0.2164 (0.2179)	grad_norm 14089.8457 (15515.9697)	mem 4918MB
[2022-02-06 22:49:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13150/40036]	eta 1:05:34 lr 0.000017	time 0.1456 (0.1464)	loss 0.2165 (0.2179)	grad_norm 11296.8652 (15513.1426)	mem 4918MB
[2022-02-06 22:49:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13160/40036]	eta 1:05:33 lr 0.000017	time 0.1445 (0.1464)	loss 0.2200 (0.2179)	grad_norm 12579.2988 (15510.8896)	mem 4918MB
[2022-02-06 22:49:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13170/40036]	eta 1:05:31 lr 0.000017	time 0.1453 (0.1464)	loss 0.2130 (0.2179)	grad_norm 13340.0947 (15508.6689)	mem 4918MB
[2022-02-06 22:49:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13180/40036]	eta 1:05:30 lr 0.000017	time 0.1454 (0.1464)	loss 0.2185 (0.2179)	grad_norm 10701.0488 (15505.9424)	mem 4918MB
[2022-02-06 22:49:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13190/40036]	eta 1:05:28 lr 0.000017	time 0.1454 (0.1464)	loss 0.2185 (0.2179)	grad_norm 12969.3057 (15503.8320)	mem 4918MB
[2022-02-06 22:49:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13200/40036]	eta 1:05:27 lr 0.000017	time 0.1455 (0.1464)	loss 0.2188 (0.2179)	grad_norm 12059.7139 (15501.2354)	mem 4918MB
[2022-02-06 22:49:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13210/40036]	eta 1:05:26 lr 0.000017	time 0.1450 (0.1464)	loss 0.2154 (0.2179)	grad_norm 12401.7812 (15498.7402)	mem 4918MB
[2022-02-06 22:49:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13220/40036]	eta 1:05:24 lr 0.000017	time 0.1452 (0.1464)	loss 0.2158 (0.2179)	grad_norm 12449.6924 (15496.1357)	mem 4918MB
[2022-02-06 22:49:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13230/40036]	eta 1:05:23 lr 0.000017	time 0.1461 (0.1464)	loss 0.2179 (0.2179)	grad_norm 11924.6289 (15493.9512)	mem 4918MB
[2022-02-06 22:49:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13240/40036]	eta 1:05:21 lr 0.000017	time 0.1465 (0.1464)	loss 0.2188 (0.2179)	grad_norm 13433.5039 (15491.6699)	mem 4918MB
[2022-02-06 22:49:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13250/40036]	eta 1:05:20 lr 0.000018	time 0.1461 (0.1464)	loss 0.2195 (0.2179)	grad_norm 11752.7607 (15489.4521)	mem 4918MB
[2022-02-06 22:49:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13260/40036]	eta 1:05:18 lr 0.000018	time 0.1465 (0.1464)	loss 0.2173 (0.2179)	grad_norm 11170.8145 (15486.8594)	mem 4918MB
[2022-02-06 22:50:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13270/40036]	eta 1:05:17 lr 0.000018	time 0.1456 (0.1464)	loss 0.2173 (0.2179)	grad_norm 13971.1162 (15484.6494)	mem 4918MB
[2022-02-06 22:50:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13280/40036]	eta 1:05:15 lr 0.000018	time 0.1452 (0.1464)	loss 0.2158 (0.2179)	grad_norm 11786.7041 (15482.4277)	mem 4918MB
[2022-02-06 22:50:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13290/40036]	eta 1:05:14 lr 0.000018	time 0.1454 (0.1464)	loss 0.2188 (0.2179)	grad_norm 12067.5977 (15480.2568)	mem 4918MB
[2022-02-06 22:50:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13300/40036]	eta 1:05:12 lr 0.000018	time 0.1452 (0.1464)	loss 0.2157 (0.2179)	grad_norm 12889.2461 (15477.8701)	mem 4918MB
[2022-02-06 22:50:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13310/40036]	eta 1:05:11 lr 0.000018	time 0.1448 (0.1464)	loss 0.2181 (0.2179)	grad_norm 14657.8594 (15476.0420)	mem 4918MB
[2022-02-06 22:50:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13320/40036]	eta 1:05:09 lr 0.000018	time 0.1445 (0.1464)	loss 0.2170 (0.2179)	grad_norm 10549.5840 (15473.5674)	mem 4918MB
[2022-02-06 22:50:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13330/40036]	eta 1:05:08 lr 0.000018	time 0.1451 (0.1464)	loss 0.2223 (0.2179)	grad_norm 13404.2832 (15471.1865)	mem 4918MB
[2022-02-06 22:50:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13340/40036]	eta 1:05:06 lr 0.000018	time 0.1459 (0.1464)	loss 0.2210 (0.2179)	grad_norm 12392.2852 (15468.8477)	mem 4918MB
[2022-02-06 22:50:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13350/40036]	eta 1:05:05 lr 0.000018	time 0.1452 (0.1464)	loss 0.2183 (0.2179)	grad_norm 11057.0615 (15466.2627)	mem 4918MB
[2022-02-06 22:50:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13360/40036]	eta 1:05:04 lr 0.000018	time 0.1449 (0.1464)	loss 0.2220 (0.2179)	grad_norm 13726.5488 (15463.8076)	mem 4918MB
[2022-02-06 22:50:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13370/40036]	eta 1:05:02 lr 0.000018	time 0.1459 (0.1464)	loss 0.2192 (0.2179)	grad_norm 11155.1963 (15461.2139)	mem 4918MB
[2022-02-06 22:50:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13380/40036]	eta 1:05:01 lr 0.000018	time 0.1461 (0.1464)	loss 0.2216 (0.2179)	grad_norm 11685.2383 (15458.5195)	mem 4918MB
[2022-02-06 22:50:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13390/40036]	eta 1:04:59 lr 0.000018	time 0.1459 (0.1464)	loss 0.2183 (0.2179)	grad_norm 11946.5273 (15456.0801)	mem 4918MB
[2022-02-06 22:50:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13400/40036]	eta 1:04:58 lr 0.000018	time 0.1449 (0.1464)	loss 0.2183 (0.2179)	grad_norm 13162.3301 (15454.5020)	mem 4918MB
[2022-02-06 22:50:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13410/40036]	eta 1:04:56 lr 0.000018	time 0.1453 (0.1464)	loss 0.2180 (0.2179)	grad_norm 13648.0010 (15452.6182)	mem 4918MB
[2022-02-06 22:50:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13420/40036]	eta 1:04:55 lr 0.000018	time 0.1449 (0.1464)	loss 0.2169 (0.2179)	grad_norm 11056.7637 (15450.1660)	mem 4918MB
[2022-02-06 22:50:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13430/40036]	eta 1:04:53 lr 0.000018	time 0.1447 (0.1464)	loss 0.2183 (0.2179)	grad_norm 11822.5449 (15447.4707)	mem 4918MB
[2022-02-06 22:50:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13440/40036]	eta 1:04:52 lr 0.000018	time 0.1443 (0.1464)	loss 0.2182 (0.2179)	grad_norm 12920.1699 (15445.3760)	mem 4918MB
[2022-02-06 22:50:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13450/40036]	eta 1:04:50 lr 0.000018	time 0.1453 (0.1464)	loss 0.2169 (0.2179)	grad_norm 12720.9385 (15442.7930)	mem 4918MB
[2022-02-06 22:50:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13460/40036]	eta 1:04:49 lr 0.000018	time 0.1454 (0.1464)	loss 0.2216 (0.2179)	grad_norm 12336.0273 (15440.1338)	mem 4918MB
[2022-02-06 22:50:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13470/40036]	eta 1:04:47 lr 0.000018	time 0.1457 (0.1464)	loss 0.2178 (0.2179)	grad_norm 12028.6611 (15437.5762)	mem 4918MB
[2022-02-06 22:50:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13480/40036]	eta 1:04:46 lr 0.000018	time 0.1453 (0.1464)	loss 0.2176 (0.2179)	grad_norm 12271.6123 (15435.1445)	mem 4918MB
[2022-02-06 22:50:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13490/40036]	eta 1:04:45 lr 0.000018	time 0.1447 (0.1464)	loss 0.2164 (0.2179)	grad_norm 11221.3916 (15432.4570)	mem 4918MB
[2022-02-06 22:50:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13500/40036]	eta 1:04:43 lr 0.000018	time 0.1460 (0.1464)	loss 0.2199 (0.2179)	grad_norm 12604.5293 (15430.2383)	mem 4918MB
[2022-02-06 22:50:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13510/40036]	eta 1:04:42 lr 0.000018	time 0.1471 (0.1464)	loss 0.2185 (0.2179)	grad_norm 12435.8574 (15428.2373)	mem 4918MB
[2022-02-06 22:50:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13520/40036]	eta 1:04:40 lr 0.000018	time 0.1457 (0.1464)	loss 0.2189 (0.2179)	grad_norm 14669.6748 (15426.2197)	mem 4918MB
[2022-02-06 22:50:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13530/40036]	eta 1:04:39 lr 0.000018	time 0.1447 (0.1464)	loss 0.2168 (0.2179)	grad_norm 11263.8760 (15423.8486)	mem 4918MB
[2022-02-06 22:50:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13540/40036]	eta 1:04:37 lr 0.000018	time 0.1458 (0.1464)	loss 0.2162 (0.2179)	grad_norm 11723.4180 (15421.5684)	mem 4918MB
[2022-02-06 22:50:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13550/40036]	eta 1:04:36 lr 0.000018	time 0.1458 (0.1464)	loss 0.2223 (0.2179)	grad_norm 13839.5723 (15419.3457)	mem 4918MB
[2022-02-06 22:50:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13560/40036]	eta 1:04:34 lr 0.000018	time 0.1445 (0.1464)	loss 0.2190 (0.2179)	grad_norm 10318.6279 (15416.8457)	mem 4918MB
[2022-02-06 22:50:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13570/40036]	eta 1:04:33 lr 0.000018	time 0.1448 (0.1464)	loss 0.2185 (0.2179)	grad_norm 12961.6201 (15414.6230)	mem 4918MB
[2022-02-06 22:50:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13580/40036]	eta 1:04:31 lr 0.000018	time 0.1453 (0.1464)	loss 0.2153 (0.2179)	grad_norm 10986.4727 (15412.7305)	mem 4918MB
[2022-02-06 22:50:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13590/40036]	eta 1:04:30 lr 0.000018	time 0.1453 (0.1464)	loss 0.2199 (0.2179)	grad_norm 11998.1348 (15410.4785)	mem 4918MB
[2022-02-06 22:50:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13600/40036]	eta 1:04:28 lr 0.000018	time 0.1446 (0.1464)	loss 0.2203 (0.2179)	grad_norm 14233.5625 (15408.8174)	mem 4918MB
[2022-02-06 22:50:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13610/40036]	eta 1:04:27 lr 0.000018	time 0.1451 (0.1464)	loss 0.2189 (0.2179)	grad_norm 11913.1201 (15406.3027)	mem 4918MB
[2022-02-06 22:50:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13620/40036]	eta 1:04:25 lr 0.000018	time 0.1448 (0.1464)	loss 0.2156 (0.2179)	grad_norm 12509.9258 (15404.0361)	mem 4918MB
[2022-02-06 22:50:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13630/40036]	eta 1:04:24 lr 0.000018	time 0.1458 (0.1464)	loss 0.2182 (0.2179)	grad_norm 10930.9004 (15401.1826)	mem 4918MB
[2022-02-06 22:50:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13640/40036]	eta 1:04:23 lr 0.000018	time 0.1458 (0.1464)	loss 0.2179 (0.2179)	grad_norm 12855.8242 (15398.9512)	mem 4918MB
[2022-02-06 22:50:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13650/40036]	eta 1:04:21 lr 0.000018	time 0.1462 (0.1464)	loss 0.2161 (0.2179)	grad_norm 11265.5322 (15396.7920)	mem 4918MB
[2022-02-06 22:50:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13660/40036]	eta 1:04:20 lr 0.000018	time 0.1463 (0.1464)	loss 0.2128 (0.2179)	grad_norm 13421.5820 (15394.2979)	mem 4918MB
[2022-02-06 22:50:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13670/40036]	eta 1:04:18 lr 0.000018	time 0.1449 (0.1464)	loss 0.2177 (0.2179)	grad_norm 12860.4326 (15391.5400)	mem 4918MB
[2022-02-06 22:51:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13680/40036]	eta 1:04:17 lr 0.000018	time 0.1451 (0.1464)	loss 0.2185 (0.2179)	grad_norm 10891.5615 (15388.8896)	mem 4918MB
[2022-02-06 22:51:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13690/40036]	eta 1:04:15 lr 0.000018	time 0.1445 (0.1464)	loss 0.2201 (0.2179)	grad_norm 12145.2090 (15386.6475)	mem 4918MB
[2022-02-06 22:51:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13700/40036]	eta 1:04:14 lr 0.000018	time 0.1448 (0.1464)	loss 0.2182 (0.2179)	grad_norm 13122.2832 (15384.8574)	mem 4918MB
[2022-02-06 22:51:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13710/40036]	eta 1:04:12 lr 0.000018	time 0.1448 (0.1464)	loss 0.2177 (0.2179)	grad_norm 12694.8066 (15382.4941)	mem 4918MB
[2022-02-06 22:51:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13720/40036]	eta 1:04:11 lr 0.000018	time 0.1454 (0.1464)	loss 0.2175 (0.2179)	grad_norm 11700.1934 (15380.1592)	mem 4918MB
[2022-02-06 22:51:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13730/40036]	eta 1:04:09 lr 0.000018	time 0.1451 (0.1464)	loss 0.2193 (0.2179)	grad_norm 11393.6992 (15377.7715)	mem 4918MB
[2022-02-06 22:51:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13740/40036]	eta 1:04:08 lr 0.000018	time 0.1451 (0.1464)	loss 0.2164 (0.2179)	grad_norm 13089.3037 (15375.1191)	mem 4918MB
[2022-02-06 22:51:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13750/40036]	eta 1:04:06 lr 0.000018	time 0.1448 (0.1464)	loss 0.2178 (0.2179)	grad_norm 12037.7510 (15372.5186)	mem 4918MB
[2022-02-06 22:51:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13760/40036]	eta 1:04:05 lr 0.000018	time 0.1446 (0.1464)	loss 0.2184 (0.2179)	grad_norm 11496.9805 (15369.8389)	mem 4918MB
[2022-02-06 22:51:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13770/40036]	eta 1:04:04 lr 0.000018	time 0.1457 (0.1464)	loss 0.2185 (0.2179)	grad_norm 11466.5723 (15367.3770)	mem 4918MB
[2022-02-06 22:51:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13780/40036]	eta 1:04:02 lr 0.000018	time 0.1452 (0.1464)	loss 0.2180 (0.2179)	grad_norm 13138.0469 (15365.1953)	mem 4918MB
[2022-02-06 22:51:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13790/40036]	eta 1:04:01 lr 0.000018	time 0.1447 (0.1464)	loss 0.2169 (0.2179)	grad_norm 11163.5742 (15362.8145)	mem 4918MB
[2022-02-06 22:51:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13800/40036]	eta 1:03:59 lr 0.000018	time 0.1448 (0.1464)	loss 0.2164 (0.2179)	grad_norm 11124.2061 (15360.3965)	mem 4918MB
[2022-02-06 22:51:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13810/40036]	eta 1:03:58 lr 0.000018	time 0.1448 (0.1464)	loss 0.2179 (0.2179)	grad_norm 11530.9062 (15358.1689)	mem 4918MB
[2022-02-06 22:51:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13820/40036]	eta 1:03:56 lr 0.000018	time 0.1456 (0.1464)	loss 0.2202 (0.2179)	grad_norm 13639.0020 (15356.0469)	mem 4918MB
[2022-02-06 22:51:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13830/40036]	eta 1:03:55 lr 0.000018	time 0.1462 (0.1464)	loss 0.2174 (0.2179)	grad_norm 11921.0010 (15353.5527)	mem 4918MB
[2022-02-06 22:51:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13840/40036]	eta 1:03:53 lr 0.000018	time 0.1455 (0.1464)	loss 0.2186 (0.2179)	grad_norm 13299.0137 (15351.0684)	mem 4918MB
[2022-02-06 22:51:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13850/40036]	eta 1:03:52 lr 0.000018	time 0.1450 (0.1464)	loss 0.2180 (0.2179)	grad_norm 11964.5342 (15348.7422)	mem 4918MB
[2022-02-06 22:51:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13860/40036]	eta 1:03:50 lr 0.000018	time 0.1452 (0.1464)	loss 0.2182 (0.2179)	grad_norm 13211.8975 (15346.5703)	mem 4918MB
[2022-02-06 22:51:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13870/40036]	eta 1:03:49 lr 0.000018	time 0.1456 (0.1464)	loss 0.2152 (0.2179)	grad_norm 12550.0156 (15343.9180)	mem 4918MB
[2022-02-06 22:51:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13880/40036]	eta 1:03:47 lr 0.000018	time 0.1457 (0.1464)	loss 0.2179 (0.2179)	grad_norm 14013.1826 (15341.6016)	mem 4918MB
[2022-02-06 22:51:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13890/40036]	eta 1:03:46 lr 0.000018	time 0.1458 (0.1464)	loss 0.2160 (0.2179)	grad_norm 11427.5967 (15339.3564)	mem 4918MB
[2022-02-06 22:51:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13900/40036]	eta 1:03:45 lr 0.000018	time 0.1447 (0.1464)	loss 0.2169 (0.2179)	grad_norm 9890.6543 (15336.5273)	mem 4918MB
[2022-02-06 22:51:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13910/40036]	eta 1:03:43 lr 0.000018	time 0.1451 (0.1464)	loss 0.2178 (0.2179)	grad_norm 11856.3564 (15334.0410)	mem 4918MB
[2022-02-06 22:51:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13920/40036]	eta 1:03:42 lr 0.000018	time 0.1449 (0.1464)	loss 0.2182 (0.2179)	grad_norm 11950.8545 (15331.6309)	mem 4918MB
[2022-02-06 22:51:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13930/40036]	eta 1:03:40 lr 0.000018	time 0.1453 (0.1464)	loss 0.2175 (0.2179)	grad_norm 13458.0703 (15329.3887)	mem 4918MB
[2022-02-06 22:51:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13940/40036]	eta 1:03:39 lr 0.000018	time 0.1456 (0.1464)	loss 0.2158 (0.2179)	grad_norm 12849.3252 (15326.7344)	mem 4918MB
[2022-02-06 22:51:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13950/40036]	eta 1:03:37 lr 0.000018	time 0.1464 (0.1464)	loss 0.2166 (0.2179)	grad_norm 11311.7383 (15324.4453)	mem 4918MB
[2022-02-06 22:51:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13960/40036]	eta 1:03:36 lr 0.000018	time 0.1455 (0.1464)	loss 0.2152 (0.2179)	grad_norm 11531.0703 (15321.9912)	mem 4918MB
[2022-02-06 22:51:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13970/40036]	eta 1:03:34 lr 0.000018	time 0.1459 (0.1464)	loss 0.2174 (0.2179)	grad_norm 9919.8984 (15319.5098)	mem 4918MB
[2022-02-06 22:51:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13980/40036]	eta 1:03:33 lr 0.000018	time 0.1454 (0.1464)	loss 0.2188 (0.2179)	grad_norm 10452.2471 (15316.8613)	mem 4918MB
[2022-02-06 22:51:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][13990/40036]	eta 1:03:31 lr 0.000018	time 0.1450 (0.1464)	loss 0.2169 (0.2179)	grad_norm 11240.5811 (15313.8662)	mem 4918MB
[2022-02-06 22:51:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14000/40036]	eta 1:03:30 lr 0.000018	time 0.1458 (0.1464)	loss 0.2161 (0.2179)	grad_norm 12450.3271 (15311.2129)	mem 4918MB
[2022-02-06 22:51:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14010/40036]	eta 1:03:28 lr 0.000018	time 0.1447 (0.1464)	loss 0.2186 (0.2179)	grad_norm 12924.0850 (15308.7314)	mem 4918MB
[2022-02-06 22:51:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14020/40036]	eta 1:03:27 lr 0.000018	time 0.1453 (0.1464)	loss 0.2145 (0.2179)	grad_norm 12473.9570 (15306.5762)	mem 4918MB
[2022-02-06 22:51:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14030/40036]	eta 1:03:25 lr 0.000018	time 0.1458 (0.1464)	loss 0.2182 (0.2179)	grad_norm 12155.1006 (15303.8496)	mem 4918MB
[2022-02-06 22:51:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14040/40036]	eta 1:03:24 lr 0.000018	time 0.1455 (0.1464)	loss 0.2172 (0.2179)	grad_norm 12551.2852 (15301.2314)	mem 4918MB
[2022-02-06 22:51:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14050/40036]	eta 1:03:23 lr 0.000019	time 0.1457 (0.1464)	loss 0.2174 (0.2179)	grad_norm 11584.2734 (15298.4092)	mem 4918MB
[2022-02-06 22:51:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14060/40036]	eta 1:03:21 lr 0.000019	time 0.1440 (0.1464)	loss 0.2151 (0.2179)	grad_norm 10099.9844 (15295.9834)	mem 4918MB
[2022-02-06 22:51:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14070/40036]	eta 1:03:20 lr 0.000019	time 0.1446 (0.1464)	loss 0.2155 (0.2179)	grad_norm 11453.8291 (15293.1709)	mem 4918MB
[2022-02-06 22:51:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14080/40036]	eta 1:03:18 lr 0.000019	time 0.1450 (0.1464)	loss 0.2166 (0.2179)	grad_norm 11004.7051 (15290.2480)	mem 4918MB
[2022-02-06 22:52:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14090/40036]	eta 1:03:17 lr 0.000019	time 0.1458 (0.1464)	loss 0.2178 (0.2179)	grad_norm 11494.0107 (15287.4072)	mem 4918MB
[2022-02-06 22:52:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14100/40036]	eta 1:03:15 lr 0.000019	time 0.1457 (0.1464)	loss 0.2176 (0.2179)	grad_norm 12986.8652 (15284.8076)	mem 4918MB
[2022-02-06 22:52:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14110/40036]	eta 1:03:14 lr 0.000019	time 0.1466 (0.1463)	loss 0.2171 (0.2179)	grad_norm 12443.7549 (15282.5420)	mem 4918MB
[2022-02-06 22:52:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14120/40036]	eta 1:03:12 lr 0.000019	time 0.1448 (0.1464)	loss 0.2180 (0.2179)	grad_norm 12816.8027 (15280.2529)	mem 4918MB
[2022-02-06 22:52:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14130/40036]	eta 1:03:11 lr 0.000019	time 0.1458 (0.1464)	loss 0.2174 (0.2179)	grad_norm 11806.5918 (15277.5332)	mem 4918MB
[2022-02-06 22:52:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14140/40036]	eta 1:03:09 lr 0.000019	time 0.1456 (0.1464)	loss 0.2190 (0.2179)	grad_norm 11328.4092 (15274.9199)	mem 4918MB
[2022-02-06 22:52:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14150/40036]	eta 1:03:08 lr 0.000019	time 0.1457 (0.1464)	loss 0.2192 (0.2179)	grad_norm 13738.1982 (15272.3916)	mem 4918MB
[2022-02-06 22:52:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14160/40036]	eta 1:03:06 lr 0.000019	time 0.1454 (0.1464)	loss 0.2165 (0.2179)	grad_norm 12751.1846 (15269.7090)	mem 4918MB
[2022-02-06 22:52:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14170/40036]	eta 1:03:05 lr 0.000019	time 0.1453 (0.1464)	loss 0.2164 (0.2179)	grad_norm 13655.5977 (15267.4307)	mem 4918MB
[2022-02-06 22:52:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14180/40036]	eta 1:03:04 lr 0.000019	time 0.1460 (0.1464)	loss 0.2199 (0.2179)	grad_norm 13634.4316 (15265.0889)	mem 4918MB
[2022-02-06 22:52:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14190/40036]	eta 1:03:02 lr 0.000019	time 0.1455 (0.1464)	loss 0.2165 (0.2179)	grad_norm 11071.3174 (15262.6807)	mem 4918MB
[2022-02-06 22:52:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14200/40036]	eta 1:03:01 lr 0.000019	time 0.1474 (0.1464)	loss 0.2186 (0.2179)	grad_norm 11203.5020 (15259.7510)	mem 4918MB
[2022-02-06 22:52:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14210/40036]	eta 1:02:59 lr 0.000019	time 0.1447 (0.1464)	loss 0.2161 (0.2179)	grad_norm 11434.0977 (15257.7764)	mem 4918MB
[2022-02-06 22:52:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14220/40036]	eta 1:02:58 lr 0.000019	time 0.1462 (0.1464)	loss 0.2172 (0.2179)	grad_norm 11003.4023 (15255.7764)	mem 4918MB
[2022-02-06 22:52:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14230/40036]	eta 1:02:56 lr 0.000019	time 0.1462 (0.1464)	loss 0.2167 (0.2179)	grad_norm 11102.2197 (15253.5811)	mem 4918MB
[2022-02-06 22:52:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14240/40036]	eta 1:02:55 lr 0.000019	time 0.1450 (0.1464)	loss 0.2170 (0.2179)	grad_norm 10259.3691 (15251.0361)	mem 4918MB
[2022-02-06 22:52:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14250/40036]	eta 1:02:53 lr 0.000019	time 0.1449 (0.1464)	loss 0.2190 (0.2179)	grad_norm 11034.7432 (15248.1426)	mem 4918MB
[2022-02-06 22:52:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14260/40036]	eta 1:02:52 lr 0.000019	time 0.1452 (0.1464)	loss 0.2174 (0.2179)	grad_norm 10601.7188 (15245.5771)	mem 4918MB
[2022-02-06 22:52:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14270/40036]	eta 1:02:50 lr 0.000019	time 0.1455 (0.1463)	loss 0.2168 (0.2179)	grad_norm 10264.8086 (15243.3340)	mem 4918MB
[2022-02-06 22:52:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14280/40036]	eta 1:02:49 lr 0.000019	time 0.1451 (0.1464)	loss 0.2192 (0.2179)	grad_norm 11990.8027 (15241.2305)	mem 4918MB
[2022-02-06 22:52:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14290/40036]	eta 1:02:47 lr 0.000019	time 0.1452 (0.1464)	loss 0.2212 (0.2179)	grad_norm 10962.0361 (15238.7139)	mem 4918MB
[2022-02-06 22:52:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14300/40036]	eta 1:02:46 lr 0.000019	time 0.1450 (0.1464)	loss 0.2183 (0.2179)	grad_norm 11827.9951 (15236.1025)	mem 4918MB
[2022-02-06 22:52:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14310/40036]	eta 1:02:45 lr 0.000019	time 0.1450 (0.1464)	loss 0.2179 (0.2179)	grad_norm 12887.7148 (15233.7393)	mem 4918MB
[2022-02-06 22:52:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14320/40036]	eta 1:02:43 lr 0.000019	time 0.1448 (0.1464)	loss 0.2164 (0.2179)	grad_norm 10188.5977 (15230.8672)	mem 4918MB
[2022-02-06 22:52:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14330/40036]	eta 1:02:42 lr 0.000019	time 0.1455 (0.1464)	loss 0.2195 (0.2179)	grad_norm 9789.1875 (15228.0400)	mem 4918MB
[2022-02-06 22:52:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14340/40036]	eta 1:02:40 lr 0.000019	time 0.1454 (0.1464)	loss 0.2177 (0.2179)	grad_norm 10072.6689 (15225.3730)	mem 4918MB
[2022-02-06 22:52:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14350/40036]	eta 1:02:39 lr 0.000019	time 0.1456 (0.1464)	loss 0.2151 (0.2179)	grad_norm 9764.0605 (15222.5654)	mem 4918MB
[2022-02-06 22:52:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14360/40036]	eta 1:02:37 lr 0.000019	time 0.1460 (0.1464)	loss 0.2177 (0.2179)	grad_norm 10136.0205 (15220.4150)	mem 4918MB
[2022-02-06 22:52:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14370/40036]	eta 1:02:36 lr 0.000019	time 0.1461 (0.1464)	loss 0.2163 (0.2179)	grad_norm 10893.3398 (15217.9199)	mem 4918MB
[2022-02-06 22:52:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14380/40036]	eta 1:02:34 lr 0.000019	time 0.1451 (0.1464)	loss 0.2159 (0.2179)	grad_norm 11664.3936 (15215.2461)	mem 4918MB
[2022-02-06 22:52:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14390/40036]	eta 1:02:33 lr 0.000019	time 0.1449 (0.1464)	loss 0.2165 (0.2179)	grad_norm 9843.8135 (15212.8486)	mem 4918MB
[2022-02-06 22:52:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14400/40036]	eta 1:02:31 lr 0.000019	time 0.1446 (0.1464)	loss 0.2156 (0.2179)	grad_norm 10869.3350 (15210.4170)	mem 4918MB
[2022-02-06 22:52:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14410/40036]	eta 1:02:30 lr 0.000019	time 0.1453 (0.1464)	loss 0.2166 (0.2179)	grad_norm 11694.8838 (15207.7119)	mem 4918MB
[2022-02-06 22:52:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14420/40036]	eta 1:02:28 lr 0.000019	time 0.1453 (0.1464)	loss 0.2168 (0.2179)	grad_norm 10135.7158 (15205.4814)	mem 4918MB
[2022-02-06 22:52:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14430/40036]	eta 1:02:27 lr 0.000019	time 0.1454 (0.1464)	loss 0.2184 (0.2179)	grad_norm 12090.9199 (15202.7744)	mem 4918MB
[2022-02-06 22:52:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14440/40036]	eta 1:02:26 lr 0.000019	time 0.1454 (0.1464)	loss 0.2169 (0.2179)	grad_norm 13254.8154 (15200.4385)	mem 4918MB
[2022-02-06 22:52:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14450/40036]	eta 1:02:24 lr 0.000019	time 0.1448 (0.1464)	loss 0.2154 (0.2179)	grad_norm 10087.1426 (15197.9678)	mem 4918MB
[2022-02-06 22:52:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14460/40036]	eta 1:02:23 lr 0.000019	time 0.1462 (0.1464)	loss 0.2168 (0.2179)	grad_norm 11857.9844 (15195.6318)	mem 4918MB
[2022-02-06 22:52:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14470/40036]	eta 1:02:21 lr 0.000019	time 0.1459 (0.1464)	loss 0.2181 (0.2179)	grad_norm 11348.2461 (15193.0762)	mem 4918MB
[2022-02-06 22:52:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14480/40036]	eta 1:02:20 lr 0.000019	time 0.1457 (0.1464)	loss 0.2217 (0.2179)	grad_norm 12468.3428 (15190.6221)	mem 4918MB
[2022-02-06 22:52:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14490/40036]	eta 1:02:18 lr 0.000019	time 0.1464 (0.1464)	loss 0.2174 (0.2179)	grad_norm 10030.5928 (15187.9854)	mem 4918MB
[2022-02-06 22:53:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14500/40036]	eta 1:02:17 lr 0.000019	time 0.1460 (0.1464)	loss 0.2174 (0.2179)	grad_norm 11874.8369 (15185.4932)	mem 4918MB
[2022-02-06 22:53:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14510/40036]	eta 1:02:15 lr 0.000019	time 0.1462 (0.1464)	loss 0.2183 (0.2179)	grad_norm 11912.0615 (15182.8789)	mem 4918MB
[2022-02-06 22:53:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14520/40036]	eta 1:02:14 lr 0.000019	time 0.1451 (0.1464)	loss 0.2190 (0.2179)	grad_norm 11193.4863 (15180.3594)	mem 4918MB
[2022-02-06 22:53:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14530/40036]	eta 1:02:12 lr 0.000019	time 0.1453 (0.1464)	loss 0.2147 (0.2179)	grad_norm 11570.4414 (15177.8750)	mem 4918MB
[2022-02-06 22:53:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14540/40036]	eta 1:02:11 lr 0.000019	time 0.1445 (0.1464)	loss 0.2175 (0.2179)	grad_norm 10585.4189 (15175.4004)	mem 4918MB
[2022-02-06 22:53:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14550/40036]	eta 1:02:09 lr 0.000019	time 0.1455 (0.1464)	loss 0.2180 (0.2179)	grad_norm 11432.0107 (15172.8184)	mem 4918MB
[2022-02-06 22:53:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14560/40036]	eta 1:02:08 lr 0.000019	time 0.1472 (0.1464)	loss 0.2190 (0.2179)	grad_norm 11000.1816 (15170.2119)	mem 4918MB
[2022-02-06 22:53:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14570/40036]	eta 1:02:06 lr 0.000019	time 0.1452 (0.1464)	loss 0.2159 (0.2179)	grad_norm 12087.1221 (15167.8877)	mem 4918MB
[2022-02-06 22:53:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14580/40036]	eta 1:02:05 lr 0.000019	time 0.1450 (0.1464)	loss 0.2155 (0.2179)	grad_norm 11548.6328 (15165.7373)	mem 4918MB
[2022-02-06 22:53:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14590/40036]	eta 1:02:04 lr 0.000019	time 0.1449 (0.1464)	loss 0.2196 (0.2179)	grad_norm 11705.1875 (15163.3262)	mem 4918MB
[2022-02-06 22:53:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14600/40036]	eta 1:02:02 lr 0.000019	time 0.1453 (0.1464)	loss 0.2187 (0.2179)	grad_norm 12765.5645 (15160.9385)	mem 4918MB
[2022-02-06 22:53:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14610/40036]	eta 1:02:01 lr 0.000019	time 0.1460 (0.1464)	loss 0.2175 (0.2179)	grad_norm 12012.4219 (15158.4111)	mem 4918MB
[2022-02-06 22:53:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14620/40036]	eta 1:01:59 lr 0.000019	time 0.1458 (0.1464)	loss 0.2198 (0.2179)	grad_norm 11023.1689 (15155.9082)	mem 4918MB
[2022-02-06 22:53:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14630/40036]	eta 1:01:58 lr 0.000019	time 0.1455 (0.1464)	loss 0.2163 (0.2179)	grad_norm 12173.0264 (15153.7686)	mem 4918MB
[2022-02-06 22:53:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14640/40036]	eta 1:01:56 lr 0.000019	time 0.1447 (0.1464)	loss 0.2147 (0.2179)	grad_norm 12946.4600 (15151.4746)	mem 4918MB
[2022-02-06 22:53:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14650/40036]	eta 1:01:55 lr 0.000019	time 0.1448 (0.1463)	loss 0.2163 (0.2179)	grad_norm 10292.3359 (15148.8047)	mem 4918MB
[2022-02-06 22:53:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14660/40036]	eta 1:01:53 lr 0.000019	time 0.1446 (0.1464)	loss 0.2185 (0.2179)	grad_norm 11277.6553 (15146.1621)	mem 4918MB
[2022-02-06 22:53:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14670/40036]	eta 1:01:52 lr 0.000019	time 0.1447 (0.1464)	loss 0.2173 (0.2179)	grad_norm 10941.7500 (15143.9258)	mem 4918MB
[2022-02-06 22:53:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14680/40036]	eta 1:01:50 lr 0.000019	time 0.1449 (0.1463)	loss 0.2178 (0.2179)	grad_norm 10384.5195 (15141.5293)	mem 4918MB
[2022-02-06 22:53:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14690/40036]	eta 1:01:49 lr 0.000019	time 0.1454 (0.1464)	loss 0.2155 (0.2179)	grad_norm 12079.7061 (15139.0811)	mem 4918MB
[2022-02-06 22:53:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14700/40036]	eta 1:01:47 lr 0.000019	time 0.1444 (0.1464)	loss 0.2178 (0.2179)	grad_norm 10314.9600 (15137.1641)	mem 4918MB
[2022-02-06 22:53:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14710/40036]	eta 1:01:46 lr 0.000019	time 0.1452 (0.1463)	loss 0.2175 (0.2179)	grad_norm 12892.0996 (15134.9160)	mem 4918MB
[2022-02-06 22:53:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14720/40036]	eta 1:01:45 lr 0.000019	time 0.1440 (0.1464)	loss 0.2173 (0.2179)	grad_norm 13159.3867 (15132.6709)	mem 4918MB
[2022-02-06 22:53:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14730/40036]	eta 1:01:43 lr 0.000019	time 0.1461 (0.1463)	loss 0.2189 (0.2179)	grad_norm 12418.6426 (15130.5078)	mem 4918MB
[2022-02-06 22:53:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14740/40036]	eta 1:01:42 lr 0.000019	time 0.1465 (0.1463)	loss 0.2170 (0.2179)	grad_norm 10851.8018 (15128.0586)	mem 4918MB
[2022-02-06 22:53:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14750/40036]	eta 1:01:40 lr 0.000019	time 0.1454 (0.1463)	loss 0.2174 (0.2179)	grad_norm 10404.0801 (15125.7314)	mem 4918MB
[2022-02-06 22:53:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14760/40036]	eta 1:01:39 lr 0.000019	time 0.1465 (0.1464)	loss 0.2153 (0.2179)	grad_norm 13023.7920 (15123.6543)	mem 4918MB
[2022-02-06 22:53:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14770/40036]	eta 1:01:37 lr 0.000019	time 0.1450 (0.1463)	loss 0.2163 (0.2179)	grad_norm 13193.2783 (15121.4473)	mem 4918MB
[2022-02-06 22:53:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14780/40036]	eta 1:01:36 lr 0.000019	time 0.1449 (0.1463)	loss 0.2205 (0.2179)	grad_norm 14206.6084 (15119.5625)	mem 4918MB
[2022-02-06 22:53:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14790/40036]	eta 1:01:34 lr 0.000019	time 0.1448 (0.1463)	loss 0.2140 (0.2179)	grad_norm 10961.1660 (15116.9697)	mem 4918MB
[2022-02-06 22:53:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14800/40036]	eta 1:01:33 lr 0.000019	time 0.1454 (0.1463)	loss 0.2160 (0.2179)	grad_norm 10032.0635 (15114.1084)	mem 4918MB
[2022-02-06 22:53:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14810/40036]	eta 1:01:31 lr 0.000019	time 0.1454 (0.1463)	loss 0.2182 (0.2179)	grad_norm 12331.7441 (15111.6074)	mem 4918MB
[2022-02-06 22:53:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14820/40036]	eta 1:01:30 lr 0.000019	time 0.1450 (0.1463)	loss 0.2182 (0.2179)	grad_norm 12531.3115 (15109.2695)	mem 4918MB
[2022-02-06 22:53:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14830/40036]	eta 1:01:28 lr 0.000019	time 0.1446 (0.1463)	loss 0.2203 (0.2179)	grad_norm 13019.6689 (15107.1455)	mem 4918MB
[2022-02-06 22:53:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14840/40036]	eta 1:01:27 lr 0.000019	time 0.1452 (0.1463)	loss 0.2160 (0.2179)	grad_norm 11669.8623 (15104.6562)	mem 4918MB
[2022-02-06 22:53:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14850/40036]	eta 1:01:25 lr 0.000020	time 0.1450 (0.1463)	loss 0.2160 (0.2179)	grad_norm 12380.4004 (15102.1279)	mem 4918MB
[2022-02-06 22:53:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14860/40036]	eta 1:01:24 lr 0.000020	time 0.1460 (0.1463)	loss 0.2173 (0.2179)	grad_norm 10742.3418 (15099.7432)	mem 4918MB
[2022-02-06 22:53:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14870/40036]	eta 1:01:23 lr 0.000020	time 0.1459 (0.1463)	loss 0.2179 (0.2179)	grad_norm 10508.6035 (15097.3965)	mem 4918MB
[2022-02-06 22:53:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14880/40036]	eta 1:01:21 lr 0.000020	time 0.1459 (0.1463)	loss 0.2188 (0.2179)	grad_norm 11870.4482 (15094.8604)	mem 4918MB
[2022-02-06 22:53:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14890/40036]	eta 1:01:20 lr 0.000020	time 0.1455 (0.1463)	loss 0.2184 (0.2179)	grad_norm 13684.7949 (15092.4531)	mem 4918MB
[2022-02-06 22:53:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14900/40036]	eta 1:01:18 lr 0.000020	time 0.1446 (0.1463)	loss 0.2146 (0.2179)	grad_norm 11445.3496 (15089.8457)	mem 4918MB
[2022-02-06 22:54:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14910/40036]	eta 1:01:17 lr 0.000020	time 0.1456 (0.1463)	loss 0.2173 (0.2179)	grad_norm 11471.3330 (15087.3213)	mem 4918MB
[2022-02-06 22:54:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14920/40036]	eta 1:01:15 lr 0.000020	time 0.1459 (0.1463)	loss 0.2174 (0.2179)	grad_norm 11624.0850 (15084.8379)	mem 4918MB
[2022-02-06 22:54:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14930/40036]	eta 1:01:14 lr 0.000020	time 0.1453 (0.1463)	loss 0.2195 (0.2179)	grad_norm 12839.2490 (15082.6865)	mem 4918MB
[2022-02-06 22:54:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14940/40036]	eta 1:01:12 lr 0.000020	time 0.1450 (0.1463)	loss 0.2167 (0.2179)	grad_norm 10838.9316 (15080.3125)	mem 4918MB
[2022-02-06 22:54:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14950/40036]	eta 1:01:11 lr 0.000020	time 0.1453 (0.1463)	loss 0.2167 (0.2179)	grad_norm 10439.4150 (15077.7275)	mem 4918MB
[2022-02-06 22:54:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14960/40036]	eta 1:01:09 lr 0.000020	time 0.1458 (0.1463)	loss 0.2194 (0.2179)	grad_norm 10924.2598 (15075.4541)	mem 4918MB
[2022-02-06 22:54:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14970/40036]	eta 1:01:08 lr 0.000020	time 0.1446 (0.1463)	loss 0.2175 (0.2179)	grad_norm 11577.0039 (15073.2197)	mem 4918MB
[2022-02-06 22:54:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14980/40036]	eta 1:01:06 lr 0.000020	time 0.1448 (0.1463)	loss 0.2189 (0.2179)	grad_norm 9652.5869 (15070.7988)	mem 4918MB
[2022-02-06 22:54:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][14990/40036]	eta 1:01:05 lr 0.000020	time 0.1455 (0.1463)	loss 0.2177 (0.2179)	grad_norm 9659.7402 (15068.0508)	mem 4918MB
[2022-02-06 22:54:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15000/40036]	eta 1:01:03 lr 0.000020	time 0.1461 (0.1463)	loss 0.2176 (0.2179)	grad_norm 11591.6768 (15065.3369)	mem 4918MB
[2022-02-06 22:54:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15010/40036]	eta 1:01:02 lr 0.000020	time 0.1452 (0.1463)	loss 0.2189 (0.2179)	grad_norm 9398.5518 (15062.9502)	mem 4918MB
[2022-02-06 22:54:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15020/40036]	eta 1:01:01 lr 0.000020	time 0.1464 (0.1463)	loss 0.2152 (0.2179)	grad_norm 10075.6787 (15060.5986)	mem 4918MB
[2022-02-06 22:54:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15030/40036]	eta 1:00:59 lr 0.000020	time 0.1451 (0.1463)	loss 0.2159 (0.2179)	grad_norm 10233.5840 (15058.2783)	mem 4918MB
[2022-02-06 22:54:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15040/40036]	eta 1:00:58 lr 0.000020	time 0.1442 (0.1463)	loss 0.2184 (0.2179)	grad_norm 12085.5723 (15056.0918)	mem 4918MB
[2022-02-06 22:54:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15050/40036]	eta 1:00:56 lr 0.000020	time 0.1445 (0.1463)	loss 0.2163 (0.2179)	grad_norm 12180.2119 (15053.6055)	mem 4918MB
[2022-02-06 22:54:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15060/40036]	eta 1:00:55 lr 0.000020	time 0.1445 (0.1463)	loss 0.2174 (0.2179)	grad_norm 12577.5186 (15051.4062)	mem 4918MB
[2022-02-06 22:54:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15070/40036]	eta 1:00:53 lr 0.000020	time 0.1455 (0.1463)	loss 0.2185 (0.2179)	grad_norm 12866.0322 (15049.2520)	mem 4918MB
[2022-02-06 22:54:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15080/40036]	eta 1:00:52 lr 0.000020	time 0.1455 (0.1463)	loss 0.2193 (0.2179)	grad_norm 11607.8408 (15046.9307)	mem 4918MB
[2022-02-06 22:54:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15090/40036]	eta 1:00:50 lr 0.000020	time 0.1454 (0.1463)	loss 0.2208 (0.2179)	grad_norm 12808.3213 (15044.7246)	mem 4918MB
[2022-02-06 22:54:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15100/40036]	eta 1:00:49 lr 0.000020	time 0.1455 (0.1463)	loss 0.2199 (0.2179)	grad_norm 11499.3008 (15042.3359)	mem 4918MB
[2022-02-06 22:54:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15110/40036]	eta 1:00:47 lr 0.000020	time 0.1450 (0.1463)	loss 0.2153 (0.2179)	grad_norm 10799.9707 (15039.8584)	mem 4918MB
[2022-02-06 22:54:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15120/40036]	eta 1:00:46 lr 0.000020	time 0.1447 (0.1463)	loss 0.2183 (0.2179)	grad_norm 13567.7031 (15037.5527)	mem 4918MB
[2022-02-06 22:54:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15130/40036]	eta 1:00:44 lr 0.000020	time 0.1468 (0.1463)	loss 0.2151 (0.2179)	grad_norm 9880.7188 (15035.1562)	mem 4918MB
[2022-02-06 22:54:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15140/40036]	eta 1:00:43 lr 0.000020	time 0.1459 (0.1463)	loss 0.2166 (0.2179)	grad_norm 13145.5049 (15032.8652)	mem 4918MB
[2022-02-06 22:54:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15150/40036]	eta 1:00:42 lr 0.000020	time 0.1455 (0.1463)	loss 0.2189 (0.2179)	grad_norm 12843.1016 (15030.4551)	mem 4918MB
[2022-02-06 22:54:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15160/40036]	eta 1:00:40 lr 0.000020	time 0.1451 (0.1463)	loss 0.2160 (0.2179)	grad_norm 10369.3291 (15028.1338)	mem 4918MB
[2022-02-06 22:54:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15170/40036]	eta 1:00:39 lr 0.000020	time 0.1451 (0.1463)	loss 0.2180 (0.2179)	grad_norm 11425.9092 (15025.6602)	mem 4918MB
[2022-02-06 22:54:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15180/40036]	eta 1:00:37 lr 0.000020	time 0.1449 (0.1463)	loss 0.2134 (0.2179)	grad_norm 11127.4893 (15022.9746)	mem 4918MB
[2022-02-06 22:54:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15190/40036]	eta 1:00:36 lr 0.000020	time 0.1452 (0.1463)	loss 0.2196 (0.2179)	grad_norm 10532.0059 (15020.5664)	mem 4918MB
[2022-02-06 22:54:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15200/40036]	eta 1:00:34 lr 0.000020	time 0.1445 (0.1463)	loss 0.2143 (0.2179)	grad_norm 11429.7764 (15018.1377)	mem 4918MB
[2022-02-06 22:54:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15210/40036]	eta 1:00:33 lr 0.000020	time 0.1451 (0.1463)	loss 0.2160 (0.2179)	grad_norm 10976.9463 (15015.5049)	mem 4918MB
[2022-02-06 22:54:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15220/40036]	eta 1:00:31 lr 0.000020	time 0.1451 (0.1463)	loss 0.2193 (0.2179)	grad_norm 11125.0459 (15012.9746)	mem 4918MB
[2022-02-06 22:54:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15230/40036]	eta 1:00:30 lr 0.000020	time 0.1454 (0.1463)	loss 0.2172 (0.2179)	grad_norm 11020.9062 (15010.5166)	mem 4918MB
[2022-02-06 22:54:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15240/40036]	eta 1:00:28 lr 0.000020	time 0.1450 (0.1463)	loss 0.2200 (0.2179)	grad_norm 11484.8340 (15008.2812)	mem 4918MB
[2022-02-06 22:54:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15250/40036]	eta 1:00:27 lr 0.000020	time 0.1449 (0.1463)	loss 0.2203 (0.2179)	grad_norm 11952.4004 (15005.5684)	mem 4918MB
[2022-02-06 22:54:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15260/40036]	eta 1:00:25 lr 0.000020	time 0.1461 (0.1463)	loss 0.2170 (0.2179)	grad_norm 10809.3955 (15003.2832)	mem 4918MB
[2022-02-06 22:54:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15270/40036]	eta 1:00:24 lr 0.000020	time 0.1464 (0.1463)	loss 0.2175 (0.2179)	grad_norm 11458.5879 (15000.5918)	mem 4918MB
[2022-02-06 22:54:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15280/40036]	eta 1:00:22 lr 0.000020	time 0.1459 (0.1463)	loss 0.2191 (0.2179)	grad_norm 12412.5635 (14997.9072)	mem 4918MB
[2022-02-06 22:54:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15290/40036]	eta 1:00:21 lr 0.000020	time 0.1448 (0.1463)	loss 0.2159 (0.2179)	grad_norm 10247.3057 (14995.5332)	mem 4918MB
[2022-02-06 22:54:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15300/40036]	eta 1:00:20 lr 0.000020	time 0.1448 (0.1463)	loss 0.2177 (0.2179)	grad_norm 11081.3701 (14992.9805)	mem 4918MB
[2022-02-06 22:54:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15310/40036]	eta 1:00:18 lr 0.000020	time 0.1450 (0.1463)	loss 0.2182 (0.2179)	grad_norm 11143.1260 (14990.9473)	mem 4918MB
[2022-02-06 22:55:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15320/40036]	eta 1:00:17 lr 0.000020	time 0.1440 (0.1463)	loss 0.2188 (0.2179)	grad_norm 11138.4941 (14988.5498)	mem 4918MB
[2022-02-06 22:55:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15330/40036]	eta 1:00:15 lr 0.000020	time 0.1450 (0.1463)	loss 0.2186 (0.2179)	grad_norm 12281.9395 (14986.1621)	mem 4918MB
[2022-02-06 22:55:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15340/40036]	eta 1:00:14 lr 0.000020	time 0.1447 (0.1463)	loss 0.2186 (0.2179)	grad_norm 11338.6201 (14983.6904)	mem 4918MB
[2022-02-06 22:55:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15350/40036]	eta 1:00:12 lr 0.000020	time 0.1446 (0.1463)	loss 0.2178 (0.2179)	grad_norm 11220.4229 (14981.5664)	mem 4918MB
[2022-02-06 22:55:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15360/40036]	eta 1:00:11 lr 0.000020	time 0.1446 (0.1463)	loss 0.2167 (0.2179)	grad_norm 10375.0977 (14979.0244)	mem 4918MB
[2022-02-06 22:55:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15370/40036]	eta 1:00:09 lr 0.000020	time 0.1450 (0.1463)	loss 0.2156 (0.2179)	grad_norm 10999.2881 (14976.4277)	mem 4918MB
[2022-02-06 22:55:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15380/40036]	eta 1:00:08 lr 0.000020	time 0.1458 (0.1463)	loss 0.2177 (0.2179)	grad_norm 11117.6465 (14974.1523)	mem 4918MB
[2022-02-06 22:55:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15390/40036]	eta 1:00:06 lr 0.000020	time 0.1456 (0.1463)	loss 0.2167 (0.2179)	grad_norm 12776.9727 (14971.7949)	mem 4918MB
[2022-02-06 22:55:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15400/40036]	eta 1:00:05 lr 0.000020	time 0.1464 (0.1463)	loss 0.2151 (0.2179)	grad_norm 11825.2871 (14969.3594)	mem 4918MB
[2022-02-06 22:55:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15410/40036]	eta 1:00:03 lr 0.000020	time 0.1461 (0.1463)	loss 0.2162 (0.2179)	grad_norm 12310.6514 (14967.2578)	mem 4918MB
[2022-02-06 22:55:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15420/40036]	eta 1:00:02 lr 0.000020	time 0.1445 (0.1463)	loss 0.2174 (0.2179)	grad_norm 10153.0703 (14964.8477)	mem 4918MB
[2022-02-06 22:55:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15430/40036]	eta 1:00:01 lr 0.000020	time 0.1451 (0.1463)	loss 0.2179 (0.2179)	grad_norm 11581.7061 (14962.6123)	mem 4918MB
[2022-02-06 22:55:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15440/40036]	eta 0:59:59 lr 0.000020	time 0.1452 (0.1463)	loss 0.2193 (0.2179)	grad_norm 10399.8154 (14960.0215)	mem 4918MB
[2022-02-06 22:55:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15450/40036]	eta 0:59:58 lr 0.000020	time 0.1447 (0.1463)	loss 0.2178 (0.2179)	grad_norm 9536.5312 (14957.4023)	mem 4918MB
[2022-02-06 22:55:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15460/40036]	eta 0:59:56 lr 0.000020	time 0.1450 (0.1463)	loss 0.2174 (0.2179)	grad_norm 13255.5107 (14955.2168)	mem 4918MB
[2022-02-06 22:55:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15470/40036]	eta 0:59:55 lr 0.000020	time 0.1449 (0.1463)	loss 0.2165 (0.2179)	grad_norm 11908.7412 (14952.8193)	mem 4918MB
[2022-02-06 22:55:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15480/40036]	eta 0:59:53 lr 0.000020	time 0.1454 (0.1463)	loss 0.2176 (0.2179)	grad_norm 10629.0908 (14950.7012)	mem 4918MB
[2022-02-06 22:55:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15490/40036]	eta 0:59:52 lr 0.000020	time 0.1450 (0.1463)	loss 0.2167 (0.2179)	grad_norm 11009.9355 (14948.3984)	mem 4918MB
[2022-02-06 22:55:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15500/40036]	eta 0:59:50 lr 0.000020	time 0.1446 (0.1463)	loss 0.2179 (0.2179)	grad_norm 12864.9238 (14946.0459)	mem 4918MB
[2022-02-06 22:55:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15510/40036]	eta 0:59:49 lr 0.000020	time 0.1457 (0.1463)	loss 0.2156 (0.2179)	grad_norm 9551.6875 (14943.3027)	mem 4918MB
[2022-02-06 22:55:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15520/40036]	eta 0:59:47 lr 0.000020	time 0.1446 (0.1463)	loss 0.2164 (0.2179)	grad_norm 10268.8730 (14940.7861)	mem 4918MB
[2022-02-06 22:55:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15530/40036]	eta 0:59:46 lr 0.000020	time 0.1469 (0.1463)	loss 0.2176 (0.2179)	grad_norm 13274.2334 (14938.5400)	mem 4918MB
[2022-02-06 22:55:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15540/40036]	eta 0:59:44 lr 0.000020	time 0.1454 (0.1463)	loss 0.2190 (0.2179)	grad_norm 9626.1465 (14936.1279)	mem 4918MB
[2022-02-06 22:55:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15550/40036]	eta 0:59:43 lr 0.000020	time 0.1447 (0.1463)	loss 0.2153 (0.2179)	grad_norm 11887.9014 (14933.4883)	mem 4918MB
[2022-02-06 22:55:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15560/40036]	eta 0:59:42 lr 0.000020	time 0.1449 (0.1463)	loss 0.2181 (0.2179)	grad_norm 10543.9609 (14931.1270)	mem 4918MB
[2022-02-06 22:55:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15570/40036]	eta 0:59:40 lr 0.000020	time 0.1450 (0.1463)	loss 0.2185 (0.2179)	grad_norm 11218.0029 (14928.6240)	mem 4918MB
[2022-02-06 22:55:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15580/40036]	eta 0:59:39 lr 0.000020	time 0.1455 (0.1463)	loss 0.2171 (0.2179)	grad_norm 12717.0986 (14926.4600)	mem 4918MB
[2022-02-06 22:55:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15590/40036]	eta 0:59:37 lr 0.000020	time 0.1454 (0.1463)	loss 0.2185 (0.2179)	grad_norm 11870.2930 (14924.0029)	mem 4918MB
[2022-02-06 22:55:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15600/40036]	eta 0:59:36 lr 0.000020	time 0.1445 (0.1463)	loss 0.2163 (0.2179)	grad_norm 11321.1123 (14921.6738)	mem 4918MB
[2022-02-06 22:55:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15610/40036]	eta 0:59:34 lr 0.000020	time 0.1448 (0.1463)	loss 0.2206 (0.2179)	grad_norm 12341.3750 (14919.5273)	mem 4918MB
[2022-02-06 22:55:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15620/40036]	eta 0:59:33 lr 0.000020	time 0.1450 (0.1463)	loss 0.2177 (0.2179)	grad_norm 11121.4238 (14917.1074)	mem 4918MB
[2022-02-06 22:55:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15630/40036]	eta 0:59:31 lr 0.000020	time 0.1449 (0.1463)	loss 0.2169 (0.2179)	grad_norm 12079.8701 (14914.8613)	mem 4918MB
[2022-02-06 22:55:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15640/40036]	eta 0:59:30 lr 0.000020	time 0.1453 (0.1463)	loss 0.2172 (0.2179)	grad_norm 10111.0039 (14912.3584)	mem 4918MB
[2022-02-06 22:55:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15650/40036]	eta 0:59:28 lr 0.000021	time 0.1463 (0.1463)	loss 0.2173 (0.2179)	grad_norm 11463.4180 (14910.2188)	mem 4918MB
[2022-02-06 22:55:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15660/40036]	eta 0:59:27 lr 0.000021	time 0.1460 (0.1463)	loss 0.2166 (0.2179)	grad_norm 10502.4072 (14907.4873)	mem 4918MB
[2022-02-06 22:55:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15670/40036]	eta 0:59:25 lr 0.000021	time 0.1450 (0.1463)	loss 0.2190 (0.2179)	grad_norm 8913.5537 (14905.1934)	mem 4918MB
[2022-02-06 22:55:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15680/40036]	eta 0:59:24 lr 0.000021	time 0.1443 (0.1463)	loss 0.2193 (0.2179)	grad_norm 12250.0713 (14902.7012)	mem 4918MB
[2022-02-06 22:55:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15690/40036]	eta 0:59:22 lr 0.000021	time 0.1459 (0.1463)	loss 0.2145 (0.2179)	grad_norm 10749.0664 (14900.0889)	mem 4918MB
[2022-02-06 22:55:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15700/40036]	eta 0:59:21 lr 0.000021	time 0.1449 (0.1463)	loss 0.2158 (0.2179)	grad_norm 11111.6084 (14897.5059)	mem 4918MB
[2022-02-06 22:55:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15710/40036]	eta 0:59:20 lr 0.000021	time 0.1450 (0.1463)	loss 0.2191 (0.2179)	grad_norm 9200.4570 (14894.9365)	mem 4918MB
[2022-02-06 22:55:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15720/40036]	eta 0:59:18 lr 0.000021	time 0.1447 (0.1463)	loss 0.2190 (0.2179)	grad_norm 11557.8389 (14892.5576)	mem 4918MB
[2022-02-06 22:56:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15730/40036]	eta 0:59:17 lr 0.000021	time 0.1455 (0.1463)	loss 0.2203 (0.2179)	grad_norm 10788.6533 (14890.2070)	mem 4918MB
[2022-02-06 22:56:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15740/40036]	eta 0:59:15 lr 0.000021	time 0.1455 (0.1463)	loss 0.2169 (0.2179)	grad_norm 11715.0293 (14887.9561)	mem 4918MB
[2022-02-06 22:56:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15750/40036]	eta 0:59:14 lr 0.000021	time 0.1445 (0.1463)	loss 0.2151 (0.2179)	grad_norm 10659.4092 (14885.5088)	mem 4918MB
[2022-02-06 22:56:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15760/40036]	eta 0:59:12 lr 0.000021	time 0.1450 (0.1463)	loss 0.2204 (0.2179)	grad_norm 9666.3057 (14882.7031)	mem 4918MB
[2022-02-06 22:56:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15770/40036]	eta 0:59:11 lr 0.000021	time 0.1463 (0.1463)	loss 0.2172 (0.2179)	grad_norm 11221.6641 (14880.5488)	mem 4918MB
[2022-02-06 22:56:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15780/40036]	eta 0:59:09 lr 0.000021	time 0.1455 (0.1463)	loss 0.2186 (0.2179)	grad_norm 10716.1094 (14878.1318)	mem 4918MB
[2022-02-06 22:56:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15790/40036]	eta 0:59:08 lr 0.000021	time 0.1467 (0.1463)	loss 0.2161 (0.2179)	grad_norm 11819.7988 (14875.8057)	mem 4918MB
[2022-02-06 22:56:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15800/40036]	eta 0:59:06 lr 0.000021	time 0.1465 (0.1463)	loss 0.2182 (0.2179)	grad_norm 10290.6182 (14873.2236)	mem 4918MB
[2022-02-06 22:56:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15810/40036]	eta 0:59:05 lr 0.000021	time 0.1462 (0.1463)	loss 0.2195 (0.2179)	grad_norm 12740.3506 (14870.6729)	mem 4918MB
[2022-02-06 22:56:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15820/40036]	eta 0:59:03 lr 0.000021	time 0.1448 (0.1463)	loss 0.2180 (0.2179)	grad_norm 10653.9004 (14868.3818)	mem 4918MB
[2022-02-06 22:56:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15830/40036]	eta 0:59:02 lr 0.000021	time 0.1453 (0.1463)	loss 0.2193 (0.2179)	grad_norm 10712.0801 (14866.0605)	mem 4918MB
[2022-02-06 22:56:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15840/40036]	eta 0:59:01 lr 0.000021	time 0.1463 (0.1463)	loss 0.2183 (0.2179)	grad_norm 10139.9688 (14863.4717)	mem 4918MB
[2022-02-06 22:56:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15850/40036]	eta 0:58:59 lr 0.000021	time 0.1464 (0.1464)	loss 0.2164 (0.2179)	grad_norm 12127.6074 (14861.0898)	mem 4918MB
[2022-02-06 22:56:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15860/40036]	eta 0:58:58 lr 0.000021	time 0.1457 (0.1464)	loss 0.2192 (0.2179)	grad_norm 11372.1797 (14858.7666)	mem 4918MB
[2022-02-06 22:56:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15870/40036]	eta 0:58:56 lr 0.000021	time 0.1464 (0.1464)	loss 0.2150 (0.2179)	grad_norm 9029.4941 (14855.8594)	mem 4918MB
[2022-02-06 22:56:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15880/40036]	eta 0:58:55 lr 0.000021	time 0.1454 (0.1464)	loss 0.2155 (0.2179)	grad_norm 11495.9268 (14853.1943)	mem 4918MB
[2022-02-06 22:56:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15890/40036]	eta 0:58:53 lr 0.000021	time 0.1474 (0.1464)	loss 0.2173 (0.2179)	grad_norm 10177.7109 (14850.6211)	mem 4918MB
[2022-02-06 22:56:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15900/40036]	eta 0:58:52 lr 0.000021	time 0.1458 (0.1464)	loss 0.2150 (0.2179)	grad_norm 11042.4404 (14848.4307)	mem 4918MB
[2022-02-06 22:56:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15910/40036]	eta 0:58:50 lr 0.000021	time 0.1456 (0.1464)	loss 0.2185 (0.2179)	grad_norm 12116.4541 (14846.0254)	mem 4918MB
[2022-02-06 22:56:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15920/40036]	eta 0:58:49 lr 0.000021	time 0.1455 (0.1464)	loss 0.2187 (0.2179)	grad_norm 12694.7285 (14843.7734)	mem 4918MB
[2022-02-06 22:56:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15930/40036]	eta 0:58:48 lr 0.000021	time 0.1473 (0.1464)	loss 0.2212 (0.2179)	grad_norm 11970.5596 (14841.7920)	mem 4918MB
[2022-02-06 22:56:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15940/40036]	eta 0:58:46 lr 0.000021	time 0.1467 (0.1464)	loss 0.2201 (0.2179)	grad_norm 12588.7178 (14839.3730)	mem 4918MB
[2022-02-06 22:56:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15950/40036]	eta 0:58:45 lr 0.000021	time 0.1442 (0.1464)	loss 0.2168 (0.2179)	grad_norm 11335.6299 (14836.8828)	mem 4918MB
[2022-02-06 22:56:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15960/40036]	eta 0:58:43 lr 0.000021	time 0.1457 (0.1464)	loss 0.2197 (0.2179)	grad_norm 10931.5273 (14834.2568)	mem 4918MB
[2022-02-06 22:56:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15970/40036]	eta 0:58:42 lr 0.000021	time 0.1455 (0.1464)	loss 0.2157 (0.2179)	grad_norm 9981.0186 (14831.7432)	mem 4918MB
[2022-02-06 22:56:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15980/40036]	eta 0:58:40 lr 0.000021	time 0.1459 (0.1464)	loss 0.2176 (0.2179)	grad_norm 10318.8916 (14829.2236)	mem 4918MB
[2022-02-06 22:56:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][15990/40036]	eta 0:58:39 lr 0.000021	time 0.1450 (0.1464)	loss 0.2188 (0.2179)	grad_norm 10948.8750 (14827.2002)	mem 4918MB
[2022-02-06 22:56:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16000/40036]	eta 0:58:37 lr 0.000021	time 0.1444 (0.1464)	loss 0.2170 (0.2179)	grad_norm 9763.5420 (14824.5410)	mem 4918MB
[2022-02-06 22:56:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16010/40036]	eta 0:58:36 lr 0.000021	time 0.1467 (0.1464)	loss 0.2172 (0.2179)	grad_norm 13343.0469 (14822.3799)	mem 4918MB
[2022-02-06 22:56:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16020/40036]	eta 0:58:34 lr 0.000021	time 0.1454 (0.1464)	loss 0.2184 (0.2179)	grad_norm 10384.1592 (14820.0303)	mem 4918MB
[2022-02-06 22:56:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16030/40036]	eta 0:58:33 lr 0.000021	time 0.1454 (0.1464)	loss 0.2173 (0.2179)	grad_norm 10817.4541 (14817.7676)	mem 4918MB
[2022-02-06 22:56:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16040/40036]	eta 0:58:32 lr 0.000021	time 0.1462 (0.1464)	loss 0.2174 (0.2179)	grad_norm 10851.5791 (14815.2324)	mem 4918MB
[2022-02-06 22:56:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16050/40036]	eta 0:58:30 lr 0.000021	time 0.1455 (0.1464)	loss 0.2179 (0.2179)	grad_norm 11484.0205 (14812.5439)	mem 4918MB
[2022-02-06 22:56:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16060/40036]	eta 0:58:29 lr 0.000021	time 0.1481 (0.1464)	loss 0.2160 (0.2179)	grad_norm 10257.4707 (14810.5029)	mem 4918MB
[2022-02-06 22:56:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16070/40036]	eta 0:58:27 lr 0.000021	time 0.1625 (0.1464)	loss 0.2170 (0.2179)	grad_norm 9579.1484 (14808.2012)	mem 4918MB
[2022-02-06 22:56:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16080/40036]	eta 0:58:26 lr 0.000021	time 0.1455 (0.1464)	loss 0.2184 (0.2179)	grad_norm 10941.9160 (14805.3740)	mem 4918MB
[2022-02-06 22:56:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16090/40036]	eta 0:58:24 lr 0.000021	time 0.1540 (0.1464)	loss 0.2183 (0.2179)	grad_norm 9639.8594 (14802.8887)	mem 4918MB
[2022-02-06 22:56:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16100/40036]	eta 0:58:23 lr 0.000021	time 0.1480 (0.1464)	loss 0.2165 (0.2179)	grad_norm 11878.2598 (14800.5957)	mem 4918MB
[2022-02-06 22:56:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16110/40036]	eta 0:58:21 lr 0.000021	time 0.1460 (0.1464)	loss 0.2187 (0.2179)	grad_norm 9272.7168 (14797.8506)	mem 4918MB
[2022-02-06 22:56:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16120/40036]	eta 0:58:20 lr 0.000021	time 0.1461 (0.1464)	loss 0.2142 (0.2179)	grad_norm 10685.7637 (14795.3857)	mem 4918MB
[2022-02-06 22:56:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16130/40036]	eta 0:58:19 lr 0.000021	time 0.1458 (0.1464)	loss 0.2162 (0.2179)	grad_norm 9547.4902 (14793.0361)	mem 4918MB
[2022-02-06 22:57:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16140/40036]	eta 0:58:17 lr 0.000021	time 0.1451 (0.1464)	loss 0.2180 (0.2179)	grad_norm 10234.1250 (14790.3926)	mem 4918MB
[2022-02-06 22:57:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16150/40036]	eta 0:58:16 lr 0.000021	time 0.1449 (0.1464)	loss 0.2168 (0.2179)	grad_norm 11743.4902 (14788.1494)	mem 4918MB
[2022-02-06 22:57:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16160/40036]	eta 0:58:14 lr 0.000021	time 0.1451 (0.1464)	loss 0.2192 (0.2179)	grad_norm 10636.5801 (14785.4932)	mem 4918MB
[2022-02-06 22:57:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16170/40036]	eta 0:58:13 lr 0.000021	time 0.1450 (0.1464)	loss 0.2192 (0.2179)	grad_norm 11137.7910 (14783.1035)	mem 4918MB
[2022-02-06 22:57:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16180/40036]	eta 0:58:11 lr 0.000021	time 0.1458 (0.1464)	loss 0.2162 (0.2179)	grad_norm 11984.9355 (14780.7061)	mem 4918MB
[2022-02-06 22:57:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16190/40036]	eta 0:58:10 lr 0.000021	time 0.1452 (0.1464)	loss 0.2212 (0.2179)	grad_norm 9640.5430 (14778.2461)	mem 4918MB
[2022-02-06 22:57:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16200/40036]	eta 0:58:08 lr 0.000021	time 0.1451 (0.1464)	loss 0.2161 (0.2179)	grad_norm 11459.2871 (14775.4580)	mem 4918MB
[2022-02-06 22:57:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16210/40036]	eta 0:58:07 lr 0.000021	time 0.1451 (0.1464)	loss 0.2140 (0.2179)	grad_norm 9313.1875 (14773.1211)	mem 4918MB
[2022-02-06 22:57:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16220/40036]	eta 0:58:06 lr 0.000021	time 0.1470 (0.1464)	loss 0.2193 (0.2179)	grad_norm 11113.8457 (14770.9697)	mem 4918MB
[2022-02-06 22:57:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16230/40036]	eta 0:58:04 lr 0.000021	time 0.1450 (0.1464)	loss 0.2185 (0.2179)	grad_norm 9813.0840 (14768.4521)	mem 4918MB
[2022-02-06 22:57:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16240/40036]	eta 0:58:03 lr 0.000021	time 0.1568 (0.1464)	loss 0.2167 (0.2179)	grad_norm 10157.4902 (14766.2627)	mem 4918MB
[2022-02-06 22:57:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16250/40036]	eta 0:58:01 lr 0.000021	time 0.1546 (0.1464)	loss 0.2129 (0.2179)	grad_norm 10488.8887 (14763.9580)	mem 4918MB
[2022-02-06 22:57:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16260/40036]	eta 0:58:00 lr 0.000021	time 0.1649 (0.1464)	loss 0.2197 (0.2179)	grad_norm 11004.7031 (14761.5449)	mem 4918MB
[2022-02-06 22:57:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16270/40036]	eta 0:57:59 lr 0.000021	time 0.1485 (0.1464)	loss 0.2177 (0.2179)	grad_norm 11264.4375 (14759.0664)	mem 4918MB
[2022-02-06 22:57:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16280/40036]	eta 0:57:57 lr 0.000021	time 0.1449 (0.1464)	loss 0.2150 (0.2179)	grad_norm 11068.4092 (14756.9092)	mem 4918MB
[2022-02-06 22:57:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16290/40036]	eta 0:57:56 lr 0.000021	time 0.1579 (0.1464)	loss 0.2140 (0.2179)	grad_norm 11240.5664 (14754.4521)	mem 4918MB
[2022-02-06 22:57:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16300/40036]	eta 0:57:54 lr 0.000021	time 0.1453 (0.1464)	loss 0.2156 (0.2179)	grad_norm 9716.3018 (14752.0869)	mem 4918MB
[2022-02-06 22:57:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16310/40036]	eta 0:57:53 lr 0.000021	time 0.1456 (0.1464)	loss 0.2178 (0.2179)	grad_norm 11473.5801 (14749.7314)	mem 4918MB
[2022-02-06 22:57:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16320/40036]	eta 0:57:51 lr 0.000021	time 0.1490 (0.1464)	loss 0.2173 (0.2179)	grad_norm 11269.9932 (14747.2520)	mem 4918MB
[2022-02-06 22:57:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16330/40036]	eta 0:57:50 lr 0.000021	time 0.1484 (0.1464)	loss 0.2177 (0.2179)	grad_norm 9548.1426 (14744.8203)	mem 4918MB
[2022-02-06 22:57:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16340/40036]	eta 0:57:49 lr 0.000021	time 0.1449 (0.1464)	loss 0.2168 (0.2179)	grad_norm 10109.2256 (14742.3379)	mem 4918MB
[2022-02-06 22:57:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16350/40036]	eta 0:57:47 lr 0.000021	time 0.1458 (0.1464)	loss 0.2188 (0.2179)	grad_norm 11557.3428 (14740.1719)	mem 4918MB
[2022-02-06 22:57:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16360/40036]	eta 0:57:46 lr 0.000021	time 0.1457 (0.1464)	loss 0.2188 (0.2179)	grad_norm 9603.9492 (14737.7969)	mem 4918MB
[2022-02-06 22:57:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16370/40036]	eta 0:57:44 lr 0.000021	time 0.1531 (0.1464)	loss 0.2195 (0.2179)	grad_norm 10947.9795 (14735.3965)	mem 4918MB
[2022-02-06 22:57:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16380/40036]	eta 0:57:43 lr 0.000021	time 0.1460 (0.1464)	loss 0.2170 (0.2179)	grad_norm 12391.8545 (14733.0146)	mem 4918MB
[2022-02-06 22:57:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16390/40036]	eta 0:57:41 lr 0.000021	time 0.1459 (0.1464)	loss 0.2155 (0.2179)	grad_norm 9015.0674 (14730.7080)	mem 4918MB
[2022-02-06 22:57:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16400/40036]	eta 0:57:40 lr 0.000021	time 0.1496 (0.1464)	loss 0.2154 (0.2179)	grad_norm 10971.0400 (14728.4297)	mem 4918MB
[2022-02-06 22:57:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16410/40036]	eta 0:57:38 lr 0.000021	time 0.1455 (0.1464)	loss 0.2183 (0.2179)	grad_norm 9339.3340 (14725.6553)	mem 4918MB
[2022-02-06 22:57:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16420/40036]	eta 0:57:37 lr 0.000021	time 0.1498 (0.1464)	loss 0.2179 (0.2179)	grad_norm 9485.0371 (14723.3975)	mem 4918MB
[2022-02-06 22:57:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16430/40036]	eta 0:57:36 lr 0.000021	time 0.1460 (0.1464)	loss 0.2173 (0.2179)	grad_norm 10183.4873 (14720.8770)	mem 4918MB
[2022-02-06 22:57:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16440/40036]	eta 0:57:34 lr 0.000021	time 0.1460 (0.1464)	loss 0.2165 (0.2179)	grad_norm 11007.4961 (14718.5059)	mem 4918MB
[2022-02-06 22:57:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16450/40036]	eta 0:57:33 lr 0.000022	time 0.1458 (0.1464)	loss 0.2192 (0.2179)	grad_norm 9424.2412 (14716.0488)	mem 4918MB
[2022-02-06 22:57:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16460/40036]	eta 0:57:31 lr 0.000022	time 0.1458 (0.1464)	loss 0.2142 (0.2179)	grad_norm 11096.5137 (14713.6943)	mem 4918MB
[2022-02-06 22:57:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16470/40036]	eta 0:57:30 lr 0.000022	time 0.1457 (0.1464)	loss 0.2166 (0.2179)	grad_norm 10663.1982 (14711.2812)	mem 4918MB
[2022-02-06 22:57:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16480/40036]	eta 0:57:28 lr 0.000022	time 0.1444 (0.1464)	loss 0.2133 (0.2178)	grad_norm 10536.0410 (14708.7266)	mem 4918MB
[2022-02-06 22:57:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16490/40036]	eta 0:57:27 lr 0.000022	time 0.1443 (0.1464)	loss 0.2157 (0.2178)	grad_norm 10632.3135 (14706.3887)	mem 4918MB
[2022-02-06 22:57:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16500/40036]	eta 0:57:25 lr 0.000022	time 0.1457 (0.1464)	loss 0.2172 (0.2178)	grad_norm 10899.0420 (14704.1055)	mem 4918MB
[2022-02-06 22:57:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16510/40036]	eta 0:57:24 lr 0.000022	time 0.1457 (0.1464)	loss 0.2160 (0.2178)	grad_norm 9498.9922 (14701.6572)	mem 4918MB
[2022-02-06 22:57:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16520/40036]	eta 0:57:22 lr 0.000022	time 0.1466 (0.1464)	loss 0.2153 (0.2178)	grad_norm 10129.7656 (14699.2842)	mem 4918MB
[2022-02-06 22:57:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16530/40036]	eta 0:57:21 lr 0.000022	time 0.1497 (0.1464)	loss 0.2180 (0.2178)	grad_norm 10696.6182 (14696.6836)	mem 4918MB
[2022-02-06 22:58:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16540/40036]	eta 0:57:20 lr 0.000022	time 0.1512 (0.1464)	loss 0.2186 (0.2178)	grad_norm 11430.1934 (14694.5029)	mem 4918MB
[2022-02-06 22:58:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16550/40036]	eta 0:57:18 lr 0.000022	time 0.1542 (0.1464)	loss 0.2164 (0.2178)	grad_norm 10039.7344 (14692.1660)	mem 4918MB
[2022-02-06 22:58:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16560/40036]	eta 0:57:17 lr 0.000022	time 0.1475 (0.1464)	loss 0.2156 (0.2178)	grad_norm 10070.2373 (14690.0391)	mem 4918MB
[2022-02-06 22:58:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16570/40036]	eta 0:57:15 lr 0.000022	time 0.1451 (0.1464)	loss 0.2155 (0.2178)	grad_norm 9963.2461 (14687.6719)	mem 4918MB
[2022-02-06 22:58:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16580/40036]	eta 0:57:14 lr 0.000022	time 0.1473 (0.1464)	loss 0.2170 (0.2178)	grad_norm 8401.2949 (14685.0518)	mem 4918MB
[2022-02-06 22:58:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16590/40036]	eta 0:57:13 lr 0.000022	time 0.1527 (0.1464)	loss 0.2163 (0.2178)	grad_norm 11901.5586 (14682.8672)	mem 4918MB
[2022-02-06 22:58:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16600/40036]	eta 0:57:11 lr 0.000022	time 0.1480 (0.1464)	loss 0.2166 (0.2178)	grad_norm 11519.3672 (14680.5693)	mem 4918MB
[2022-02-06 22:58:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16610/40036]	eta 0:57:10 lr 0.000022	time 0.1512 (0.1464)	loss 0.2191 (0.2178)	grad_norm 10896.6025 (14678.2520)	mem 4918MB
[2022-02-06 22:58:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16620/40036]	eta 0:57:08 lr 0.000022	time 0.1468 (0.1464)	loss 0.2172 (0.2178)	grad_norm 10689.0684 (14675.9043)	mem 4918MB
[2022-02-06 22:58:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16630/40036]	eta 0:57:07 lr 0.000022	time 0.1458 (0.1464)	loss 0.2160 (0.2178)	grad_norm 11416.0537 (14673.4844)	mem 4918MB
[2022-02-06 22:58:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16640/40036]	eta 0:57:05 lr 0.000022	time 0.1458 (0.1464)	loss 0.2172 (0.2178)	grad_norm 9957.6719 (14670.9746)	mem 4918MB
[2022-02-06 22:58:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16650/40036]	eta 0:57:04 lr 0.000022	time 0.1508 (0.1464)	loss 0.2159 (0.2178)	grad_norm 11202.2402 (14668.5693)	mem 4918MB
[2022-02-06 22:58:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16660/40036]	eta 0:57:03 lr 0.000022	time 0.1484 (0.1464)	loss 0.2194 (0.2178)	grad_norm 11517.4727 (14666.1914)	mem 4918MB
[2022-02-06 22:58:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16670/40036]	eta 0:57:01 lr 0.000022	time 0.1484 (0.1464)	loss 0.2188 (0.2178)	grad_norm 10510.5762 (14663.8877)	mem 4918MB
[2022-02-06 22:58:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16680/40036]	eta 0:57:00 lr 0.000022	time 0.1461 (0.1464)	loss 0.2152 (0.2178)	grad_norm 11424.8799 (14661.7031)	mem 4918MB
[2022-02-06 22:58:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16690/40036]	eta 0:56:58 lr 0.000022	time 0.1491 (0.1464)	loss 0.2186 (0.2178)	grad_norm 12019.0605 (14659.5078)	mem 4918MB
[2022-02-06 22:58:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16700/40036]	eta 0:56:57 lr 0.000022	time 0.1497 (0.1464)	loss 0.2166 (0.2178)	grad_norm 9895.9531 (14656.9189)	mem 4918MB
[2022-02-06 22:58:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16710/40036]	eta 0:56:55 lr 0.000022	time 0.1524 (0.1464)	loss 0.2154 (0.2178)	grad_norm 10168.1328 (14654.5020)	mem 4918MB
[2022-02-06 22:58:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16720/40036]	eta 0:56:54 lr 0.000022	time 0.1456 (0.1464)	loss 0.2201 (0.2178)	grad_norm 10882.8750 (14652.2617)	mem 4918MB
[2022-02-06 22:58:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16730/40036]	eta 0:56:53 lr 0.000022	time 0.1464 (0.1464)	loss 0.2188 (0.2178)	grad_norm 10670.9502 (14649.8799)	mem 4918MB
[2022-02-06 22:58:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16740/40036]	eta 0:56:51 lr 0.000022	time 0.1492 (0.1464)	loss 0.2163 (0.2178)	grad_norm 10410.9189 (14647.4297)	mem 4918MB
[2022-02-06 22:58:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16750/40036]	eta 0:56:50 lr 0.000022	time 0.1458 (0.1464)	loss 0.2160 (0.2178)	grad_norm 9746.9209 (14644.7920)	mem 4918MB
[2022-02-06 22:58:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16760/40036]	eta 0:56:48 lr 0.000022	time 0.1465 (0.1464)	loss 0.2194 (0.2178)	grad_norm 10788.1689 (14642.1768)	mem 4918MB
[2022-02-06 22:58:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16770/40036]	eta 0:56:47 lr 0.000022	time 0.1462 (0.1464)	loss 0.2188 (0.2178)	grad_norm 11430.8623 (14639.6807)	mem 4918MB
[2022-02-06 22:58:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16780/40036]	eta 0:56:45 lr 0.000022	time 0.1448 (0.1464)	loss 0.2156 (0.2178)	grad_norm 9390.5762 (14637.2568)	mem 4918MB
[2022-02-06 22:58:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16790/40036]	eta 0:56:44 lr 0.000022	time 0.1447 (0.1464)	loss 0.2131 (0.2178)	grad_norm 11368.3818 (14634.8662)	mem 4918MB
[2022-02-06 22:58:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16800/40036]	eta 0:56:42 lr 0.000022	time 0.1448 (0.1464)	loss 0.2186 (0.2178)	grad_norm 10675.3564 (14632.5303)	mem 4918MB
[2022-02-06 22:58:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16810/40036]	eta 0:56:41 lr 0.000022	time 0.1476 (0.1464)	loss 0.2185 (0.2178)	grad_norm 10118.0469 (14630.3965)	mem 4918MB
[2022-02-06 22:58:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16820/40036]	eta 0:56:39 lr 0.000022	time 0.1461 (0.1464)	loss 0.2196 (0.2178)	grad_norm 12646.4160 (14628.2373)	mem 4918MB
[2022-02-06 22:58:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16830/40036]	eta 0:56:38 lr 0.000022	time 0.1468 (0.1464)	loss 0.2162 (0.2178)	grad_norm 10172.2266 (14625.6494)	mem 4918MB
[2022-02-06 22:58:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16840/40036]	eta 0:56:37 lr 0.000022	time 0.1450 (0.1464)	loss 0.2164 (0.2178)	grad_norm 10158.6924 (14622.8906)	mem 4918MB
[2022-02-06 22:58:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16850/40036]	eta 0:56:35 lr 0.000022	time 0.1460 (0.1464)	loss 0.2192 (0.2178)	grad_norm 10461.2373 (14620.4385)	mem 4918MB
[2022-02-06 22:58:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16860/40036]	eta 0:56:34 lr 0.000022	time 0.1448 (0.1464)	loss 0.2176 (0.2178)	grad_norm 11026.4639 (14618.2598)	mem 4918MB
[2022-02-06 22:58:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16870/40036]	eta 0:56:32 lr 0.000022	time 0.1461 (0.1465)	loss 0.2206 (0.2178)	grad_norm 12521.7061 (14615.9697)	mem 4918MB
[2022-02-06 22:58:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16880/40036]	eta 0:56:31 lr 0.000022	time 0.1459 (0.1465)	loss 0.2157 (0.2178)	grad_norm 9876.1523 (14613.3398)	mem 4918MB
[2022-02-06 22:58:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16890/40036]	eta 0:56:29 lr 0.000022	time 0.1507 (0.1465)	loss 0.2222 (0.2178)	grad_norm 12372.7842 (14611.2246)	mem 4918MB
[2022-02-06 22:58:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16900/40036]	eta 0:56:28 lr 0.000022	time 0.1478 (0.1465)	loss 0.2148 (0.2178)	grad_norm 10737.8105 (14608.9541)	mem 4918MB
[2022-02-06 22:58:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16910/40036]	eta 0:56:26 lr 0.000022	time 0.1475 (0.1465)	loss 0.2193 (0.2178)	grad_norm 10141.3154 (14606.6152)	mem 4918MB
[2022-02-06 22:58:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16920/40036]	eta 0:56:25 lr 0.000022	time 0.1465 (0.1465)	loss 0.2187 (0.2178)	grad_norm 10359.9814 (14604.2041)	mem 4918MB
[2022-02-06 22:58:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16930/40036]	eta 0:56:24 lr 0.000022	time 0.1475 (0.1465)	loss 0.2178 (0.2178)	grad_norm 10159.4902 (14601.9502)	mem 4918MB
[2022-02-06 22:58:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16940/40036]	eta 0:56:22 lr 0.000022	time 0.1489 (0.1465)	loss 0.2215 (0.2178)	grad_norm 10681.4053 (14599.9023)	mem 4918MB
[2022-02-06 22:59:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16950/40036]	eta 0:56:21 lr 0.000022	time 0.1514 (0.1465)	loss 0.2159 (0.2178)	grad_norm 11652.3652 (14597.5547)	mem 4918MB
[2022-02-06 22:59:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16960/40036]	eta 0:56:19 lr 0.000022	time 0.1492 (0.1465)	loss 0.2178 (0.2178)	grad_norm 11267.7383 (14595.3213)	mem 4918MB
[2022-02-06 22:59:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16970/40036]	eta 0:56:18 lr 0.000022	time 0.1493 (0.1465)	loss 0.2175 (0.2178)	grad_norm 9120.9434 (14592.8154)	mem 4918MB
[2022-02-06 22:59:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16980/40036]	eta 0:56:16 lr 0.000022	time 0.1493 (0.1465)	loss 0.2179 (0.2178)	grad_norm 11795.2178 (14590.3848)	mem 4918MB
[2022-02-06 22:59:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][16990/40036]	eta 0:56:15 lr 0.000022	time 0.1462 (0.1465)	loss 0.2162 (0.2178)	grad_norm 8879.4980 (14587.9951)	mem 4918MB
[2022-02-06 22:59:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17000/40036]	eta 0:56:14 lr 0.000022	time 0.1444 (0.1465)	loss 0.2157 (0.2178)	grad_norm 10767.2783 (14585.5801)	mem 4918MB
[2022-02-06 22:59:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17010/40036]	eta 0:56:12 lr 0.000022	time 0.1452 (0.1465)	loss 0.2190 (0.2178)	grad_norm 9206.4277 (14583.2275)	mem 4918MB
[2022-02-06 22:59:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17020/40036]	eta 0:56:11 lr 0.000022	time 0.1452 (0.1465)	loss 0.2161 (0.2178)	grad_norm 10588.7695 (14580.9238)	mem 4918MB
[2022-02-06 22:59:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17030/40036]	eta 0:56:09 lr 0.000022	time 0.1486 (0.1465)	loss 0.2184 (0.2178)	grad_norm 9804.8857 (14578.8291)	mem 4918MB
[2022-02-06 22:59:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17040/40036]	eta 0:56:08 lr 0.000022	time 0.1474 (0.1465)	loss 0.2192 (0.2178)	grad_norm 10769.6318 (14576.6553)	mem 4918MB
[2022-02-06 22:59:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17050/40036]	eta 0:56:06 lr 0.000022	time 0.1460 (0.1465)	loss 0.2174 (0.2178)	grad_norm 9973.6592 (14574.3760)	mem 4918MB
[2022-02-06 22:59:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17060/40036]	eta 0:56:05 lr 0.000022	time 0.1455 (0.1465)	loss 0.2188 (0.2178)	grad_norm 9988.9326 (14572.1133)	mem 4918MB
[2022-02-06 22:59:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17070/40036]	eta 0:56:03 lr 0.000022	time 0.1471 (0.1465)	loss 0.2178 (0.2178)	grad_norm 9714.1934 (14570.0107)	mem 4918MB
[2022-02-06 22:59:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17080/40036]	eta 0:56:02 lr 0.000022	time 0.1476 (0.1465)	loss 0.2161 (0.2178)	grad_norm 10832.4209 (14567.8164)	mem 4918MB
[2022-02-06 22:59:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17090/40036]	eta 0:56:00 lr 0.000022	time 0.1468 (0.1465)	loss 0.2172 (0.2178)	grad_norm 11648.5039 (14565.3555)	mem 4918MB
[2022-02-06 22:59:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17100/40036]	eta 0:55:59 lr 0.000022	time 0.1459 (0.1465)	loss 0.2144 (0.2178)	grad_norm 9044.9307 (14562.7549)	mem 4918MB
[2022-02-06 22:59:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17110/40036]	eta 0:55:58 lr 0.000022	time 0.1462 (0.1465)	loss 0.2185 (0.2178)	grad_norm 10066.1494 (14560.3115)	mem 4918MB
[2022-02-06 22:59:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17120/40036]	eta 0:55:56 lr 0.000022	time 0.1457 (0.1465)	loss 0.2180 (0.2178)	grad_norm 10550.3945 (14557.8281)	mem 4918MB
[2022-02-06 22:59:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17130/40036]	eta 0:55:55 lr 0.000022	time 0.1448 (0.1465)	loss 0.2166 (0.2178)	grad_norm 10307.9502 (14555.6006)	mem 4918MB
[2022-02-06 22:59:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17140/40036]	eta 0:55:53 lr 0.000022	time 0.1452 (0.1465)	loss 0.2146 (0.2178)	grad_norm 9298.9746 (14553.3271)	mem 4918MB
[2022-02-06 22:59:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17150/40036]	eta 0:55:52 lr 0.000022	time 0.1451 (0.1465)	loss 0.2175 (0.2178)	grad_norm 11083.1445 (14550.7441)	mem 4918MB
[2022-02-06 22:59:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17160/40036]	eta 0:55:50 lr 0.000022	time 0.1461 (0.1465)	loss 0.2185 (0.2178)	grad_norm 10351.1191 (14548.5322)	mem 4918MB
[2022-02-06 22:59:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17170/40036]	eta 0:55:49 lr 0.000022	time 0.1479 (0.1465)	loss 0.2180 (0.2178)	grad_norm 10385.8809 (14546.0801)	mem 4918MB
[2022-02-06 22:59:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17180/40036]	eta 0:55:47 lr 0.000022	time 0.1456 (0.1465)	loss 0.2164 (0.2178)	grad_norm 12234.8330 (14543.8486)	mem 4918MB
[2022-02-06 22:59:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17190/40036]	eta 0:55:46 lr 0.000022	time 0.1455 (0.1465)	loss 0.2211 (0.2178)	grad_norm 10676.7646 (14541.8301)	mem 4918MB
[2022-02-06 22:59:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17200/40036]	eta 0:55:44 lr 0.000022	time 0.1449 (0.1465)	loss 0.2156 (0.2178)	grad_norm 10325.6064 (14539.4375)	mem 4918MB
[2022-02-06 22:59:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17210/40036]	eta 0:55:43 lr 0.000022	time 0.1454 (0.1465)	loss 0.2181 (0.2178)	grad_norm 9241.9688 (14536.9551)	mem 4918MB
[2022-02-06 22:59:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17220/40036]	eta 0:55:41 lr 0.000022	time 0.1451 (0.1465)	loss 0.2174 (0.2178)	grad_norm 10320.6943 (14534.5566)	mem 4918MB
[2022-02-06 22:59:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17230/40036]	eta 0:55:40 lr 0.000022	time 0.1458 (0.1465)	loss 0.2176 (0.2178)	grad_norm 11602.3691 (14532.5635)	mem 4918MB
[2022-02-06 22:59:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17240/40036]	eta 0:55:39 lr 0.000022	time 0.1458 (0.1465)	loss 0.2159 (0.2178)	grad_norm 9627.1270 (14530.2256)	mem 4918MB
[2022-02-06 22:59:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17250/40036]	eta 0:55:37 lr 0.000023	time 0.1447 (0.1465)	loss 0.2162 (0.2178)	grad_norm 10492.1387 (14527.9629)	mem 4918MB
[2022-02-06 22:59:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17260/40036]	eta 0:55:36 lr 0.000023	time 0.1470 (0.1465)	loss 0.2151 (0.2178)	grad_norm 10730.0557 (14525.5547)	mem 4918MB
[2022-02-06 22:59:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17270/40036]	eta 0:55:34 lr 0.000023	time 0.1461 (0.1465)	loss 0.2167 (0.2178)	grad_norm 12556.0615 (14523.6074)	mem 4918MB
[2022-02-06 22:59:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17280/40036]	eta 0:55:33 lr 0.000023	time 0.1448 (0.1465)	loss 0.2193 (0.2178)	grad_norm 8911.1787 (14521.2266)	mem 4918MB
[2022-02-06 22:59:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17290/40036]	eta 0:55:31 lr 0.000023	time 0.1469 (0.1465)	loss 0.2181 (0.2178)	grad_norm 10056.9375 (14518.9170)	mem 4918MB
[2022-02-06 22:59:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17300/40036]	eta 0:55:30 lr 0.000023	time 0.1453 (0.1465)	loss 0.2186 (0.2178)	grad_norm 8819.9951 (14516.5068)	mem 4918MB
[2022-02-06 22:59:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17310/40036]	eta 0:55:28 lr 0.000023	time 0.1454 (0.1465)	loss 0.2174 (0.2178)	grad_norm 9480.2217 (14513.9072)	mem 4918MB
[2022-02-06 22:59:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17320/40036]	eta 0:55:27 lr 0.000023	time 0.1463 (0.1465)	loss 0.2178 (0.2178)	grad_norm 9363.7090 (14511.4727)	mem 4918MB
[2022-02-06 22:59:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17330/40036]	eta 0:55:25 lr 0.000023	time 0.1450 (0.1465)	loss 0.2195 (0.2178)	grad_norm 8690.9668 (14508.6191)	mem 4918MB
[2022-02-06 22:59:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17340/40036]	eta 0:55:24 lr 0.000023	time 0.1456 (0.1465)	loss 0.2190 (0.2178)	grad_norm 11568.8555 (14506.2900)	mem 4918MB
[2022-02-06 22:59:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17350/40036]	eta 0:55:22 lr 0.000023	time 0.1460 (0.1465)	loss 0.2176 (0.2178)	grad_norm 10407.9736 (14504.1387)	mem 4918MB
[2022-02-06 23:00:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17360/40036]	eta 0:55:21 lr 0.000023	time 0.1486 (0.1465)	loss 0.2159 (0.2178)	grad_norm 10015.0693 (14501.8643)	mem 4918MB
[2022-02-06 23:00:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17370/40036]	eta 0:55:20 lr 0.000023	time 0.1481 (0.1465)	loss 0.2212 (0.2178)	grad_norm 9473.7412 (14499.4736)	mem 4918MB
[2022-02-06 23:00:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17380/40036]	eta 0:55:18 lr 0.000023	time 0.1463 (0.1465)	loss 0.2131 (0.2178)	grad_norm 11959.2109 (14497.3633)	mem 4918MB
[2022-02-06 23:00:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17390/40036]	eta 0:55:17 lr 0.000023	time 0.1484 (0.1465)	loss 0.2161 (0.2178)	grad_norm 11167.8447 (14494.9707)	mem 4918MB
[2022-02-06 23:00:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17400/40036]	eta 0:55:15 lr 0.000023	time 0.1446 (0.1465)	loss 0.2170 (0.2178)	grad_norm 11862.4775 (14492.5742)	mem 4918MB
[2022-02-06 23:00:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17410/40036]	eta 0:55:14 lr 0.000023	time 0.1446 (0.1465)	loss 0.2160 (0.2178)	grad_norm 10371.5752 (14490.2705)	mem 4918MB
[2022-02-06 23:00:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17420/40036]	eta 0:55:12 lr 0.000023	time 0.1452 (0.1465)	loss 0.2148 (0.2178)	grad_norm 10745.5361 (14487.5898)	mem 4918MB
[2022-02-06 23:00:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17430/40036]	eta 0:55:11 lr 0.000023	time 0.1466 (0.1465)	loss 0.2173 (0.2178)	grad_norm 9939.2109 (14485.1816)	mem 4918MB
[2022-02-06 23:00:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17440/40036]	eta 0:55:09 lr 0.000023	time 0.1450 (0.1465)	loss 0.2182 (0.2178)	grad_norm 10023.9863 (14482.9639)	mem 4918MB
[2022-02-06 23:00:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17450/40036]	eta 0:55:08 lr 0.000023	time 0.1452 (0.1465)	loss 0.2164 (0.2178)	grad_norm 11485.3994 (14480.7705)	mem 4918MB
[2022-02-06 23:00:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17460/40036]	eta 0:55:06 lr 0.000023	time 0.1454 (0.1465)	loss 0.2178 (0.2178)	grad_norm 8780.5615 (14478.5879)	mem 4918MB
[2022-02-06 23:00:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17470/40036]	eta 0:55:05 lr 0.000023	time 0.1456 (0.1465)	loss 0.2164 (0.2178)	grad_norm 11739.5791 (14476.0898)	mem 4918MB
[2022-02-06 23:00:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17480/40036]	eta 0:55:03 lr 0.000023	time 0.1461 (0.1465)	loss 0.2174 (0.2178)	grad_norm 9352.4238 (14474.0068)	mem 4918MB
[2022-02-06 23:00:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17490/40036]	eta 0:55:02 lr 0.000023	time 0.1463 (0.1465)	loss 0.2163 (0.2178)	grad_norm 11075.6787 (14471.5879)	mem 4918MB
[2022-02-06 23:00:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17500/40036]	eta 0:55:01 lr 0.000023	time 0.1478 (0.1465)	loss 0.2175 (0.2178)	grad_norm 11086.2080 (14469.4004)	mem 4918MB
[2022-02-06 23:00:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17510/40036]	eta 0:54:59 lr 0.000023	time 0.1452 (0.1465)	loss 0.2174 (0.2178)	grad_norm 10020.9668 (14466.9746)	mem 4918MB
[2022-02-06 23:00:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17520/40036]	eta 0:54:58 lr 0.000023	time 0.1465 (0.1465)	loss 0.2158 (0.2178)	grad_norm 9042.9551 (14464.4727)	mem 4918MB
[2022-02-06 23:00:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17530/40036]	eta 0:54:56 lr 0.000023	time 0.1452 (0.1465)	loss 0.2174 (0.2178)	grad_norm 10977.7080 (14462.2402)	mem 4918MB
[2022-02-06 23:00:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17540/40036]	eta 0:54:55 lr 0.000023	time 0.1452 (0.1465)	loss 0.2178 (0.2178)	grad_norm 10207.9609 (14459.9521)	mem 4918MB
[2022-02-06 23:00:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17550/40036]	eta 0:54:53 lr 0.000023	time 0.1472 (0.1465)	loss 0.2167 (0.2178)	grad_norm 12062.9141 (14457.6875)	mem 4918MB
[2022-02-06 23:00:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17560/40036]	eta 0:54:52 lr 0.000023	time 0.1456 (0.1465)	loss 0.2183 (0.2178)	grad_norm 9385.9102 (14455.4512)	mem 4918MB
[2022-02-06 23:00:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17570/40036]	eta 0:54:50 lr 0.000023	time 0.1455 (0.1465)	loss 0.2171 (0.2178)	grad_norm 10504.3682 (14453.1553)	mem 4918MB
[2022-02-06 23:00:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17580/40036]	eta 0:54:49 lr 0.000023	time 0.1453 (0.1465)	loss 0.2158 (0.2178)	grad_norm 10214.0586 (14450.7793)	mem 4918MB
[2022-02-06 23:00:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17590/40036]	eta 0:54:47 lr 0.000023	time 0.1450 (0.1465)	loss 0.2183 (0.2178)	grad_norm 10286.6729 (14448.2217)	mem 4918MB
[2022-02-06 23:00:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17600/40036]	eta 0:54:46 lr 0.000023	time 0.1475 (0.1465)	loss 0.2170 (0.2178)	grad_norm 8223.3398 (14445.6445)	mem 4918MB
[2022-02-06 23:00:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17610/40036]	eta 0:54:44 lr 0.000023	time 0.1459 (0.1465)	loss 0.2180 (0.2178)	grad_norm 8923.8926 (14443.2041)	mem 4918MB
[2022-02-06 23:00:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17620/40036]	eta 0:54:43 lr 0.000023	time 0.1462 (0.1465)	loss 0.2186 (0.2178)	grad_norm 9639.6836 (14440.9873)	mem 4918MB
[2022-02-06 23:00:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17630/40036]	eta 0:54:42 lr 0.000023	time 0.1461 (0.1465)	loss 0.2167 (0.2178)	grad_norm 10211.6562 (14438.6426)	mem 4918MB
[2022-02-06 23:00:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17640/40036]	eta 0:54:40 lr 0.000023	time 0.1451 (0.1465)	loss 0.2175 (0.2178)	grad_norm 10346.1191 (14436.5830)	mem 4918MB
[2022-02-06 23:00:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17650/40036]	eta 0:54:39 lr 0.000023	time 0.1454 (0.1465)	loss 0.2175 (0.2178)	grad_norm 10627.8271 (14434.3594)	mem 4918MB
[2022-02-06 23:00:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17660/40036]	eta 0:54:37 lr 0.000023	time 0.1469 (0.1465)	loss 0.2209 (0.2178)	grad_norm 11519.8252 (14431.8799)	mem 4918MB
[2022-02-06 23:00:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17670/40036]	eta 0:54:36 lr 0.000023	time 0.1458 (0.1465)	loss 0.2202 (0.2178)	grad_norm 9705.7891 (14429.4551)	mem 4918MB
[2022-02-06 23:00:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17680/40036]	eta 0:54:34 lr 0.000023	time 0.1454 (0.1465)	loss 0.2140 (0.2178)	grad_norm 10670.4502 (14427.2383)	mem 4918MB
[2022-02-06 23:00:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17690/40036]	eta 0:54:33 lr 0.000023	time 0.1453 (0.1465)	loss 0.2155 (0.2178)	grad_norm 12584.5908 (14425.5117)	mem 4918MB
[2022-02-06 23:00:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17700/40036]	eta 0:54:31 lr 0.000023	time 0.1449 (0.1465)	loss 0.2177 (0.2178)	grad_norm 8806.6387 (14423.2842)	mem 4918MB
[2022-02-06 23:00:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17710/40036]	eta 0:54:30 lr 0.000023	time 0.1464 (0.1465)	loss 0.2169 (0.2178)	grad_norm 8815.5381 (14421.0107)	mem 4918MB
[2022-02-06 23:00:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17720/40036]	eta 0:54:28 lr 0.000023	time 0.1465 (0.1465)	loss 0.2181 (0.2178)	grad_norm 9865.0508 (14418.9072)	mem 4918MB
[2022-02-06 23:00:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17730/40036]	eta 0:54:27 lr 0.000023	time 0.1491 (0.1465)	loss 0.2140 (0.2178)	grad_norm 10667.3857 (14416.6396)	mem 4918MB
[2022-02-06 23:00:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17740/40036]	eta 0:54:26 lr 0.000023	time 0.1466 (0.1465)	loss 0.2174 (0.2178)	grad_norm 11603.4229 (14414.3330)	mem 4918MB
[2022-02-06 23:00:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17750/40036]	eta 0:54:24 lr 0.000023	time 0.1458 (0.1465)	loss 0.2173 (0.2178)	grad_norm 11616.3281 (14411.8955)	mem 4918MB
[2022-02-06 23:01:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17760/40036]	eta 0:54:23 lr 0.000023	time 0.1460 (0.1465)	loss 0.2184 (0.2178)	grad_norm 10124.9102 (14409.1953)	mem 4918MB
[2022-02-06 23:01:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17770/40036]	eta 0:54:21 lr 0.000023	time 0.1447 (0.1465)	loss 0.2155 (0.2178)	grad_norm 10172.4062 (14407.0244)	mem 4918MB
[2022-02-06 23:01:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17780/40036]	eta 0:54:20 lr 0.000023	time 0.1465 (0.1465)	loss 0.2169 (0.2178)	grad_norm 9433.1270 (14404.8701)	mem 4918MB
[2022-02-06 23:01:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17790/40036]	eta 0:54:18 lr 0.000023	time 0.1462 (0.1465)	loss 0.2163 (0.2178)	grad_norm 10489.6670 (14402.6426)	mem 4918MB
[2022-02-06 23:01:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17800/40036]	eta 0:54:17 lr 0.000023	time 0.1460 (0.1465)	loss 0.2168 (0.2178)	grad_norm 10263.5205 (14400.1943)	mem 4918MB
[2022-02-06 23:01:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17810/40036]	eta 0:54:15 lr 0.000023	time 0.1459 (0.1465)	loss 0.2174 (0.2178)	grad_norm 10681.9014 (14397.8369)	mem 4918MB
[2022-02-06 23:01:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17820/40036]	eta 0:54:14 lr 0.000023	time 0.1449 (0.1465)	loss 0.2174 (0.2178)	grad_norm 9641.4590 (14395.6260)	mem 4918MB
[2022-02-06 23:01:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17830/40036]	eta 0:54:12 lr 0.000023	time 0.1494 (0.1465)	loss 0.2179 (0.2178)	grad_norm 10317.4873 (14393.4141)	mem 4918MB
[2022-02-06 23:01:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17840/40036]	eta 0:54:11 lr 0.000023	time 0.1471 (0.1465)	loss 0.2162 (0.2178)	grad_norm 11583.5703 (14390.9443)	mem 4918MB
[2022-02-06 23:01:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17850/40036]	eta 0:54:10 lr 0.000023	time 0.1514 (0.1465)	loss 0.2147 (0.2178)	grad_norm 10939.9014 (14388.6670)	mem 4918MB
[2022-02-06 23:01:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17860/40036]	eta 0:54:08 lr 0.000023	time 0.1510 (0.1465)	loss 0.2178 (0.2178)	grad_norm 10922.6738 (14386.2773)	mem 4918MB
[2022-02-06 23:01:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17870/40036]	eta 0:54:07 lr 0.000023	time 0.1489 (0.1465)	loss 0.2186 (0.2178)	grad_norm 10461.9629 (14383.9697)	mem 4918MB
[2022-02-06 23:01:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17880/40036]	eta 0:54:05 lr 0.000023	time 0.1473 (0.1465)	loss 0.2164 (0.2178)	grad_norm 9435.5146 (14381.7734)	mem 4918MB
[2022-02-06 23:01:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17890/40036]	eta 0:54:04 lr 0.000023	time 0.1451 (0.1465)	loss 0.2152 (0.2178)	grad_norm 11329.8906 (14379.7344)	mem 4918MB
[2022-02-06 23:01:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17900/40036]	eta 0:54:02 lr 0.000023	time 0.1484 (0.1465)	loss 0.2180 (0.2178)	grad_norm 8943.8984 (14377.2920)	mem 4918MB
[2022-02-06 23:01:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17910/40036]	eta 0:54:01 lr 0.000023	time 0.1551 (0.1465)	loss 0.2181 (0.2178)	grad_norm 9092.6777 (14374.6162)	mem 4918MB
[2022-02-06 23:01:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17920/40036]	eta 0:54:00 lr 0.000023	time 0.1454 (0.1465)	loss 0.2183 (0.2178)	grad_norm 11254.2725 (14372.1689)	mem 4918MB
[2022-02-06 23:01:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17930/40036]	eta 0:53:58 lr 0.000023	time 0.1463 (0.1465)	loss 0.2151 (0.2178)	grad_norm 10832.6953 (14369.7539)	mem 4918MB
[2022-02-06 23:01:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17940/40036]	eta 0:53:57 lr 0.000023	time 0.1488 (0.1465)	loss 0.2177 (0.2178)	grad_norm 9308.5293 (14367.3232)	mem 4918MB
[2022-02-06 23:01:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17950/40036]	eta 0:53:55 lr 0.000023	time 0.1460 (0.1465)	loss 0.2157 (0.2178)	grad_norm 9762.5098 (14364.9414)	mem 4918MB
[2022-02-06 23:01:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17960/40036]	eta 0:53:54 lr 0.000023	time 0.1473 (0.1465)	loss 0.2167 (0.2178)	grad_norm 9972.1016 (14362.5869)	mem 4918MB
[2022-02-06 23:01:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17970/40036]	eta 0:53:52 lr 0.000023	time 0.1459 (0.1465)	loss 0.2153 (0.2178)	grad_norm 8978.1875 (14360.2832)	mem 4918MB
[2022-02-06 23:01:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17980/40036]	eta 0:53:51 lr 0.000023	time 0.1459 (0.1465)	loss 0.2152 (0.2178)	grad_norm 11965.8633 (14357.9160)	mem 4918MB
[2022-02-06 23:01:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][17990/40036]	eta 0:53:49 lr 0.000023	time 0.1480 (0.1465)	loss 0.2168 (0.2178)	grad_norm 10159.9775 (14355.8701)	mem 4918MB
[2022-02-06 23:01:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18000/40036]	eta 0:53:48 lr 0.000023	time 0.1485 (0.1465)	loss 0.2177 (0.2178)	grad_norm 8308.5635 (14353.5518)	mem 4918MB
[2022-02-06 23:01:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18010/40036]	eta 0:53:47 lr 0.000023	time 0.1453 (0.1465)	loss 0.2166 (0.2178)	grad_norm 9506.4600 (14350.9541)	mem 4918MB
[2022-02-06 23:01:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18020/40036]	eta 0:53:45 lr 0.000023	time 0.1465 (0.1465)	loss 0.2192 (0.2178)	grad_norm 10240.6475 (14348.4385)	mem 4918MB
[2022-02-06 23:01:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18030/40036]	eta 0:53:44 lr 0.000023	time 0.1455 (0.1465)	loss 0.2204 (0.2178)	grad_norm 10597.9922 (14346.1895)	mem 4918MB
[2022-02-06 23:01:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18040/40036]	eta 0:53:42 lr 0.000023	time 0.1454 (0.1465)	loss 0.2165 (0.2178)	grad_norm 10103.2783 (14344.0410)	mem 4918MB
[2022-02-06 23:01:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18050/40036]	eta 0:53:41 lr 0.000024	time 0.1474 (0.1465)	loss 0.2200 (0.2178)	grad_norm 10960.5762 (14341.7676)	mem 4918MB
[2022-02-06 23:01:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18060/40036]	eta 0:53:39 lr 0.000024	time 0.1459 (0.1465)	loss 0.2146 (0.2178)	grad_norm 9595.2949 (14339.5391)	mem 4918MB
[2022-02-06 23:01:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18070/40036]	eta 0:53:38 lr 0.000024	time 0.1467 (0.1465)	loss 0.2161 (0.2178)	grad_norm 9685.7090 (14337.2422)	mem 4918MB
[2022-02-06 23:01:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18080/40036]	eta 0:53:36 lr 0.000024	time 0.1451 (0.1465)	loss 0.2195 (0.2178)	grad_norm 11467.6914 (14334.8311)	mem 4918MB
[2022-02-06 23:01:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18090/40036]	eta 0:53:35 lr 0.000024	time 0.1456 (0.1465)	loss 0.2179 (0.2178)	grad_norm 10678.4502 (14332.6904)	mem 4918MB
[2022-02-06 23:01:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18100/40036]	eta 0:53:33 lr 0.000024	time 0.1478 (0.1465)	loss 0.2160 (0.2178)	grad_norm 11343.0264 (14330.2480)	mem 4918MB
[2022-02-06 23:01:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18110/40036]	eta 0:53:32 lr 0.000024	time 0.1545 (0.1465)	loss 0.2191 (0.2178)	grad_norm 8936.7666 (14327.9639)	mem 4918MB
[2022-02-06 23:01:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18120/40036]	eta 0:53:31 lr 0.000024	time 0.1454 (0.1465)	loss 0.2182 (0.2178)	grad_norm 10015.4609 (14325.6426)	mem 4918MB
[2022-02-06 23:01:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18130/40036]	eta 0:53:29 lr 0.000024	time 0.1500 (0.1465)	loss 0.2167 (0.2178)	grad_norm 11161.7070 (14323.3545)	mem 4918MB
[2022-02-06 23:01:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18140/40036]	eta 0:53:28 lr 0.000024	time 0.1476 (0.1465)	loss 0.2162 (0.2178)	grad_norm 10136.4268 (14320.9580)	mem 4918MB
[2022-02-06 23:01:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18150/40036]	eta 0:53:26 lr 0.000024	time 0.1510 (0.1465)	loss 0.2159 (0.2178)	grad_norm 9164.9746 (14318.3037)	mem 4918MB
[2022-02-06 23:01:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18160/40036]	eta 0:53:25 lr 0.000024	time 0.1455 (0.1465)	loss 0.2186 (0.2178)	grad_norm 10411.2500 (14316.0605)	mem 4918MB
[2022-02-06 23:02:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18170/40036]	eta 0:53:23 lr 0.000024	time 0.1476 (0.1465)	loss 0.2188 (0.2178)	grad_norm 9418.6553 (14313.6426)	mem 4918MB
[2022-02-06 23:02:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18180/40036]	eta 0:53:22 lr 0.000024	time 0.1480 (0.1465)	loss 0.2184 (0.2178)	grad_norm 10764.5264 (14311.4287)	mem 4918MB
[2022-02-06 23:02:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18190/40036]	eta 0:53:20 lr 0.000024	time 0.1465 (0.1465)	loss 0.2167 (0.2178)	grad_norm 8526.9766 (14309.1865)	mem 4918MB
[2022-02-06 23:02:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18200/40036]	eta 0:53:19 lr 0.000024	time 0.1459 (0.1465)	loss 0.2164 (0.2178)	grad_norm 10125.2090 (14306.9492)	mem 4918MB
[2022-02-06 23:02:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18210/40036]	eta 0:53:18 lr 0.000024	time 0.1579 (0.1465)	loss 0.2147 (0.2178)	grad_norm 9845.9609 (14304.5977)	mem 4918MB
[2022-02-06 23:02:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18220/40036]	eta 0:53:16 lr 0.000024	time 0.1471 (0.1465)	loss 0.2161 (0.2178)	grad_norm 9021.2842 (14302.2256)	mem 4918MB
[2022-02-06 23:02:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18230/40036]	eta 0:53:15 lr 0.000024	time 0.1493 (0.1465)	loss 0.2190 (0.2178)	grad_norm 9061.9600 (14299.9775)	mem 4918MB
[2022-02-06 23:02:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18240/40036]	eta 0:53:13 lr 0.000024	time 0.1547 (0.1465)	loss 0.2186 (0.2178)	grad_norm 10967.2236 (14297.3867)	mem 4918MB
[2022-02-06 23:02:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18250/40036]	eta 0:53:12 lr 0.000024	time 0.1472 (0.1465)	loss 0.2200 (0.2178)	grad_norm 9601.5439 (14294.9053)	mem 4918MB
[2022-02-06 23:02:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18260/40036]	eta 0:53:10 lr 0.000024	time 0.1472 (0.1465)	loss 0.2163 (0.2178)	grad_norm 9718.8438 (14292.5205)	mem 4918MB
[2022-02-06 23:02:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18270/40036]	eta 0:53:09 lr 0.000024	time 0.1494 (0.1465)	loss 0.2173 (0.2178)	grad_norm 9770.2490 (14290.1465)	mem 4918MB
[2022-02-06 23:02:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18280/40036]	eta 0:53:08 lr 0.000024	time 0.1532 (0.1465)	loss 0.2174 (0.2178)	grad_norm 10013.3799 (14287.6133)	mem 4918MB
[2022-02-06 23:02:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18290/40036]	eta 0:53:06 lr 0.000024	time 0.1566 (0.1465)	loss 0.2181 (0.2178)	grad_norm 11956.8877 (14285.3799)	mem 4918MB
[2022-02-06 23:02:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18300/40036]	eta 0:53:05 lr 0.000024	time 0.1458 (0.1465)	loss 0.2196 (0.2178)	grad_norm 9639.6592 (14283.1006)	mem 4918MB
[2022-02-06 23:02:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18310/40036]	eta 0:53:03 lr 0.000024	time 0.1461 (0.1465)	loss 0.2169 (0.2178)	grad_norm 9671.7900 (14280.8691)	mem 4918MB
[2022-02-06 23:02:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18320/40036]	eta 0:53:02 lr 0.000024	time 0.1445 (0.1465)	loss 0.2190 (0.2178)	grad_norm 10175.4619 (14278.7324)	mem 4918MB
[2022-02-06 23:02:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18330/40036]	eta 0:53:00 lr 0.000024	time 0.1508 (0.1465)	loss 0.2172 (0.2178)	grad_norm 10270.2012 (14276.3994)	mem 4918MB
[2022-02-06 23:02:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18340/40036]	eta 0:52:59 lr 0.000024	time 0.1461 (0.1465)	loss 0.2146 (0.2178)	grad_norm 11834.5811 (14274.1748)	mem 4918MB
[2022-02-06 23:02:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18350/40036]	eta 0:52:57 lr 0.000024	time 0.1481 (0.1465)	loss 0.2159 (0.2178)	grad_norm 9022.3398 (14271.9189)	mem 4918MB
[2022-02-06 23:02:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18360/40036]	eta 0:52:56 lr 0.000024	time 0.1469 (0.1465)	loss 0.2194 (0.2178)	grad_norm 9109.4990 (14269.6797)	mem 4918MB
[2022-02-06 23:02:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18370/40036]	eta 0:52:55 lr 0.000024	time 0.1459 (0.1465)	loss 0.2188 (0.2178)	grad_norm 9297.9414 (14267.3096)	mem 4918MB
[2022-02-06 23:02:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18380/40036]	eta 0:52:53 lr 0.000024	time 0.1485 (0.1465)	loss 0.2192 (0.2178)	grad_norm 10426.9941 (14264.8066)	mem 4918MB
[2022-02-06 23:02:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18390/40036]	eta 0:52:52 lr 0.000024	time 0.1442 (0.1465)	loss 0.2172 (0.2178)	grad_norm 10265.9482 (14262.5020)	mem 4918MB
[2022-02-06 23:02:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18400/40036]	eta 0:52:50 lr 0.000024	time 0.1486 (0.1465)	loss 0.2178 (0.2178)	grad_norm 10166.1592 (14260.4492)	mem 4918MB
[2022-02-06 23:02:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18410/40036]	eta 0:52:49 lr 0.000024	time 0.1454 (0.1465)	loss 0.2136 (0.2178)	grad_norm 9573.7812 (14257.9697)	mem 4918MB
[2022-02-06 23:02:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18420/40036]	eta 0:52:47 lr 0.000024	time 0.1459 (0.1466)	loss 0.2181 (0.2178)	grad_norm 10133.7969 (14255.8027)	mem 4918MB
[2022-02-06 23:02:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18430/40036]	eta 0:52:46 lr 0.000024	time 0.1461 (0.1466)	loss 0.2172 (0.2178)	grad_norm 8745.4844 (14253.6455)	mem 4918MB
[2022-02-06 23:02:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18440/40036]	eta 0:52:44 lr 0.000024	time 0.1481 (0.1466)	loss 0.2158 (0.2178)	grad_norm 10389.0439 (14251.4775)	mem 4918MB
[2022-02-06 23:02:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18450/40036]	eta 0:52:43 lr 0.000024	time 0.1455 (0.1466)	loss 0.2177 (0.2178)	grad_norm 10965.7812 (14249.1914)	mem 4918MB
[2022-02-06 23:02:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18460/40036]	eta 0:52:42 lr 0.000024	time 0.1451 (0.1466)	loss 0.2158 (0.2178)	grad_norm 9793.3242 (14247.0039)	mem 4918MB
[2022-02-06 23:02:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18470/40036]	eta 0:52:40 lr 0.000024	time 0.1464 (0.1466)	loss 0.2185 (0.2178)	grad_norm 9864.2529 (14244.7920)	mem 4918MB
[2022-02-06 23:02:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18480/40036]	eta 0:52:39 lr 0.000024	time 0.1508 (0.1466)	loss 0.2159 (0.2178)	grad_norm 10928.7236 (14242.2539)	mem 4918MB
[2022-02-06 23:02:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18490/40036]	eta 0:52:37 lr 0.000024	time 0.1498 (0.1466)	loss 0.2174 (0.2178)	grad_norm 9418.2197 (14239.9814)	mem 4918MB
[2022-02-06 23:02:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18500/40036]	eta 0:52:36 lr 0.000024	time 0.1506 (0.1466)	loss 0.2181 (0.2178)	grad_norm 10110.8057 (14237.7979)	mem 4918MB
[2022-02-06 23:02:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18510/40036]	eta 0:52:34 lr 0.000024	time 0.1461 (0.1466)	loss 0.2167 (0.2178)	grad_norm 10448.3330 (14235.6006)	mem 4918MB
[2022-02-06 23:02:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18520/40036]	eta 0:52:33 lr 0.000024	time 0.1456 (0.1466)	loss 0.2162 (0.2178)	grad_norm 9134.6016 (14233.1289)	mem 4918MB
[2022-02-06 23:02:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18530/40036]	eta 0:52:32 lr 0.000024	time 0.1447 (0.1466)	loss 0.2188 (0.2178)	grad_norm 10604.0615 (14230.8027)	mem 4918MB
[2022-02-06 23:02:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18540/40036]	eta 0:52:30 lr 0.000024	time 0.1454 (0.1466)	loss 0.2174 (0.2178)	grad_norm 10554.4922 (14228.9033)	mem 4918MB
[2022-02-06 23:02:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18550/40036]	eta 0:52:29 lr 0.000024	time 0.1506 (0.1466)	loss 0.2170 (0.2178)	grad_norm 10592.7041 (14226.6123)	mem 4918MB
[2022-02-06 23:02:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18560/40036]	eta 0:52:27 lr 0.000024	time 0.1476 (0.1466)	loss 0.2177 (0.2178)	grad_norm 9795.3330 (14224.2441)	mem 4918MB
[2022-02-06 23:03:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18570/40036]	eta 0:52:26 lr 0.000024	time 0.1529 (0.1466)	loss 0.2155 (0.2178)	grad_norm 9488.3301 (14221.9746)	mem 4918MB
[2022-02-06 23:03:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18580/40036]	eta 0:52:24 lr 0.000024	time 0.1447 (0.1466)	loss 0.2194 (0.2178)	grad_norm 9117.7490 (14219.6973)	mem 4918MB
[2022-02-06 23:03:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18590/40036]	eta 0:52:23 lr 0.000024	time 0.1462 (0.1466)	loss 0.2162 (0.2178)	grad_norm 8889.1084 (14217.3184)	mem 4918MB
[2022-02-06 23:03:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18600/40036]	eta 0:52:22 lr 0.000024	time 0.1456 (0.1466)	loss 0.2143 (0.2178)	grad_norm 8518.8193 (14214.9531)	mem 4918MB
[2022-02-06 23:03:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18610/40036]	eta 0:52:20 lr 0.000024	time 0.1465 (0.1466)	loss 0.2186 (0.2178)	grad_norm 9472.2109 (14212.6602)	mem 4918MB
[2022-02-06 23:03:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18620/40036]	eta 0:52:19 lr 0.000024	time 0.1503 (0.1466)	loss 0.2169 (0.2178)	grad_norm 7906.7632 (14210.1270)	mem 4918MB
[2022-02-06 23:03:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18630/40036]	eta 0:52:17 lr 0.000024	time 0.1447 (0.1466)	loss 0.2189 (0.2178)	grad_norm 11030.5020 (14208.1689)	mem 4918MB
[2022-02-06 23:03:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18640/40036]	eta 0:52:16 lr 0.000024	time 0.1474 (0.1466)	loss 0.2186 (0.2178)	grad_norm 9374.6719 (14205.7861)	mem 4918MB
[2022-02-06 23:03:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18650/40036]	eta 0:52:14 lr 0.000024	time 0.1471 (0.1466)	loss 0.2185 (0.2178)	grad_norm 9199.5879 (14203.5645)	mem 4918MB
[2022-02-06 23:03:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18660/40036]	eta 0:52:13 lr 0.000024	time 0.1493 (0.1466)	loss 0.2169 (0.2178)	grad_norm 9461.4453 (14201.3154)	mem 4918MB
[2022-02-06 23:03:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18670/40036]	eta 0:52:11 lr 0.000024	time 0.1454 (0.1466)	loss 0.2155 (0.2178)	grad_norm 9172.1709 (14199.2803)	mem 4918MB
[2022-02-06 23:03:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18680/40036]	eta 0:52:10 lr 0.000024	time 0.1502 (0.1466)	loss 0.2204 (0.2178)	grad_norm 10736.4541 (14197.0947)	mem 4918MB
[2022-02-06 23:03:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18690/40036]	eta 0:52:08 lr 0.000024	time 0.1483 (0.1466)	loss 0.2134 (0.2178)	grad_norm 8614.6221 (14194.6787)	mem 4918MB
[2022-02-06 23:03:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18700/40036]	eta 0:52:07 lr 0.000024	time 0.1526 (0.1466)	loss 0.2165 (0.2178)	grad_norm 8947.3389 (14192.4014)	mem 4918MB
[2022-02-06 23:03:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18710/40036]	eta 0:52:06 lr 0.000024	time 0.1480 (0.1466)	loss 0.2156 (0.2178)	grad_norm 10077.3584 (14190.3604)	mem 4918MB
[2022-02-06 23:03:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18720/40036]	eta 0:52:04 lr 0.000024	time 0.1530 (0.1466)	loss 0.2160 (0.2178)	grad_norm 9004.5029 (14188.1699)	mem 4918MB
[2022-02-06 23:03:24 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18730/40036]	eta 0:52:03 lr 0.000024	time 0.1457 (0.1466)	loss 0.2177 (0.2178)	grad_norm 10053.3662 (14186.0420)	mem 4918MB
[2022-02-06 23:03:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18740/40036]	eta 0:52:01 lr 0.000024	time 0.1471 (0.1466)	loss 0.2157 (0.2178)	grad_norm 9691.4951 (14183.7324)	mem 4918MB
[2022-02-06 23:03:27 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18750/40036]	eta 0:52:00 lr 0.000024	time 0.1496 (0.1466)	loss 0.2180 (0.2178)	grad_norm 12376.6719 (14181.4346)	mem 4918MB
[2022-02-06 23:03:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18760/40036]	eta 0:51:58 lr 0.000024	time 0.1497 (0.1466)	loss 0.2155 (0.2178)	grad_norm 9491.3516 (14179.0703)	mem 4918MB
[2022-02-06 23:03:30 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18770/40036]	eta 0:51:57 lr 0.000024	time 0.1462 (0.1466)	loss 0.2199 (0.2178)	grad_norm 9462.8076 (14176.5488)	mem 4918MB
[2022-02-06 23:03:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18780/40036]	eta 0:51:56 lr 0.000024	time 0.1511 (0.1466)	loss 0.2190 (0.2178)	grad_norm 9311.4219 (14174.1846)	mem 4918MB
[2022-02-06 23:03:33 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18790/40036]	eta 0:51:54 lr 0.000024	time 0.1503 (0.1466)	loss 0.2187 (0.2178)	grad_norm 11583.3105 (14171.7852)	mem 4918MB
[2022-02-06 23:03:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18800/40036]	eta 0:51:53 lr 0.000024	time 0.1522 (0.1466)	loss 0.2185 (0.2178)	grad_norm 10507.6377 (14169.4727)	mem 4918MB
[2022-02-06 23:03:36 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18810/40036]	eta 0:51:51 lr 0.000024	time 0.1525 (0.1466)	loss 0.2171 (0.2178)	grad_norm 8750.1914 (14167.1973)	mem 4918MB
[2022-02-06 23:03:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18820/40036]	eta 0:51:50 lr 0.000024	time 0.1471 (0.1466)	loss 0.2179 (0.2178)	grad_norm 10737.0645 (14164.8213)	mem 4918MB
[2022-02-06 23:03:39 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18830/40036]	eta 0:51:48 lr 0.000024	time 0.1523 (0.1466)	loss 0.2166 (0.2178)	grad_norm 9898.2627 (14162.4619)	mem 4918MB
[2022-02-06 23:03:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18840/40036]	eta 0:51:47 lr 0.000024	time 0.1449 (0.1466)	loss 0.2191 (0.2178)	grad_norm 10946.8311 (14160.3867)	mem 4918MB
[2022-02-06 23:03:42 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18850/40036]	eta 0:51:46 lr 0.000025	time 0.1498 (0.1466)	loss 0.2163 (0.2178)	grad_norm 9430.2148 (14158.3154)	mem 4918MB
[2022-02-06 23:03:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18860/40036]	eta 0:51:44 lr 0.000025	time 0.1508 (0.1466)	loss 0.2171 (0.2178)	grad_norm 9082.4053 (14155.8867)	mem 4918MB
[2022-02-06 23:03:45 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18870/40036]	eta 0:51:43 lr 0.000025	time 0.1498 (0.1466)	loss 0.2161 (0.2178)	grad_norm 8686.8311 (14153.7754)	mem 4918MB
[2022-02-06 23:03:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18880/40036]	eta 0:51:41 lr 0.000025	time 0.1449 (0.1466)	loss 0.2190 (0.2178)	grad_norm 11402.7432 (14151.4385)	mem 4918MB
[2022-02-06 23:03:48 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18890/40036]	eta 0:51:40 lr 0.000025	time 0.1476 (0.1466)	loss 0.2168 (0.2178)	grad_norm 9406.5771 (14148.9336)	mem 4918MB
[2022-02-06 23:03:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18900/40036]	eta 0:51:38 lr 0.000025	time 0.1506 (0.1466)	loss 0.2168 (0.2178)	grad_norm 10542.9072 (14146.5762)	mem 4918MB
[2022-02-06 23:03:51 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18910/40036]	eta 0:51:37 lr 0.000025	time 0.1475 (0.1466)	loss 0.2180 (0.2178)	grad_norm 9058.6152 (14144.1914)	mem 4918MB
[2022-02-06 23:03:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18920/40036]	eta 0:51:36 lr 0.000025	time 0.1591 (0.1466)	loss 0.2149 (0.2178)	grad_norm 10328.2383 (14141.9385)	mem 4918MB
[2022-02-06 23:03:54 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18930/40036]	eta 0:51:34 lr 0.000025	time 0.1449 (0.1466)	loss 0.2162 (0.2178)	grad_norm 10345.3838 (14139.6416)	mem 4918MB
[2022-02-06 23:03:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18940/40036]	eta 0:51:33 lr 0.000025	time 0.1461 (0.1466)	loss 0.2162 (0.2178)	grad_norm 10720.2061 (14137.4092)	mem 4918MB
[2022-02-06 23:03:57 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18950/40036]	eta 0:51:31 lr 0.000025	time 0.1540 (0.1466)	loss 0.2189 (0.2178)	grad_norm 9630.3535 (14134.9268)	mem 4918MB
[2022-02-06 23:03:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18960/40036]	eta 0:51:30 lr 0.000025	time 0.1455 (0.1466)	loss 0.2168 (0.2178)	grad_norm 9496.9307 (14132.9570)	mem 4918MB
[2022-02-06 23:04:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18970/40036]	eta 0:51:28 lr 0.000025	time 0.1498 (0.1466)	loss 0.2162 (0.2178)	grad_norm 11039.5586 (14130.4756)	mem 4918MB
[2022-02-06 23:04:01 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18980/40036]	eta 0:51:27 lr 0.000025	time 0.1541 (0.1466)	loss 0.2167 (0.2178)	grad_norm 9981.1162 (14128.3926)	mem 4918MB
[2022-02-06 23:04:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][18990/40036]	eta 0:51:26 lr 0.000025	time 0.1515 (0.1466)	loss 0.2162 (0.2178)	grad_norm 7944.1465 (14126.2842)	mem 4918MB
[2022-02-06 23:04:04 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19000/40036]	eta 0:51:24 lr 0.000025	time 0.1536 (0.1466)	loss 0.2166 (0.2178)	grad_norm 7990.6748 (14123.8496)	mem 4918MB
[2022-02-06 23:04:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19010/40036]	eta 0:51:23 lr 0.000025	time 0.1517 (0.1466)	loss 0.2166 (0.2178)	grad_norm 8916.5752 (14121.5352)	mem 4918MB
[2022-02-06 23:04:07 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19020/40036]	eta 0:51:21 lr 0.000025	time 0.1495 (0.1466)	loss 0.2179 (0.2178)	grad_norm 9016.6309 (14119.2695)	mem 4918MB
[2022-02-06 23:04:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19030/40036]	eta 0:51:20 lr 0.000025	time 0.1661 (0.1466)	loss 0.2146 (0.2178)	grad_norm 10477.3867 (14117.0205)	mem 4918MB
[2022-02-06 23:04:10 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19040/40036]	eta 0:51:19 lr 0.000025	time 0.1455 (0.1466)	loss 0.2166 (0.2178)	grad_norm 9208.9590 (14114.7363)	mem 4918MB
[2022-02-06 23:04:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19050/40036]	eta 0:51:17 lr 0.000025	time 0.1469 (0.1466)	loss 0.2160 (0.2178)	grad_norm 10609.3760 (14112.6855)	mem 4918MB
[2022-02-06 23:04:13 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19060/40036]	eta 0:51:16 lr 0.000025	time 0.1488 (0.1466)	loss 0.2147 (0.2178)	grad_norm 9613.6094 (14110.4268)	mem 4918MB
[2022-02-06 23:04:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19070/40036]	eta 0:51:14 lr 0.000025	time 0.1468 (0.1466)	loss 0.2168 (0.2178)	grad_norm 10251.8008 (14108.1162)	mem 4918MB
[2022-02-06 23:04:16 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19080/40036]	eta 0:51:13 lr 0.000025	time 0.1461 (0.1466)	loss 0.2186 (0.2178)	grad_norm 9837.2793 (14105.7930)	mem 4918MB
[2022-02-06 23:04:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19090/40036]	eta 0:51:11 lr 0.000025	time 0.1452 (0.1466)	loss 0.2171 (0.2178)	grad_norm 9965.1426 (14103.5889)	mem 4918MB
[2022-02-06 23:04:19 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19100/40036]	eta 0:51:10 lr 0.000025	time 0.1458 (0.1466)	loss 0.2170 (0.2178)	grad_norm 9586.7832 (14101.3096)	mem 4918MB
[2022-02-06 23:04:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19110/40036]	eta 0:51:08 lr 0.000025	time 0.1461 (0.1466)	loss 0.2178 (0.2178)	grad_norm 8975.5869 (14098.8076)	mem 4918MB
[2022-02-06 23:04:22 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19120/40036]	eta 0:51:07 lr 0.000025	time 0.1461 (0.1466)	loss 0.2180 (0.2178)	grad_norm 8361.3799 (14096.5781)	mem 4918MB
[2022-02-06 23:04:23 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19130/40036]	eta 0:51:05 lr 0.000025	time 0.1460 (0.1466)	loss 0.2156 (0.2178)	grad_norm 11282.1406 (14094.5215)	mem 4918MB
[2022-02-06 23:04:25 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19140/40036]	eta 0:51:04 lr 0.000025	time 0.1451 (0.1466)	loss 0.2176 (0.2178)	grad_norm 11990.9463 (14092.7559)	mem 4918MB
[2022-02-06 23:04:26 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19150/40036]	eta 0:51:02 lr 0.000025	time 0.1460 (0.1466)	loss 0.2179 (0.2178)	grad_norm 9034.9229 (14090.5137)	mem 4918MB
[2022-02-06 23:04:28 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19160/40036]	eta 0:51:01 lr 0.000025	time 0.1561 (0.1467)	loss 0.2164 (0.2178)	grad_norm 9832.0449 (14088.3379)	mem 4918MB
[2022-02-06 23:04:29 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19170/40036]	eta 0:51:00 lr 0.000025	time 0.1450 (0.1467)	loss 0.2121 (0.2178)	grad_norm 10472.6299 (14086.0605)	mem 4918MB
[2022-02-06 23:04:31 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19180/40036]	eta 0:50:58 lr 0.000025	time 0.1458 (0.1467)	loss 0.2154 (0.2178)	grad_norm 9244.7354 (14083.8135)	mem 4918MB
[2022-02-06 23:04:32 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19190/40036]	eta 0:50:57 lr 0.000025	time 0.1502 (0.1467)	loss 0.2172 (0.2178)	grad_norm 9636.7256 (14081.6025)	mem 4918MB
[2022-02-06 23:04:34 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19200/40036]	eta 0:50:55 lr 0.000025	time 0.1502 (0.1467)	loss 0.2166 (0.2178)	grad_norm 8750.0127 (14079.3350)	mem 4918MB
[2022-02-06 23:04:35 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19210/40036]	eta 0:50:54 lr 0.000025	time 0.1453 (0.1467)	loss 0.2163 (0.2178)	grad_norm 10034.9521 (14076.9961)	mem 4918MB
[2022-02-06 23:04:37 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19220/40036]	eta 0:50:52 lr 0.000025	time 0.1471 (0.1467)	loss 0.2204 (0.2178)	grad_norm 9629.5898 (14074.7061)	mem 4918MB
[2022-02-06 23:04:38 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19230/40036]	eta 0:50:51 lr 0.000025	time 0.1446 (0.1467)	loss 0.2172 (0.2178)	grad_norm 9726.3691 (14072.1650)	mem 4918MB
[2022-02-06 23:04:40 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19240/40036]	eta 0:50:49 lr 0.000025	time 0.1550 (0.1467)	loss 0.2203 (0.2178)	grad_norm 9147.4492 (14070.0850)	mem 4918MB
[2022-02-06 23:04:41 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19250/40036]	eta 0:50:48 lr 0.000025	time 0.1464 (0.1467)	loss 0.2175 (0.2178)	grad_norm 11910.8477 (14068.0615)	mem 4918MB
[2022-02-06 23:04:43 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19260/40036]	eta 0:50:46 lr 0.000025	time 0.1489 (0.1467)	loss 0.2175 (0.2178)	grad_norm 8059.8755 (14065.9746)	mem 4918MB
[2022-02-06 23:04:44 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19270/40036]	eta 0:50:45 lr 0.000025	time 0.1470 (0.1467)	loss 0.2177 (0.2178)	grad_norm 9802.8955 (14064.1064)	mem 4918MB
[2022-02-06 23:04:46 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19280/40036]	eta 0:50:44 lr 0.000025	time 0.1488 (0.1467)	loss 0.2179 (0.2178)	grad_norm 10207.8057 (14062.1650)	mem 4918MB
[2022-02-06 23:04:47 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19290/40036]	eta 0:50:42 lr 0.000025	time 0.1481 (0.1467)	loss 0.2157 (0.2178)	grad_norm 9829.7764 (14059.9561)	mem 4918MB
[2022-02-06 23:04:49 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19300/40036]	eta 0:50:41 lr 0.000025	time 0.1506 (0.1467)	loss 0.2181 (0.2178)	grad_norm 9481.3281 (14057.6514)	mem 4918MB
[2022-02-06 23:04:50 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19310/40036]	eta 0:50:39 lr 0.000025	time 0.1480 (0.1467)	loss 0.2162 (0.2178)	grad_norm 9534.3164 (14055.3262)	mem 4918MB
[2022-02-06 23:04:52 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19320/40036]	eta 0:50:38 lr 0.000025	time 0.1451 (0.1467)	loss 0.2186 (0.2178)	grad_norm 9832.5752 (14053.0615)	mem 4918MB
[2022-02-06 23:04:53 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19330/40036]	eta 0:50:36 lr 0.000025	time 0.1444 (0.1467)	loss 0.2184 (0.2178)	grad_norm 10864.7412 (14050.9893)	mem 4918MB
[2022-02-06 23:04:55 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19340/40036]	eta 0:50:35 lr 0.000025	time 0.1451 (0.1467)	loss 0.2167 (0.2178)	grad_norm 10577.9805 (14048.6855)	mem 4918MB
[2022-02-06 23:04:56 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19350/40036]	eta 0:50:33 lr 0.000025	time 0.1489 (0.1467)	loss 0.2171 (0.2178)	grad_norm 9294.7705 (14046.2500)	mem 4918MB
[2022-02-06 23:04:58 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19360/40036]	eta 0:50:32 lr 0.000025	time 0.1493 (0.1467)	loss 0.2167 (0.2178)	grad_norm 8197.0645 (14043.7461)	mem 4918MB
[2022-02-06 23:04:59 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19370/40036]	eta 0:50:30 lr 0.000025	time 0.1458 (0.1467)	loss 0.2194 (0.2178)	grad_norm 9091.6953 (14041.4873)	mem 4918MB
[2022-02-06 23:05:00 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19380/40036]	eta 0:50:29 lr 0.000025	time 0.1490 (0.1467)	loss 0.2198 (0.2178)	grad_norm 10381.4277 (14039.5264)	mem 4918MB
[2022-02-06 23:05:02 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19390/40036]	eta 0:50:28 lr 0.000025	time 0.1478 (0.1467)	loss 0.2149 (0.2178)	grad_norm 10138.1094 (14037.3848)	mem 4918MB
[2022-02-06 23:05:03 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19400/40036]	eta 0:50:26 lr 0.000025	time 0.1455 (0.1467)	loss 0.2147 (0.2178)	grad_norm 8373.0439 (14035.1426)	mem 4918MB
[2022-02-06 23:05:05 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19410/40036]	eta 0:50:25 lr 0.000025	time 0.1500 (0.1467)	loss 0.2172 (0.2178)	grad_norm 10539.1807 (14032.9189)	mem 4918MB
[2022-02-06 23:05:06 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19420/40036]	eta 0:50:23 lr 0.000025	time 0.1512 (0.1467)	loss 0.2161 (0.2178)	grad_norm 10427.5400 (14030.7041)	mem 4918MB
[2022-02-06 23:05:08 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19430/40036]	eta 0:50:22 lr 0.000025	time 0.1511 (0.1467)	loss 0.2195 (0.2178)	grad_norm 9599.4951 (14028.5850)	mem 4918MB
[2022-02-06 23:05:09 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19440/40036]	eta 0:50:20 lr 0.000025	time 0.1451 (0.1467)	loss 0.2172 (0.2178)	grad_norm 9886.1787 (14026.4463)	mem 4918MB
[2022-02-06 23:05:11 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19450/40036]	eta 0:50:19 lr 0.000025	time 0.1464 (0.1467)	loss 0.2155 (0.2178)	grad_norm 8025.4072 (14024.1357)	mem 4918MB
[2022-02-06 23:05:12 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19460/40036]	eta 0:50:17 lr 0.000025	time 0.1474 (0.1467)	loss 0.2146 (0.2178)	grad_norm 9675.5938 (14021.8506)	mem 4918MB
[2022-02-06 23:05:14 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19470/40036]	eta 0:50:16 lr 0.000025	time 0.1453 (0.1467)	loss 0.2151 (0.2178)	grad_norm 10190.3711 (14019.7227)	mem 4918MB
[2022-02-06 23:05:15 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19480/40036]	eta 0:50:14 lr 0.000025	time 0.1464 (0.1467)	loss 0.2156 (0.2178)	grad_norm 10618.0352 (14017.2520)	mem 4918MB
[2022-02-06 23:05:17 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19490/40036]	eta 0:50:13 lr 0.000025	time 0.1452 (0.1467)	loss 0.2146 (0.2178)	grad_norm 8646.6865 (14014.8965)	mem 4918MB
[2022-02-06 23:05:18 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19500/40036]	eta 0:50:12 lr 0.000025	time 0.1461 (0.1467)	loss 0.2173 (0.2178)	grad_norm 9827.6123 (14012.7266)	mem 4918MB
[2022-02-06 23:05:20 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19510/40036]	eta 0:50:10 lr 0.000025	time 0.1447 (0.1467)	loss 0.2193 (0.2178)	grad_norm 8882.9502 (14010.4375)	mem 4918MB
[2022-02-06 23:05:21 swin_small_patch4_window7_224] (main.py 229): INFO Train: [0/300][19520/40036]	eta 0:50:09 lr 0.000025	time 0.1477 (0.1467)	loss 0.2163 (0.2178)	grad_norm 8373.4688 (14008.2314)	mem 4918MB
